{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_players' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mall_players\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_players' is not defined"
     ]
    }
   ],
   "source": [
    "print(all_players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MILB DATA FROM TABLE DOWNLOADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\vile3\\\\Documents\\\\GitHub\\\\baseball_pred\\\\SCRAPE_FANGRAPHS\\\\Pit\\\\Batted\\\\fangraphs-minor-league-leaders.csv' -> 'C:\\\\Users\\\\vile3\\\\Documents\\\\GitHub\\\\baseball_pred\\\\SCRAPE_FANGRAPHS\\\\Pit\\\\Batted/pit_batted.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# from concurrent.futures import ThreadPoolExecutor\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# Rename the downloaded csv files to match their data_type and data_level\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(download_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/*.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 81\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdownload_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_level\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# data_types_levels = [\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#     ('bat', 'standard'),\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m#     ('bat', 'advanced'),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# with ThreadPoolExecutor(max_workers=len(data_types_levels)) as executor:\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m#     executor.map(download_wrapper, data_types_levels)\u001b[39;00m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\vile3\\\\Documents\\\\GitHub\\\\baseball_pred\\\\SCRAPE_FANGRAPHS\\\\Pit\\\\Batted\\\\fangraphs-minor-league-leaders.csv' -> 'C:\\\\Users\\\\vile3\\\\Documents\\\\GitHub\\\\baseball_pred\\\\SCRAPE_FANGRAPHS\\\\Pit\\\\Batted/pit_batted.csv'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import glob\n",
    "import os\n",
    "def download_fangraphs_data(data_type='bat', data_level='standard', download_dir='default_path'):\n",
    "    # Define the URLs for different data types and levels\n",
    "    urls = {\n",
    "        'bat_standard': 'https://www.fangraphs.com/leaders/minor-league?pos=all&level=0&lg=all&stats=bat&qual=10&type=0&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false&players=&sort=18,1&pageitems=2000000000&pg=0',\n",
    "        'bat_advanced': 'https://www.fangraphs.com/leaders/minor-league?pos=all&level=0&lg=all&stats=bat&qual=10&type=1&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false&players=&sort=18,1&pageitems=2000000000&pg=0',\n",
    "        'pit_standard': 'https://www.fangraphs.com/leaders/minor-league?pos=all&level=0&lg=all&stats=pit&qual=10&type=0&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false&players=&sort=18,1&pageitems=2000000000&pg=0',\n",
    "        'pit_advanced': 'https://www.fangraphs.com/leaders/minor-league?pos=all&level=0&lg=all&stats=pit&qual=10&type=1&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false&players=&sort=18,1&pageitems=2000000000&pg=0',\n",
    "        'bat_batted': 'https://www.fangraphs.com/leaders/minor-league?pos=all&level=0&lg=all&stats=bat&qual=10&type=2&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false&players=&sort=18,1&pageitems=2000000000&pg=0',\n",
    "        'bat_batted': 'https://www.fangraphs.com/leaders/minor-league?pos=all&level=0&lg=all&stats=bat&qual=10&type=2&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false&players=&sort=18,1&pageitems=2000000000&pg=0',\n",
    "        'pit_batted': 'https://www.fangraphs.com/leaders/minor-league?pos=all&level=0&lg=all&stats=pit&qual=10&type=2&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false&players=&sort=18,1&pageitems=2000000000&pg=0',       \n",
    "    }\n",
    "\n",
    "    # Set the download directory\n",
    "    if download_dir == 'default_path':\n",
    "        download_dir = r\"C:\\Users\\vile3\\Documents\\GitHub\\baseball_pred\\SCRAPE_FANGRAPHS\"\n",
    "\n",
    "    # Initialize the Options object\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "\n",
    "    # Set download preferences\n",
    "    chrome_options.add_experimental_option(\"prefs\", {\n",
    "        \"download.default_directory\": download_dir,\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True\n",
    "    })\n",
    "\n",
    "    # Initialize the Chrome driver with the options\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    # Construct the key for the URL dictionary\n",
    "    key = f'{data_type}_{data_level}'\n",
    "    url = urls.get(key, None)\n",
    "    if not url:\n",
    "        print(f\"Invalid data type or level: {data_type}, {data_level}\")\n",
    "        return\n",
    "\n",
    "    # Open the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the 'Export Data' button to be clickable\n",
    "    wait = WebDriverWait(driver, 30)\n",
    "    export_button = wait.until(EC.element_to_be_clickable((By.LINK_TEXT, 'Export Data')))\n",
    "    # Click the 'Export Data' button\n",
    "    export_button.click()\n",
    "\n",
    "    # Wait for the download to start\n",
    "    wait = WebDriverWait(driver, 5)\n",
    "\n",
    "    # Close the browser when done\n",
    "    driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "data_types_levels = [\n",
    "    ('bat', 'standard'),\n",
    "    ('bat', 'advanced'),\n",
    "    ('bat', 'batted'),\n",
    "    ('pit', 'standard'),\n",
    "    ('pit', 'advanced'),\n",
    "    ('pit', 'batted')\n",
    "]\n",
    "\n",
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "    # Rename the downloaded csv files to match their data_type and data_level\n",
    "for  data_type, data_level in data_types_levels:\n",
    "    download_dir = f\"C:\\\\Users\\\\vile3\\\\Documents\\\\GitHub\\\\baseball_pred\\\\SCRAPE_FANGRAPHS\\\\{data_type.capitalize()}\\\\{data_level.capitalize()}\"\n",
    "    download_fangraphs_data(data_type=data_type, data_level=data_level, download_dir=download_dir)\n",
    "    os.rename(f\"{download_dir}/fangraphs-minor-league-leaders.csv\", f\"{download_dir}/{data_type}_{data_level}.csv\")\n",
    "    \n",
    "\n",
    "# data_types_levels = [\n",
    "#     ('bat', 'standard'),\n",
    "#     ('bat', 'advanced'),\n",
    "#     ('bat', 'batted'),\n",
    "#     ('pit', 'standard'),\n",
    "#     ('pit', 'advanced'),\n",
    "#     ('pit', 'batted')\n",
    "# ]\n",
    "\n",
    "# def download_wrapper(args):\n",
    "#     data_type, data_level = args\n",
    "#     download_dir = f\"C:\\\\Users\\\\vile3\\\\Documents\\\\GitHub\\\\baseball_pred\\\\SCRAPE_FANGRAPHS\\\\{data_type.capitalize()}\\\\{data_level.capitalize()}\"\n",
    "#     download_fangraphs_data(data_type=data_type, data_level=data_level, download_dir=download_dir)\n",
    "\n",
    "# # Use ThreadPoolExecutor to run each download concurrently\n",
    "# with ThreadPoolExecutor(max_workers=len(data_types_levels)) as executor:\n",
    "#     executor.map(download_wrapper, data_types_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vile3\\Documents\\GitHub\\baseball_pred\\SCRAPE_FANGRAPHS\\Pit\\Batted\n"
     ]
    }
   ],
   "source": [
    "print(download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and converted mlbbat_standard to CSV.\n",
      "Downloaded and converted mlbbat_advanced to CSV.\n",
      "Downloaded and converted mlbbat_batted to CSV.\n",
      "Downloaded and converted mlbpit_standard to CSV.\n",
      "Downloaded and converted mlbpit_advanced to CSV.\n",
      "Downloaded and converted mlbpit_batted to CSV.\n",
      "Combined CSV saved at: C:\\Users\\vile3\\Documents\\GitHub\\baseball_pred\\SCRAPE_FANGRAPHS\\APIMlbbat\\combined_data.csv\n",
      "Combined CSV saved at: C:\\Users\\vile3\\Documents\\GitHub\\baseball_pred\\SCRAPE_FANGRAPHS\\APIMlbpit\\combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Function to clean HTML tags from data\n",
    "def clean_html(raw_html):\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Function to download data as JSON\n",
    "def download_api_data_json(url, download_dir, file_name):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        if 'application/json' in response.headers.get('Content-Type'):\n",
    "            os.makedirs(download_dir, exist_ok=True)\n",
    "            file_path = os.path.join(download_dir, f'{file_name}.json')\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(response.text)\n",
    "            return file_path\n",
    "        else:\n",
    "            print(\"The response content is not in JSON format.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to convert JSON to CSV and clean HTML\n",
    "import json\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to clean HTML tags from data\n",
    "def clean_html(raw_html):\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def convert_json_to_csv(json_file_path, csv_file_path):\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        # Parse the JSON data into a Python object\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # Access the list of data under the 'data' key\n",
    "    data = json_data.get('data', [])\n",
    "\n",
    "    # Check if data is a list and proceed\n",
    "    if isinstance(data, list) and data:\n",
    "        # Clean the HTML content in the JSON data\n",
    "        for entry in data:\n",
    "            for key, value in entry.items():\n",
    "                if isinstance(value, str) and ('<' in value and '>' in value):\n",
    "                    entry[key] = clean_html(value)\n",
    "\n",
    "        # Determine the fieldnames from the keys of the first record in your JSON data\n",
    "        fieldnames = data[0].keys()\n",
    "\n",
    "        # Write to CSV\n",
    "        with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for entry in data:\n",
    "                writer.writerow(entry)\n",
    "    else:\n",
    "        print(\"JSON data is not a list or is empty.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to merge CSV files\n",
    "def merge_csv_files(directory_path, unique_identifier):\n",
    "    # Create a pattern for all CSV files in the directory\n",
    "    csv_pattern = os.path.join(directory_path, '*.csv')\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    # Initialize an empty DataFrame for the combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for file in csv_files:\n",
    "        # Read the current CSV into a DataFrame\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        if combined_df.empty:\n",
    "            # If the combined DataFrame is empty, start with the first file's data\n",
    "            combined_df = df\n",
    "        else:\n",
    "            # Merge with an outer join to add new columns (stats) and rows (players)\n",
    "            combined_df = pd.merge(combined_df, df, on=unique_identifier, how='outer', suffixes=('', '_dup'))\n",
    "\n",
    "            # Handle duplicate columns if they exist\n",
    "            for column in combined_df.columns:\n",
    "                if '_dup' in column:\n",
    "                    original_column = column.replace('_dup', '')\n",
    "                    # If the original column is not NaN, keep it, otherwise use the duplicate column's value\n",
    "                    combined_df[original_column] = combined_df.apply(\n",
    "                        lambda row: row[original_column] if not pd.isnull(row[original_column]) else row[column],\n",
    "                        axis=1\n",
    "                    )\n",
    "                    # Drop the duplicate column\n",
    "                    combined_df.drop(columns=[column], inplace=True)\n",
    "\n",
    "    # Save the combined DataFrame to a new CSV file\n",
    "    combined_csv_path = os.path.join(directory_path, 'combined_data.csv')\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"Combined CSV saved at: {combined_csv_path}\")\n",
    "\n",
    "# Define the API URLs\n",
    "api_urls = {\n",
    "    'bat_standard': 'https://www.fangraphs.com/api/leaders/minor-league/data?pos=all&level=0&lg=all&stats=bat&qual=10&type=0&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false',\n",
    "    'bat_advanced': 'https://www.fangraphs.com/api/leaders/minor-league/data?pos=all&level=0&lg=all&stats=bat&qual=10&type=1&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false',\n",
    "    'bat_batted': 'https://www.fangraphs.com/api/leaders/minor-league/data?pos=all&level=0&lg=all&stats=bat&qual=10&type=2&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false',\n",
    "    'pit_standard': 'https://www.fangraphs.com/api/leaders/minor-league/data?pos=all&level=0&lg=all&stats=pit&qual=10&type=0&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false',\n",
    "    'pit_advanced': 'https://www.fangraphs.com/api/leaders/minor-league/data?pos=all&level=0&lg=all&stats=pit&qual=10&type=1&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false',\n",
    "    'pit_batted': 'https://www.fangraphs.com/api/leaders/minor-league/data?pos=all&level=0&lg=all&stats=pit&qual=10&type=2&team=&season=2006&seasonEnd=2023&org=all&ind=0&splitTeam=false',\n",
    "    'mlbbat_standard': 'https://www.fangraphs.com/api/leaders/major-league/data?age=&pos=all&stats=bat&lg=all&qual=10&season=2023&season1=2006&startdate=2023-03-01&enddate=2023-11-01&month=0&hand=&team=0&pageitems=2000000&pagenum=1&ind=0&rost=0&players=&type=0&postseason=&sortdir=default&sortstat=AVG',\n",
    "    'mlbbat_advanced': 'https://www.fangraphs.com/api/leaders/major-league/data?age=&pos=all&stats=bat&lg=all&qual=10&season=2023&season1=2006&startdate=2023-03-01&enddate=2023-11-01&month=0&hand=&team=0&pageitems=2000000&pagenum=1&ind=0&rost=0&players=&type=1&postseason=&sortdir=default&sortstat=wRC%2B',\n",
    "    'mlbbat_batted': 'https://www.fangraphs.com/api/leaders/major-league/data?age=&pos=all&stats=bat&lg=all&qual=10&season=2023&season1=2006&startdate=2023-03-01&enddate=2023-11-01&month=0&hand=&team=0&pageitems=2000000&pagenum=1&ind=0&rost=0&players=&type=2&postseason=&sortdir=default&sortstat=Hard%25',    'mlbpit_standard': 'https://www.fangraphs.com/api/leaders/major-league/data?pos=all&stats=pit&lg=all&qual=10&type=0&season=2006&seasonEnd=2023&ind=0',\n",
    "    'mlbpit_standard': 'https://www.fangraphs.com/api/leaders/major-league/data?age=&pos=all&stats=pit&lg=all&qual=10&season=2023&season1=2006&startdate=2023-03-01&enddate=2023-11-01&month=0&hand=&team=0&pageitems=2000000&pagenum=1&ind=0&rost=0&players=&type=0&postseason=&sortdir=default&sortstat=SO',\n",
    "    'mlbpit_advanced': 'https://www.fangraphs.com/api/leaders/major-league/data?age=&pos=all&stats=pit&lg=all&qual=10&season=2023&season1=2006&startdate=2023-03-01&enddate=2023-11-01&month=0&hand=&team=0&pageitems=2000000&pagenum=1&ind=0&rost=0&players=&type=1&postseason=&sortdir=default&sortstat=SIERA',\n",
    "    'mlbpit_batted': 'https://www.fangraphs.com/api/leaders/major-league/data?age=&pos=all&stats=pit&lg=all&qual=10&season=2023&season1=2006&startdate=2023-03-01&enddate=2023-11-01&month=0&hand=&team=0&pageitems=2000000&pagenum=1&ind=0&rost=0&players=&type=2&postseason=&sortdir=default&sortstat=Hard%25',\n",
    "}\n",
    "# Function to process selected data types\n",
    "def process_selected_data_types(selected_data_types):\n",
    "    for key in selected_data_types:\n",
    "        url = api_urls.get(key)\n",
    "        if url:\n",
    "            data_type, data_level = key.split('_')\n",
    "            download_dir = os.path.join(r'C:\\Users\\vile3\\Documents\\GitHub\\baseball_pred\\SCRAPE_FANGRAPHS', 'API' + data_type.capitalize())\n",
    "            file_name = f'{data_type}_{data_level}'\n",
    "            \n",
    "            # Download the data as JSON\n",
    "            json_file_path = download_api_data_json(url, download_dir, file_name)\n",
    "            \n",
    "            if json_file_path:\n",
    "                # Convert JSON to CSV\n",
    "                csv_file_path = os.path.join(download_dir, f'{file_name}.csv')\n",
    "                convert_json_to_csv(json_file_path, csv_file_path)\n",
    "                print(f\"Downloaded and converted {file_name} to CSV.\")\n",
    "        else:\n",
    "            print(f\"No URL found for {key}\")\n",
    "\n",
    "# Usage example for a single data type\n",
    "\n",
    "# process_selected_data_types(['bat_standard'])\n",
    "process_selected_data_types(['mlbbat_standard'])\n",
    "process_selected_data_types(['mlbbat_advanced'])\n",
    "process_selected_data_types(['mlbbat_batted'])\n",
    "process_selected_data_types(['mlbpit_standard'])\n",
    "process_selected_data_types(['mlbpit_advanced'])\n",
    "process_selected_data_types(['mlbpit_batted'])\n",
    "# Usage example for all data types\n",
    "# process_selected_data_types(api_urls.keys())\n",
    "\n",
    "# After processing the data types, you can merge the CSV files as before:\n",
    "bat_directory = r'C:\\Users\\vile3\\Documents\\GitHub\\baseball_pred\\SCRAPE_FANGRAPHS\\APIMlbbat'\n",
    "pit_directory = r'C:\\Users\\vile3\\Documents\\GitHub\\baseball_pred\\SCRAPE_FANGRAPHS\\APIMlbpit'\n",
    "\n",
    "merge_csv_files(bat_directory, 'playerid')\n",
    "merge_csv_files(pit_directory, 'playerid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
