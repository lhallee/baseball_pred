{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "9pxTo6ch5qYo",
        "outputId": "9c453c03-e2d5-41c6-b42d-40882531f5c0"
      },
      "outputs": [],
      "source": [
        "#def main_function(test_year):\n",
        "\n",
        "# The rest of your script can go here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "QhIfx4UEM1Hd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pybaseball in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.7)\n",
            "Requirement already satisfied: numpy>=1.13.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pybaseball) (1.26.2)\n",
            "Requirement already satisfied: pandas>=1.0.3 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pybaseball) (2.1.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pybaseball) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.18.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pybaseball) (2.31.0)\n",
            "Requirement already satisfied: lxml>=4.2.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pybaseball) (4.9.4)\n",
            "Requirement already satisfied: pyarrow>=1.0.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pybaseball) (14.0.2)\n",
            "Requirement already satisfied: pygithub>=1.51 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pybaseball) (2.1.1)\n",
            "Requirement already satisfied: scipy>=1.4.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pybaseball) (1.11.4)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pybaseball) (3.8.2)\n",
            "Requirement already satisfied: tqdm>=4.50.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pybaseball) (4.66.1)\n",
            "Requirement already satisfied: attrs>=20.3.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pybaseball) (23.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4>=4.4.0->pybaseball) (2.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=2.0.0->pybaseball) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=2.0.0->pybaseball) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=2.0.0->pybaseball) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=2.0.0->pybaseball) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=2.0.0->pybaseball) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=2.0.0->pybaseball) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=2.0.0->pybaseball) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=2.0.0->pybaseball) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.0.3->pybaseball) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.0.3->pybaseball) (2023.3)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pygithub>=1.51->pybaseball) (1.5.0)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyjwt[crypto]>=2.4.0->pygithub>=1.51->pybaseball) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pygithub>=1.51->pybaseball) (4.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pygithub>=1.51->pybaseball) (2.1.0)\n",
            "Requirement already satisfied: Deprecated in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pygithub>=1.51->pybaseball) (1.2.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.18.1->pybaseball) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.18.1->pybaseball) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.18.1->pybaseball) (2023.11.17)\n",
            "Requirement already satisfied: colorama in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.50.0->pybaseball) (0.4.6)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyjwt[crypto]>=2.4.0->pygithub>=1.51->pybaseball) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.4.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pynacl>=1.4.0->pygithub>=1.51->pybaseball) (1.16.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pybaseball) (1.16.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Deprecated->pygithub>=1.51->pybaseball) (1.16.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cffi>=1.4.1->pynacl>=1.4.0->pygithub>=1.51->pybaseball) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install pybaseball"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "UMzZjDAwM6KK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.4.0)\n",
            "Requirement already satisfied: imbalanced-learn in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade scikit-learn imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ekoD3VlRM7yy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: featureranker in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.1.2)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.8.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from featureranker) (3.8.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from featureranker) (1.26.2)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from featureranker) (2.1.3)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.3.2 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from featureranker) (1.4.0)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.11.2 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from featureranker) (1.11.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from featureranker) (4.66.1)\n",
            "Requirement already satisfied: xgboost<3.0.0,>=2.0.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from featureranker) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3.0.0,>=2.1.1->featureranker) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas<3.0.0,>=2.1.1->featureranker) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn<2.0.0,>=1.3.2->featureranker) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn<2.0.0,>=1.3.2->featureranker) (3.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm<5.0.0,>=4.66.1->featureranker) (0.4.6)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.8.1->featureranker) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install featureranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "xQ6PJjFjVQU5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "import pybaseball as pyb\n",
        "import pandas as pd\n",
        "import time\n",
        "import inspect\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch import nn\n",
        "import json\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import os\n",
        "# from automatic_betting import Starting_Lineup\n",
        "from featureranker.utils import view_data\n",
        "from featureranker.rankers import classification_ranking\n",
        "from featureranker.rankers import voting\n",
        "# from featureranker import view_data\n",
        "# from featureranker import classification_hyper_param_search\n",
        "# from featureranker import classification_ranking\n",
        "# from featureranker import voting\n",
        "# from featureranker import plot_ranking\n",
        "from bs4 import BeautifulSoup\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import glob\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import pickle\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "from sklearn.model_selection import KFold\n",
        "warnings.filterwarnings('ignore')\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Seed value\n",
        "seed_value = 42\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `torch` pseudo-random generator at a fixed value\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "# 5. Depending on whether you are using CUDA\n",
        "torch.cuda.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
        "\n",
        "# 6. Configure a new global `torch` default floating point tensor type\n",
        "# (optional, if you are using PyTorch)\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "# 7. For `torch.backends.cudnn`\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "# import pycuda.autoinit\n",
        "# import pycuda.gpuarray as gpuarray\n",
        "pyb.cache.enable()\n",
        "pyb.cache.config.cache_type='csv'\n",
        "pyb.cache.config.save()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "UQhweF660E5m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Check if the code is running in Google Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    save_path='/content/drive/My Drive/baseball_pred/pybaseball_v3_saved'\n",
        "    #Vegas Odds path\n",
        "    vegas_betting_paths='/content/drive/My Drive/baseball_pred/betting_odds/'\n",
        "    vegas_paths = [path for path in glob.glob(vegas_betting_paths + '*.xlsx')]\n",
        "\n",
        "    #Scrambled Odds path\n",
        "    scrambled_betting_paths='/content/drive/My Drive/baseball_pred/all_money_lines.csv'\n",
        "\n",
        "\n",
        "    #Per_game_data path\n",
        "    per_game_path='/content/drive/My Drive/baseball_pred/pybaseball/pybaseball/data/Lahman_MLB_per_game_data.csv'\n",
        "\n",
        "\n",
        "\n",
        "    milb_batter_path = '/content/drive/My Drive/baseball_pred/SCRAPE_FANGRAPHS/APIBat/bat_standard.csv'\n",
        "    milb_pitcher_path = '/content/drive/My Drive/baseball_pred/SCRAPE_FANGRAPHS/APIPit/pit_standard.csv'\n",
        "    mlb_batter_path = '/content/drive/My Drive/baseball_pred/SCRAPE_FANGRAPHS/APIMlbbat/mlbbat_standard.csv'\n",
        "    mlb_pitcher_path = '/content/drive/My Drive/baseball_pred/SCRAPE_FANGRAPHS/APIMlbpit/mlbpit_standard.csv'\n",
        "    scoring_full_all_years='/content/drive/My Drive/baseball_pred/scoring_full_all_years.pkl'\n",
        "    scoring_save_path='/content/drive/My Drive/baseball_pred/scoring_milb.pkl'\n",
        "    def save_results_format(year):\n",
        "      save_results=f'/content/drive/My Drive/baseball_pred/results/better_data_{year}.xlsx'\n",
        "      return save_results\n",
        "else:\n",
        "    # Code to run if not in Google Colab\n",
        "    # For example, set a local path for your files if on a local machine\n",
        "    # local_drive_path = 'd:/Users/vile3/Google Drive'\n",
        "    # if not os.path.exists(local_drive_path):\n",
        "        # os.makedirs(local_drive_path)\n",
        "        save_path='pybaseball_v3_saved'\n",
        "        vegas_betting_paths='./betting_odds/'\n",
        "        vegas_paths = [path for path in glob.glob(vegas_betting_paths + '*.xlsx')]\n",
        "\n",
        "        #Scrambled Odds path\n",
        "        scrambled_betting_paths='all_money_lines.csv'\n",
        "\n",
        "\n",
        "        #Per_game_data path\n",
        "        per_game_path='./pybaseball/pybaseball/data/Lahman_MLB_per_game_data.csv'\n",
        "\n",
        "\n",
        "\n",
        "        milb_batter_path = './SCRAPE_FANGRAPHS/APIBat/bat_standard.csv'\n",
        "        milb_pitcher_path = './SCRAPE_FANGRAPHS/APIPit/pit_standard.csv'\n",
        "        mlb_batter_path = './SCRAPE_FANGRAPHS/APIMlbbat/mlbbat_standard.csv'\n",
        "        mlb_pitcher_path = './SCRAPE_FANGRAPHS/APIMlbpit/mlbpit_standard.csv'\n",
        "        scoring_full_all_years='scoring_full_all_years.pkl'\n",
        "        scoring_save_path='scoring_milb.pkl'\n",
        "        def save_results_format(year):\n",
        "          save_results=f'./results/better_data_{year}.xlsx'\n",
        "          return save_results\n",
        "    # Now you can use local_drive_path as the base path for your file operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDQ9gqifVQU7"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: clicknium in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.2.2)\n",
            "Requirement already satisfied: pythonnet>=3.0.0rc1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clicknium) (3.0.3)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clicknium) (4.9.0)\n",
            "Requirement already satisfied: requests in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clicknium) (2.31.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clicknium) (5.9.6)\n",
            "Requirement already satisfied: clr-loader<0.3.0,>=0.2.6 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pythonnet>=3.0.0rc1->clicknium) (0.2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->clicknium) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->clicknium) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->clicknium) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->clicknium) (2023.11.17)\n",
            "Requirement already satisfied: cffi>=1.13 in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from clr-loader<0.3.0,>=0.2.6->pythonnet>=3.0.0rc1->clicknium) (1.16.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\vile3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cffi>=1.13->clr-loader<0.3.0,>=0.2.6->pythonnet>=3.0.0rc1->clicknium) (2.21)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install clicknium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Game_ID': '2024-07-14_KC_BOS_Game1', 'Home_Team': 'KC', 'Away_Team': 'BOS', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_OAK_PHI_Game1', 'Home_Team': 'OAK', 'Away_Team': 'PHI', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_NYY_BAL_Game1', 'Home_Team': 'NYY', 'Away_Team': 'BAL', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_MIA_CIN_Game1', 'Home_Team': 'MIA', 'Away_Team': 'CIN', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_LAD_DET_Game1', 'Home_Team': 'LAD', 'Away_Team': 'DET', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_COL_NYM_Game1', 'Home_Team': 'COL', 'Away_Team': 'NYM', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_CLE_TB_Game1', 'Home_Team': 'CLE', 'Away_Team': 'TB', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_TEX_HOU_Game1', 'Home_Team': 'TEX', 'Away_Team': 'HOU', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_PIT_CHW_Game1', 'Home_Team': 'PIT', 'Away_Team': 'CHW', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_WAS_MIL_Game1', 'Home_Team': 'WAS', 'Away_Team': 'MIL', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_CHC_STL_Game1', 'Home_Team': 'CHC', 'Away_Team': 'STL', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_MIN_SF_Game1', 'Home_Team': 'MIN', 'Away_Team': 'SF', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_SEA_LAA_Game1', 'Home_Team': 'SEA', 'Away_Team': 'LAA', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_TOR_ARI_Game1', 'Home_Team': 'TOR', 'Away_Team': 'ARI', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n",
            "{'Game_ID': '2024-07-14_ATL_SD_Game1', 'Home_Team': 'ATL', 'Away_Team': 'SD', 'Home_Odds': 'ML: Off', 'Away_Odds': 'ML: Off'}\n"
          ]
        }
      ],
      "source": [
        "from clicknium import clicknium as cc, locator\n",
        "from collections import defaultdict\n",
        "betting_date = '2024-07-14'\n",
        "url = f\"https://www.covers.com/sports/MLB/matchups?selectedDate={betting_date}\"\n",
        "ok = cc.chrome.open(url)\n",
        "team_elements = cc.find_elements(locator.covers.div_tb)\n",
        "team_scores = cc.find_elements(locator.covers.div_6)\n",
        "home_odds = cc.find_elements(locator.covers.span_ml_off)\n",
        "away_odds = cc.find_elements(locator.covers.span_ml_off)\n",
        "# Initialize a dictionary to keep track of games count\n",
        "games_count = defaultdict(int)\n",
        "\n",
        "# Lists to store extracted data\n",
        "games_data = []\n",
        "\n",
        "# Iterate over the elements to extract data and construct Game_ID\n",
        "for i in range(0, len(team_elements), 2):  # Step by 2 since teams are one after another\n",
        "    home_team_text = team_elements[i].get_text().strip().replace('\\n', '').replace('                ', ' ')\n",
        "    away_team_text = team_elements[i + 1].get_text().strip().replace('\\n', '').replace('                ', ' ')\n",
        "\n",
        "    home_odds_text = home_odds[i // 2].get_text().strip().replace('\\n', '').replace('ML:        ', 'ML: ')\n",
        "    away_odds_text = away_odds[i // 2].get_text().strip().replace('\\n', '').replace('ML:        ', 'ML: ')\n",
        "    \n",
        "    # Construct a key to track the number of games between the same teams on the same date\n",
        "    game_key = f\"{betting_date}_{home_team_text}_{away_team_text}\"\n",
        "    \n",
        "    # Increment the game count for this specific matchup\n",
        "    games_count[game_key] += 1\n",
        "    \n",
        "    # Construct the Game_ID using the date, teams, and the game count for uniqueness\n",
        "    game_id = f\"{betting_date}_{home_team_text}_{away_team_text}_Game{games_count[game_key]}\"\n",
        "    \n",
        "    # Store the extracted data along with the Game_ID\n",
        "    games_data.append({\n",
        "        \"Game_ID\": game_id,\n",
        "        \"Home_Team\": home_team_text,\n",
        "        \"Away_Team\": away_team_text,\n",
        "        \"Home_Odds\": home_odds_text,\n",
        "        \"Away_Odds\": away_odds_text\n",
        "    })\n",
        "\n",
        "# Example: Print the constructed Game_IDs and associated data\n",
        "for game in games_data:\n",
        "    print(game)\n",
        "\n",
        "ok.close()  # Close the browser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "xOV3PKKTVQU8"
      },
      "outputs": [],
      "source": [
        "# Check if CUDA is available\n",
        "\n",
        "# Move your model to the GPU\n",
        "# model = model.to(device)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# When you load your data, move it to the GPU\n",
        "# Assume inputs and labels are your input data and labels\n",
        "# inputs, labels = inputs.to(device), labels.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CEOkRzXVQU8"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Iznud8V5VQU8"
      },
      "outputs": [],
      "source": [
        "class vector_dataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = np.array(X)\n",
        "        self.y = np.array(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vec = torch.tensor(self.X[idx], dtype=torch.float)\n",
        "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
        "        return vec, label\n",
        "\n",
        "class neural_net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, hidden_num, output_size, dropout_rate):\n",
        "        super(neural_net, self).__init__()\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.hidden_num = hidden_num\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(hidden_num):\n",
        "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gelu(self.input_layer(x))\n",
        "        x = self.dropout(x)\n",
        "        for i in range(self.hidden_num):\n",
        "            x = self.gelu(self.hidden_layers[i](x))\n",
        "            x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "#CHECK TO SEE HOW GOOD MODEL IS\n",
        "def cross_validate(model, X, y, n_splits=5):\n",
        "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_indices, valid_indices in kfold.split(X):\n",
        "        # Split the data using .iloc for position-based indexing\n",
        "        X_train, X_valid = X.iloc[train_indices], X.iloc[valid_indices]\n",
        "        y_train, y_valid = y.iloc[train_indices], y.iloc[valid_indices]\n",
        "\n",
        "        # Convert to PyTorch datasets\n",
        "        train_dataset = vector_dataset(X_train, y_train)\n",
        "        valid_dataset = vector_dataset(X_valid, y_valid)\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "        valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "        # Define loss function and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "        # Train the model on the training data\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate the model on the validation data\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valid_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        accuracy = correct / total\n",
        "        scores.append(accuracy)\n",
        "\n",
        "    return scores\n",
        "\n",
        "def train_and_evaluate_model(model, train_loader, val_loader, test_loader, optimizer, criterion, scheduler, patience=5, num_epochs=50):\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_path = 'best_model.pth'  # File path for saving the best model\n",
        "    val_losses_per_epoch = []\n",
        "    # global label1,label2, label3, label4, label5, label6,model1, model2, model3, model_all\n",
        "    for param in model.parameters():\n",
        "        model_all=(param.device)\n",
        "    for epoch in range(num_epochs):  # number of epochs\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
        "            # label1=(inputs.device)\n",
        "            # label2=(labels.device)\n",
        "            # model1=(next(model.parameters()).device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        print(f'Training Loss: {avg_train_loss}')\n",
        "\n",
        "        model.eval()\n",
        "        valid_losses = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:  # Use validation data for validation\n",
        "                inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
        "                # label3=(inputs.device)\n",
        "                # label4=(labels.device)\n",
        "                # model2=(next(model.parameters()).device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.squeeze())\n",
        "\n",
        "                valid_losses.append(loss.item())\n",
        "\n",
        "        avg_valid_loss = np.mean(valid_losses)\n",
        "        print(f'Validation Loss: {avg_valid_loss}')\n",
        "\n",
        "        scheduler.step(avg_valid_loss)\n",
        "        # Save the model at each epoch\n",
        "        epoch_model_path = f'model_epoch_{epoch + 1}.pth'\n",
        "        torch.save(model.state_dict(), epoch_model_path)\n",
        "\n",
        "        # Update the best model if validation loss improves\n",
        "        if avg_valid_loss < best_loss:\n",
        "            best_loss = avg_valid_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print('Early stopping triggered')\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    # model.load_state_dict(torch.load(best_model_path))\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "\n",
        "    # Evaluate the best model\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:  # Use test data for final evaluation\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
        "\n",
        "            # label5=(inputs.device)\n",
        "            # label6=(labels.device)\n",
        "            # model3=(next(model.parameters()).device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate confusion matrix and classification report\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    class_report = classification_report(all_labels, all_preds,output_dict=True)\n",
        "    print('Confusion Matrix:')\n",
        "    print(conf_matrix)\n",
        "    print('Classification Report:')\n",
        "    print(class_report)\n",
        "    return conf_matrix, class_report\n",
        "\n",
        "import os\n",
        "\n",
        "def save_to_path(df, filename, folder=save_path):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    path = os.path.join(folder, f'{filename}.csv')\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        print(f'File {path} already exists.')\n",
        "    else:\n",
        "        df.to_csv(path, index=True)\n",
        "        print(f'{filename} saved to {path}')\n",
        "\n",
        "def process_per_game_data(per_game_data, columns_to_keep, train_years, betting_date):\n",
        "    # Create 'DateHmTmVisTm' column\n",
        "    per_game_data['Game_Number'] = per_game_data.groupby(['Date', 'HmTm', 'VisTm']).cumcount() + 1\n",
        "    per_game_data['Game_ID'] = (\n",
        "        per_game_data['Date'].astype(str) +\n",
        "        per_game_data['HmTm'] +\n",
        "        per_game_data['VisTm'] +\n",
        "        per_game_data['Game_Number'].astype(str)\n",
        "    )\n",
        "    # Reset the current index\n",
        "    per_game_data = per_game_data.reset_index()\n",
        "    # Set the new index\n",
        "    per_game_data.set_index('Game_ID', inplace=True)\n",
        "    # Drop unnecessary columns\n",
        "    per_game_data = per_game_data[columns_to_keep]\n",
        "    # Create 'winner' column\n",
        "    per_game_data['winner'] = np.where(per_game_data['HmRuns'] > per_game_data['VisRuns'], 1, 0)\n",
        "\n",
        "    # Truncate rows based on the lowest train year\n",
        "    lowest_train_year = min(train_years)\n",
        "    per_game_data = per_game_data[per_game_data.index.str[:4] >= str(lowest_train_year)]\n",
        "    \n",
        "    # Convert betting_date from 'yyyy-mm-dd' to 'yyyymmdd' to match the index format\n",
        "    betting_date_formatted = betting_date.replace('-', '')\n",
        "    betting_date_formatted = str(int(betting_date_formatted)-1)\n",
        "    # Truncate rows based on the betting date using the formatted index\n",
        "    # Since the index is 'yyyymmddhometeamvisteam', we only compare the 'yyyymmdd' part\n",
        "    per_game_data = per_game_data[per_game_data.index.str[:8] <= betting_date_formatted]\n",
        "    \n",
        "    return per_game_data\n",
        "\n",
        "def process_vegas_odds(paths):\n",
        "\n",
        "    xlsx_dataframes = []\n",
        "    for i, path in enumerate(paths):\n",
        "        temp_df = pd.read_excel(path, skiprows=0 if i > 0 else 0)\n",
        "        temp_df.columns = temp_df.columns.str.replace('[^a-zA-Z0-9]', '')\n",
        "        year = path[-9:-5]  # extract year from file name\n",
        "        if 'Date' in temp_df.columns:\n",
        "            temp_df['Date'] = year + temp_df['Date'].astype(str).str.zfill(4)  # format date as yyyymmdd\n",
        "        xlsx_dataframes.append(temp_df)\n",
        "\n",
        "    try:\n",
        "        xlsx_dataframes = pd.concat(xlsx_dataframes, ignore_index=True)\n",
        "    except pd.errors.InvalidIndexError:\n",
        "        print('Error: Reindexing only valid with uniquely valued Index objects')\n",
        "\n",
        "    xlsx_dataframes = xlsx_dataframes[['Date', 'VH', 'Team', 'Open']]\n",
        "    home_teams_df = xlsx_dataframes[xlsx_dataframes['VH'] == 'H'].copy()\n",
        "    visiting_teams_df = xlsx_dataframes[xlsx_dataframes['VH'] == 'V'].copy()\n",
        "\n",
        "    home_teams_df.rename(columns={'Date': 'date', 'Team': 'home_team', 'Open': 'home_open'}, inplace=True)\n",
        "    visiting_teams_df.rename(columns={'Date': 'date', 'Team': 'visiting_team', 'Open': 'visiting_open'}, inplace=True)\n",
        "\n",
        "    # Merge on 'date'\n",
        "    xlsx_dataframes = pd.concat([home_teams_df.reset_index(drop=True), visiting_teams_df.reset_index(drop=True)], axis=1)\n",
        "    xlsx_dataframes = xlsx_dataframes.loc[:,~xlsx_dataframes.columns.duplicated()]\n",
        "    xlsx_dataframes = xlsx_dataframes[['date', 'home_team', 'visiting_team','home_open','visiting_open']]\n",
        "    xlsx_dataframes['Game_Number'] = xlsx_dataframes.groupby(['date', 'home_team', 'visiting_team']).cumcount() + 1\n",
        "    xlsx_dataframes['Game_ID'] = (\n",
        "        xlsx_dataframes['date'].astype(str) +\n",
        "        xlsx_dataframes['home_team'] +\n",
        "        xlsx_dataframes['visiting_team'] +\n",
        "        xlsx_dataframes['Game_Number'].astype(str)\n",
        "    )\n",
        "\n",
        "    xlsx_dataframes.set_index('Game_ID', inplace=True)\n",
        "\n",
        "    xlsx_dataframes.drop(['Game_Number','date','home_team','visiting_team'], axis=1, inplace=True)\n",
        "    print(xlsx_dataframes)\n",
        "\n",
        "    return xlsx_dataframes\n",
        "\n",
        "import time\n",
        "from requests.exceptions import RequestException\n",
        "\n",
        "def fetch_data_with_retry(fetch_func, data_type, max_retries=5, retry_delay=3):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            data = fetch_func(True)\n",
        "            print(f\"{data_type} data fetched successfully.\")\n",
        "            return data\n",
        "        except RequestException as e:\n",
        "            retries += 1\n",
        "            print(f\"Attempt {retries} failed with error: {e}. Retrying after {retry_delay} seconds...\")\n",
        "            time.sleep(retry_delay)\n",
        "            retry_delay *= 2  # Exponential backoff\n",
        "    print(f\"Failed to fetch {data_type} data after {max_retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "def process_scrambled_odds(df):\n",
        "    df['Game_Number'] = df.groupby(['date', 'home_team', 'visiting_team']).cumcount() + 1\n",
        "    df['Game_ID'] = (\n",
        "        df['date'].astype(str) +\n",
        "        df['home_team'] +\n",
        "        df['visiting_team'] +\n",
        "        df['Game_Number'].astype(str)\n",
        "    )\n",
        "\n",
        "    df.set_index('Game_ID', inplace=True)\n",
        "\n",
        "    # Fix: Correct the drop method usage by specifying the column indices correctly\n",
        "    columns_to_drop = df.columns[[0, 1]]\n",
        "    print(\"Dropping columns:\", columns_to_drop)\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "    return df\n",
        "\n",
        "def remove_columns_with_nan(df, NaN_cutoff_percentage):\n",
        "    NaN_cutoff = NaN_cutoff_percentage / 100.0\n",
        "    return df.loc[:, df.isnull().mean() < NaN_cutoff]\n",
        "\n",
        "def weighted_average(group, weights):\n",
        "    return pd.Series(\n",
        "        {col: np.average(group[col], weights=weights.loc[group.index]) for col in group.columns}\n",
        "    )\n",
        "\n",
        "# def replace_ids_with_stats_and_catalog_missing(per_game_data, player_id_columns, player_data_dicts, years):\n",
        "    # Ensure years is a list\n",
        "    if isinstance(years, int):\n",
        "        years = [years]\n",
        "\n",
        "    # Convert years to strings\n",
        "    years_str = [str(year) for year in years]\n",
        "\n",
        "    # Filter rows where the first 4 characters of the index are in years_str\n",
        "    per_game_data = per_game_data[per_game_data.index.str.slice(0, 4).isin(years_str)]\n",
        "    missing_player_ids = {}\n",
        "\n",
        "    for column in player_id_columns:\n",
        "        player_type = 'pitcher' if 'PchID' in column else 'batter'\n",
        "        player_data_dict = player_data_dicts[player_type]\n",
        "\n",
        "        # Map player IDs to stats\n",
        "\n",
        "        per_game_data[column] = per_game_data[column].map(player_data_dict)\n",
        "        # print(per_game_data[column])\n",
        "        # Split the stats dictionary into separate columns\n",
        "        stats_df = per_game_data[column].apply(pd.Series)\n",
        "        stats_df.columns = [f\"{column}_{col}\" for col in stats_df.columns]\n",
        "\n",
        "        # Concatenate the original DataFrame with the new columns\n",
        "        per_game_data = pd.concat([per_game_data, stats_df], axis=1)\n",
        "\n",
        "        # Check for missing player IDs\n",
        "        missing_ids = per_game_data[per_game_data[column].isna()].index.unique()\n",
        "        for missing_id in missing_ids:\n",
        "            if missing_id not in missing_player_ids:\n",
        "                missing_player_ids[missing_id] = []\n",
        "            missing_player_ids[missing_id].extend(\n",
        "                per_game_data[per_game_data[column] == missing_id].index.tolist()\n",
        "            )\n",
        "\n",
        "        # Drop the original player_ID column\n",
        "        per_game_data.drop(column, axis=1, inplace=True)\n",
        "\n",
        "    # Remove duplicates from the Game_ID lists and sort them\n",
        "    for player_id, game_ids in missing_player_ids.items():\n",
        "        missing_player_ids[player_id] = sorted(set(game_ids))\n",
        "\n",
        "    return per_game_data, missing_player_ids\n",
        "\n",
        "# def replace_player_ids_with_stats(per_game_data, player_data, player_id_columns):\n",
        "    # Calculate averages\n",
        "    player_avg = player_data.groupby(player_data.index).mean()\n",
        " \n",
        "\n",
        "\n",
        "# Continue with the rest of your code...\n",
        "\n",
        "    # Create a dictionary for each stat\n",
        "    stat_dicts = {stat: player_avg[stat].to_dict() for stat in player_avg.columns}\n",
        "\n",
        "    for column in player_id_columns:\n",
        "        # Replace the player_IDs in the column with the stats\n",
        "        for stat, stat_dict in stat_dicts.items():\n",
        "            per_game_data[column + '_' + stat] = per_game_data[column].map(stat_dict)\n",
        "\n",
        "        # Drop the original player_ID column\n",
        "        per_game_data.drop(column, axis=1, inplace=True)\n",
        "\n",
        "    return per_game_data\n",
        "\n",
        "# def replace_ids_with_stats_and_catalog_missing(per_game_data, player_id_columns, player_data_dicts):\n",
        "#     missing_player_ids = {}\n",
        "\n",
        "#     for column in player_id_columns:\n",
        "#         player_type = 'pitcher' if 'PchID' in column else 'batter'\n",
        "#         player_data_dict = player_data_dicts[player_type]\n",
        "\n",
        "#         # Preparing a DataFrame from player_data_dict for easier merging\n",
        "#         stats_df = pd.DataFrame.from_dict(player_data_dict, orient='index').reset_index().rename(columns={'index': column})\n",
        "        \n",
        "#         # Merging the stats into the main DataFrame\n",
        "#         per_game_data = per_game_data.merge(stats_df, on=column, how='left', suffixes=('', f'_{column}_stats'))\n",
        "\n",
        "#         # Identifying missing player IDs\n",
        "#         missing_ids_mask = per_game_data.loc[:, per_game_data.columns.str.contains(f'_{column}_stats')].isna().any(axis=1)\n",
        "#         missing_ids = per_game_data.loc[missing_ids_mask, column].dropna().unique()\n",
        "        \n",
        "#         for missing_id in missing_ids:\n",
        "#             if missing_id not in missing_player_ids:\n",
        "#                 missing_player_ids[missing_id] = []\n",
        "#             missing_player_ids[missing_id].extend(\n",
        "#                 per_game_data[per_game_data[column] == missing_id].index.tolist()\n",
        "#             )\n",
        "\n",
        "#         # Drop the original player_ID column\n",
        "#         per_game_data.drop(column, axis=1, inplace=True)\n",
        "\n",
        "#     # Remove duplicates from the Game_ID lists and sort them\n",
        "#     for player_id, game_ids in missing_player_ids.items():\n",
        "#         missing_player_ids[player_id] = sorted(set(game_ids))\n",
        "\n",
        "#     return per_game_data, missing_player_ids\n",
        "\n",
        "def replace_ids_with_stats_and_catalog_missing(per_game_data, player_id_columns, player_data_dicts):\n",
        "    missing_player_ids = {}\n",
        "\n",
        "    # Temporarily add the index as a column to preserve it through the merge\n",
        "    per_game_data = per_game_data.reset_index().rename(columns={'index': 'Game_ID'})\n",
        "\n",
        "    for column in player_id_columns:\n",
        "        player_type = 'pitcher' if 'PchID' in column else 'batter'\n",
        "        player_data_dict = player_data_dicts[player_type]\n",
        "\n",
        "        # Preparing a DataFrame from player_data_dict for easier merging\n",
        "        stats_df = pd.DataFrame.from_dict(player_data_dict, orient='index').reset_index().rename(columns={'index': column})\n",
        "        \n",
        "        # Merging the stats into the main DataFrame\n",
        "        per_game_data = per_game_data.merge(stats_df, on=column, how='left', suffixes=('', f'_{column}_stats'))\n",
        "\n",
        "        # Identifying missing player IDs\n",
        "        missing_ids_mask = per_game_data.loc[:, per_game_data.columns.str.contains(f'_{column}_stats')].isna().any(axis=1)\n",
        "        missing_ids = per_game_data.loc[missing_ids_mask, column].dropna().unique()\n",
        "        \n",
        "        for missing_id in missing_ids:\n",
        "            if missing_id not in missing_player_ids:\n",
        "                missing_player_ids[missing_id] = []\n",
        "            missing_player_ids[missing_id].extend(\n",
        "                per_game_data[per_game_data[column] == missing_id]['Game_ID'].tolist()\n",
        "            )\n",
        "\n",
        "        # Drop the original player_ID column\n",
        "        per_game_data.drop(column, axis=1, inplace=True)\n",
        "\n",
        "    # Remove duplicates from the Game_ID lists and sort them\n",
        "    for player_id, game_ids in missing_player_ids.items():\n",
        "        missing_player_ids[player_id] = sorted(set(game_ids))\n",
        "\n",
        "    # Set 'Game_ID' back as the index\n",
        "    per_game_data = per_game_data.set_index('Game_ID')\n",
        "\n",
        "    return per_game_data, missing_player_ids\n",
        "\n",
        "def label_encode(df):\n",
        "    le = LabelEncoder()\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = le.fit_transform(df[col])\n",
        "    return df\n",
        "\n",
        "def remove_excess_player_columns(player_data,columns_to_remove):\n",
        "    player_data = player_data.drop(columns=columns_to_remove)\n",
        "    return player_data\n",
        "\n",
        "def split_dataframe_data(per_game_data_reduced, train_years, validation_year, test_year):\n",
        "    # Create a mask for the training set\n",
        "    train_years = [str(year) for year in train_years]\n",
        "    validation_year = str(validation_year)\n",
        "    test_year = str(test_year)\n",
        "    train_mask = per_game_data_reduced.index.str.contains('|'.join(train_years))\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    X_train = per_game_data_reduced[train_mask].drop(['winner', 'home_odds', 'visiting_odds', 'HmRuns', 'VisRuns'], axis=1)\n",
        "    y_train = per_game_data_reduced.loc[train_mask, 'winner']\n",
        "\n",
        "    # Create a mask for the validation set\n",
        "    validation_mask = per_game_data_reduced.index.str.contains(validation_year)\n",
        "    X_validation = per_game_data_reduced[validation_mask].drop(['winner', 'home_odds', 'visiting_odds', 'HmRuns', 'VisRuns'], axis=1)\n",
        "    y_validation = per_game_data_reduced.loc[validation_mask, 'winner']\n",
        "\n",
        "    # Create a mask for the test set\n",
        "    test_mask = per_game_data_reduced.index.str.contains(test_year)\n",
        "    X_test = per_game_data_reduced[test_mask].drop(['winner', 'home_odds', 'visiting_odds', 'HmRuns', 'VisRuns'], axis=1)\n",
        "    y_test = per_game_data_reduced.loc[test_mask, 'winner']\n",
        "\n",
        "    return X_train, y_train, X_validation, y_validation, X_test, y_test\n",
        "\n",
        "def split_data_by_year(df, train_years, validation_year, test_year):\n",
        "    # Convert train_years and test_year to integers for the comparison\n",
        "    train_years_int = [int(year) for year in train_years]\n",
        "    test_year_int = int(test_year)\n",
        "\n",
        "    # Extract the year from the 'Game_ID' column\n",
        "    df['year'] = df.index.str[:4].astype(int)\n",
        "\n",
        "    # Create masks for splitting the data based on 'year'\n",
        "    train_mask = df['year'].isin(train_years_int)\n",
        "    val_mask = df['year'] == validation_year\n",
        "    test_mask = df['year'] == test_year_int\n",
        "\n",
        "    # Split the data\n",
        "    train_data = df[train_mask]\n",
        "    val_data = df[val_mask]\n",
        "    test_data = df[test_mask]\n",
        "\n",
        "    # Drop the 'year' column as it's no longer needed\n",
        "    df.drop('year', axis=1, inplace=True)\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def split_data(X, y, years):\n",
        "    # Check if 'years' is a list, if not, make it a list\n",
        "    if not isinstance(years, list):\n",
        "        years = [years]\n",
        "\n",
        "    # Convert all elements in 'years' to strings\n",
        "    years = [str(year) for year in years]\n",
        "\n",
        "    # Create a mask for the specified years\n",
        "    year_mask = X.index.str.contains('|'.join(years))\n",
        "\n",
        "    # Split the data for the specified years\n",
        "    X_years = X[year_mask]\n",
        "    y_years = y[year_mask]\n",
        "\n",
        "    return X_years, y_years\n",
        "\n",
        "def predict_game_outcome(game_ids, game_data, model, augment=False, base_bet=100, scaler=1):\n",
        "    num_games = len(game_ids)\n",
        "    results, bets, probss, skipped = [None]*num_games, [None]*num_games, [None]*num_games, [None]*num_games\n",
        "\n",
        "  \n",
        "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # model.eval()\n",
        "    # model.to(device)  # Move your model to the GPU\n",
        "    for i in range(num_games):\n",
        "        try:\n",
        "            game_vector = torch.tensor([game_data[i]], dtype=torch.float).to(device)\n",
        "            # game_vector = torch.tensor([game_data[i]], dtype=torch.float)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(game_vector)\n",
        "                probs = logits.softmax(dim=-1)\n",
        "                _, pred = torch.max(logits, 1)\n",
        "                pred = pred.item()\n",
        "                prob = probs[0][pred].item()\n",
        "            results[i] = (game_ids[i], 'True' if pred else 'False')\n",
        "            bets[i] = base_bet * prob * scaler if augment else base_bet\n",
        "            probss[i] = prob\n",
        "        except:\n",
        "            skipped.append(i)\n",
        "    return results, bets, skipped, probss\n",
        "\n",
        "def Impute(df, method):\n",
        "    # Create an imputer instance\n",
        "    imputer = SimpleImputer(strategy=method)\n",
        "    # Fit and transform the DataFrame, but keep the index\n",
        "    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns, index=df.index)\n",
        "    return df_imputed\n",
        "\n",
        "class Better:\n",
        "    def __init__(self, initial_wallet=1000):\n",
        "        self.wallet = initial_wallet\n",
        "        self.wallet_history = [initial_wallet]  # Store the initial wallet balance\n",
        "        self.bet_history = []\n",
        "    def bet(self, game_indices, bet_on_home_team, amount_bet, game_data, prob, augment=False, scaler=1):\n",
        "        # Ensure the bettor has enough money in the wallet\n",
        "        amount_bet = max(self.wallet * 0.01, 10)\n",
        "        # amount_bet = 10\n",
        "        amount_bet = amount_bet * prob * scaler if augment else amount_bet\n",
        "        if amount_bet > self.wallet:\n",
        "            print(\"Insufficient funds for this bet.\")\n",
        "            return\n",
        "\n",
        "        # Retrieve the game data\n",
        "        games = game_data.loc[game_indices]\n",
        "\n",
        "        # If games is a DataFrame, iterate over its rows\n",
        "        if isinstance(games, pd.DataFrame):\n",
        "            game_iter = games.iterrows()\n",
        "        else:  # If games is a Series (single row), create a one-item iterator\n",
        "            game_iter = [(game_indices, games)]\n",
        "\n",
        "        for _, game in game_iter:\n",
        "            # Retrieve the odds\n",
        "            home_odds = game['home_odds']\n",
        "            away_odds = game['visiting_odds']\n",
        "\n",
        "            # Determine the odds based on the team bet on\n",
        "            odds = home_odds if bet_on_home_team else away_odds\n",
        "\n",
        "            # Determine if the bet was a win or loss\n",
        "            bet_won = (game['winner'] == bet_on_home_team)\n",
        "\n",
        "            # Calculate the amount gained or lost\n",
        "            if bet_won:\n",
        "                if odds > 0:\n",
        "                    # For positive odds, the gain is the odds * the amount bet / 100\n",
        "                    gain = amount_bet * odds / 100\n",
        "                else:\n",
        "                    # For negative odds, the gain is the amount bet / (odds / -100)\n",
        "                    gain = amount_bet / (odds / -100)\n",
        "                self.wallet += gain\n",
        "            else:\n",
        "                # If the bet was lost, the loss is the amount bet\n",
        "                self.wallet -= amount_bet\n",
        "\n",
        "            # Store the new wallet balance\n",
        "            self.wallet_history.append(self.wallet)\n",
        "            self.bet_history.append(amount_bet)\n",
        "        return self.wallet\n",
        "\n",
        "def combine_odds(per_game_data, vegas_odds, scrambled_odds):\n",
        "    # Merge vegas_odds and scrambled_odds into per_game_data\n",
        "    per_game_data = per_game_data.merge(vegas_odds, how='left', left_index=True, right_index=True)\n",
        "    per_game_data = per_game_data.merge(scrambled_odds, how='left', left_index=True, right_index=True, suffixes=('_vegas', '_scrambled'))\n",
        "\n",
        "    # Create new columns 'home_odds' and 'visiting_odds' where vegas_odds takes precedence\n",
        "    per_game_data['home_odds'] = per_game_data['home_open_vegas'].combine_first(per_game_data['home_open_scrambled'])\n",
        "    per_game_data['visiting_odds'] = per_game_data['visiting_open_vegas'].combine_first(per_game_data['visiting_open_scrambled'])\n",
        "\n",
        "    # Fill any remaining NaNs with -110\n",
        "    per_game_data['home_odds'].fillna(-110, inplace=True)\n",
        "    per_game_data['visiting_odds'].fillna(-110, inplace=True)\n",
        "\n",
        "    # Drop the original odds columns\n",
        "    per_game_data.drop(columns=['home_open_vegas', 'visiting_open_vegas', 'home_open_scrambled', 'visiting_open_scrambled'], inplace=True)\n",
        "\n",
        "    # Reset the index before returning\n",
        "\n",
        "\n",
        "    # return per_game_data[['home_odds', 'visiting_odds']]\n",
        "    return per_game_data\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import shuffle\n",
        "import copy\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def permutation_importance(model, loader, criterion, metric):\n",
        "    model.eval()\n",
        "    original_score = metric(model, loader, criterion)\n",
        "    importances = []\n",
        "    for i in range(loader.dataset.X.size(1)):  # Assuming 'X' is the attribute storing your input data\n",
        "        temp = loader.dataset.X.clone()\n",
        "        temp[:, i] = torch.randperm(temp.size(0))\n",
        "        temp_dataset = vector_dataset(temp, loader.dataset.y)  # Assuming 'y' is the attribute storing your labels\n",
        "        temp_loader = DataLoader(temp_dataset, batch_size=64, shuffle=False)\n",
        "        score = metric(model, temp_loader, criterion)\n",
        "        importances.append(original_score - score)\n",
        "    return importances\n",
        "\n",
        "def automated_feature_selection(model, train_loader, val_loader, test_loader, optimizer, criterion, metric, patience=5, num_epochs=50):\n",
        "    best_score = 0\n",
        "    best_model = None\n",
        "    important_features = list(range(len(train_loader.dataset[0][0])))\n",
        "    while len(important_features) > 0:\n",
        "        # Train the model\n",
        "        conf_matrix, class_report = train_and_evaluate_model(model, train_loader, val_loader, test_loader, optimizer, criterion, patience, num_epochs)\n",
        "        # Calculate the score\n",
        "        score = class_report['accuracy']  # Assuming class_report is a dictionary with 'accuracy' key\n",
        "        # If the score has improved, update the best score and best model\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_model = copy.deepcopy(model)\n",
        "        else:\n",
        "            # If the score has not improved, add the last removed feature back and break the loop\n",
        "            important_features.append(last_removed_feature)\n",
        "            break\n",
        "        # Calculate the permutation importance\n",
        "        importances = permutation_importance(model, val_loader.dataset.tensors[0][:, important_features], val_loader.dataset.tensors[1], metric)\n",
        "        # Remove the least important feature\n",
        "        last_removed_feature = important_features.pop(np.argmin(importances))\n",
        "    return best_model, important_features\n",
        "# usage\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer, MaxAbsScaler, PowerTransformer, QuantileTransformer\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer, MaxAbsScaler, PowerTransformer, QuantileTransformer\n",
        "\n",
        "def scale_data(X, method='minmax'):\n",
        "    \"\"\"\n",
        "    Scales the data using various scaling methods.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Data as a pandas DataFrame.\n",
        "    - method: The scaling method to use ('minmax', 'standard', 'robust', 'normalizer', 'maxabs', 'power', 'quantile').\n",
        "\n",
        "    Returns:\n",
        "    - X_scaled: Scaled data as a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    scalers = {\n",
        "        'minmax': MinMaxScaler(),\n",
        "        'standard': StandardScaler(),\n",
        "        'robust': RobustScaler(),\n",
        "        'normalizer': Normalizer(),\n",
        "        'maxabs': MaxAbsScaler(),\n",
        "        'power': PowerTransformer(),\n",
        "        'quantile': QuantileTransformer(output_distribution='normal')\n",
        "    }\n",
        "\n",
        "    if method not in scalers:\n",
        "        raise ValueError(f\"Method should be one of {list(scalers.keys())}\")\n",
        "\n",
        "    scaler = scalers[method]\n",
        "\n",
        "    # Fit and transform the data.\n",
        "    X_scaled = pd.DataFrame(scaler.fit_transform(X.values), columns=X.columns, index=X.index)\n",
        "\n",
        "    return X_scaled\n",
        "\n",
        "    from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def process_data(df, labels, thresh=0.8, columns_to_drop=None):\n",
        "    # Ensure labels is a string and not a column itself\n",
        "    if not isinstance(labels, str):\n",
        "        raise ValueError(\"labels parameter should be a string representing the column name of the target variable\")\n",
        "\n",
        "    # Separate the target variable\n",
        "    y = df[labels].copy()\n",
        "    df_clean = df.drop(columns=columns_to_drop + [labels] if columns_to_drop is not None else [labels])\n",
        "\n",
        "    # Impute missing values for numerical columns\n",
        "    num_columns = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
        "    num_imputer = SimpleImputer(strategy='mean')\n",
        "    df_clean[num_columns] = num_imputer.fit_transform(df_clean[num_columns])\n",
        "\n",
        "    # Impute missing values for categorical columns\n",
        "    cat_columns = df_clean.select_dtypes(include=['object', 'string', 'bool']).columns\n",
        "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    df_clean[cat_columns] = cat_imputer.fit_transform(df_clean[cat_columns])\n",
        "\n",
        "    # Apply threshold for dropping columns with too many NAs\n",
        "    threshold = thresh * len(df)\n",
        "    df_clean = df_clean.dropna(axis=1, thresh=threshold)\n",
        "\n",
        "    # Label encoding for categorical columns\n",
        "    le = LabelEncoder()\n",
        "    for column in cat_columns:\n",
        "        # Ensure no NaN values are present before label encoding\n",
        "        df_clean[column] = le.fit_transform(df_clean[column])\n",
        "\n",
        "    # Concatenate the cleaned df_clean with the target y\n",
        "    combined = pd.concat([df_clean, y], axis=1)\n",
        "\n",
        "    # Drop rows with any remaining NaN values\n",
        "    combined_clean = combined.dropna()\n",
        "\n",
        "    # Separate the features and target again\n",
        "    df_clean = combined_clean.drop(columns=[labels])\n",
        "    y = combined_clean[labels]\n",
        "\n",
        "    # Convert boolean to int if necessary\n",
        "    if y.dtype == 'boolean':\n",
        "        y = y.astype(int)\n",
        "\n",
        "    return df_clean, y\n",
        "\n",
        "\n",
        "def aggregate_player_data(data, playerid):\n",
        "    # Set the playerid as the index if it's not already\n",
        "    # if isinstance(years, int):\n",
        "    #     years = [years]\n",
        "    # else:\n",
        "    #     years = list(map(int, years))\n",
        "    # data = data[data['year_ID'].isin(years)]\n",
        "    numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
        "    categorical_cols = data.select_dtypes(exclude=['int64', 'float64']).columns\n",
        "\n",
        "    # Impute numerical columns with mean\n",
        "    data[numerical_cols] = Impute(data[numerical_cols], 'mean')\n",
        "\n",
        "    # Impute categorical columns with the most frequent value\n",
        "    data[categorical_cols] = Impute(data[categorical_cols], 'most_frequent')\n",
        "\n",
        "    # Group by playerid and aggregate: mean for numerical columns, most frequent for categorical columns\n",
        "    data_aggregated = data.groupby(playerid).agg({**{col: 'mean' for col in numerical_cols}, \n",
        "                                                 **{col: lambda x: x.mode()[0] if not x.empty else None for col in categorical_cols}})\n",
        "\n",
        "    # Apply label encoding to categorical columns\n",
        "    data_aggregated = label_encode_columns(data_aggregated, categorical_cols)\n",
        " \n",
        "    return data_aggregated\n",
        "\n",
        "# Assuming batter_data and pitcher_data are pandas DataFrames and playerid is the column with player IDs\n",
        "# If you need to label encode the categorical columns after aggregation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Function to label encode categorical columns\n",
        "def label_encode_columns(df, categorical_columns):\n",
        "    le = LabelEncoder()\n",
        "    for col in categorical_columns:\n",
        "        # Fill NaN with a placeholder string and encode\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "    return df\n",
        "\n",
        "def prepare_data(batter_data, pitcher_data, per_game_data, playerid, years):\n",
        "    # Aggregate the player data for the specified years\n",
        "    batter_aggregated = aggregate_player_data(batter_data, playerid, years)\n",
        "    pitcher_aggregated = aggregate_player_data(pitcher_data, playerid, years)\n",
        "\n",
        "    # Check if the index has a 0 value and if so, remove the corresponding row\n",
        "    if 0 in batter_aggregated.index:\n",
        "        batter_aggregated.drop(index=0, inplace=True)\n",
        "    if 0 in pitcher_aggregated.index:\n",
        "        pitcher_aggregated.drop(index=0, inplace=True)\n",
        "\n",
        "\n",
        "    pitcher_columns = ['HmStPchID', 'VisStPchID']\n",
        "    batter_columns = ['HmBat1ID', 'HmBat2ID', 'HmBat3ID', 'HmBat4ID', 'HmBat5ID', 'HmBat6ID', 'HmBat7ID', 'HmBat8ID', 'HmBat9ID', 'VisBat1ID', 'VisBat2ID', 'VisBat3ID', 'VisBat4ID', 'VisBat5ID', 'VisBat6ID', 'VisBat7ID', 'VisBat8ID', 'VisBat9ID']\n",
        "\n",
        "    # Create separate dictionaries for the data\n",
        "    player_data_dictionary = {\n",
        "        'pitcher': pitcher_aggregated.to_dict('index'),\n",
        "        'batter': batter_aggregated.to_dict('index')\n",
        "    }\n",
        "\n",
        "    # Call the function with the player ID columns\n",
        "    player_id_columns = pitcher_columns + batter_columns\n",
        "\n",
        "    # Replace player IDs with stats and catalog missing IDs\n",
        "    per_game_finished, missing_player_ids = replace_ids_with_stats_and_catalog_missing(per_game_data, player_id_columns, player_data_dictionary)\n",
        "\n",
        "    return per_game_finished, missing_player_ids\n",
        "\n",
        "def list_old_bwar_data():\n",
        "    # The URL of the page where the zip files are listed\n",
        "    url = 'https://www.baseball-reference.com/data/'\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        # Find all the <a> tags in the HTML\n",
        "        a_tags = soup.find_all('a')\n",
        "        \n",
        "        # Filter out the <a> tags that link to .zip files and prepend the base URL to form a complete URL\n",
        "        zip_links = [url + a['href'] for a in a_tags if a['href'].endswith('.zip')]\n",
        "        \n",
        "        # Print out the list of zip file links\n",
        "        # for link in zip_links:\n",
        "            # print(link)\n",
        "    else:\n",
        "        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
        "    return zip_links\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def find_most_recent_zip(zip_links, input_date):\n",
        "    from datetime import datetime  # Ensure datetime is imported\n",
        "\n",
        "    # Convert input_date string to a datetime object\n",
        "    input_date = datetime.strptime(input_date, '%Y-%m-%d')\n",
        "    \n",
        "    # Initialize variable to keep track of the most recent date before input_date\n",
        "    most_recent_date = None\n",
        "    most_recent_link = None\n",
        "    \n",
        "    # Iterate over the list of zip links\n",
        "    for link in zip_links:\n",
        "        # Extract the date from the link (assuming the date is in the format YYYY-MM-DD)\n",
        "        parts = link.split('/')[-1].split('-')\n",
        "        link_date_str = '-'.join(parts[1:4]).replace('.zip', '')\n",
        "        try:\n",
        "            # Convert the extracted date string to a datetime object\n",
        "            link_date = datetime.strptime(link_date_str, '%Y-%m-%d')\n",
        "            \n",
        "            # Check if this date is more recent than the current most_recent_date and before the input_date\n",
        "            if (most_recent_date is None or link_date > most_recent_date) and link_date < input_date:\n",
        "                most_recent_date = link_date\n",
        "                most_recent_link = link\n",
        "        except ValueError:\n",
        "            # If there is a ValueError, it means the conversion to a datetime object failed, likely due to incorrect format\n",
        "            continue  # Skip this link if the date cannot be parsed\n",
        "    \n",
        "    # Return the most recent link\n",
        "    return most_recent_link\n",
        "\n",
        "def scrape_old_bwar_data(most_recent_link):\n",
        "    # URL of the zip file\n",
        "    zip_file_url = most_recent_link\n",
        "\n",
        "    # The path to the folder where you want to save the extracted files\n",
        "    # Make sure this folder exists or create it with os.makedirs()\n",
        "    download_folder = os.path.join(os.getcwd(), 'old_bwar_data')\n",
        "    if not os.path.exists(download_folder):\n",
        "        os.makedirs(download_folder)\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(zip_file_url)\n",
        "\n",
        "    # Initialize a list to hold the content of each text file\n",
        "    text_files_content = []\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Use BytesIO to read the downloaded zip file\n",
        "        zip_file_bytes = io.BytesIO(response.content)\n",
        "\n",
        "        # Open the zip file\n",
        "        with zipfile.ZipFile(zip_file_bytes, 'r') as zip_ref:\n",
        "            # Extract all the contents into the specified directory\n",
        "            zip_ref.extractall(download_folder)\n",
        "            # List all the file names in the zip file\n",
        "            for file_name in zip_ref.namelist():\n",
        "                # Check if the file is a text file\n",
        "                if file_name.endswith('.txt'):\n",
        "                    # Read the text file\n",
        "                    with zip_ref.open(file_name) as text_file:\n",
        "                        text_content = text_file.read()\n",
        "                        # Add the text content to the list\n",
        "                        text_files_content.append(text_content.decode('utf-8'))\n",
        "    else:\n",
        "        print(f\"Failed to download the zip file. Status code: {response.status_code}\")\n",
        "\n",
        "    # Return the list of text file contents\n",
        "    return text_files_content\n",
        "\n",
        "\n",
        "def Starting_Lineup(date):\n",
        "\n",
        "\n",
        "    \"\"\"# Functions\n",
        "\n",
        "    # Importing Data\n",
        "\n",
        "    # Formatting player data\n",
        "\n",
        "    # Formatting per game data\n",
        "    \"\"\"\n",
        "\n",
        "    # from bs4 import BeautifulSoup\n",
        "    import re\n",
        "    # import pybaseball as pyb\n",
        "    # Fetch the HTML content from the webpage\n",
        "    url = f'https://www.mlb.com/starting-lineups/{date}'\n",
        "    response = requests.get(url)\n",
        "    html_content = response.text\n",
        "\n",
        "\n",
        "    # Assuming `html_content` contains the HTML source you provided\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Regular expression to extract numeric ID from player URL\n",
        "    player_id_pattern = re.compile(r'/player/.*-(\\d+)$')\n",
        "\n",
        "    # Collect all MLBAM IDs\n",
        "    all_mlbam_ids = set()\n",
        "\n",
        "    # Find all game containers\n",
        "    games = soup.find_all('div', class_='starting-lineups__matchup')\n",
        "\n",
        "    for game in games:\n",
        "        # Extract pitcher and batter IDs\n",
        "        pitchers = game.find_all('a', class_='starting-lineups__pitcher--link')\n",
        "        batters = game.find_all('a', class_='starting-lineups__player--link')\n",
        "        player_links = pitchers + batters\n",
        "\n",
        "        for player_link in player_links:\n",
        "            player_id_match = re.search(player_id_pattern, player_link['href'])\n",
        "            if player_id_match:\n",
        "                all_mlbam_ids.add(int(player_id_match.group(1)))\n",
        "\n",
        "    # Perform reverse lookup to get Retro IDs\n",
        "    all_player_info = pyb.playerid_reverse_lookup(list(all_mlbam_ids), key_type='mlbam')\n",
        "    mlbam_to_retro = all_player_info.set_index('key_mlbam')['key_retro'].to_dict()\n",
        "\n",
        "\n",
        "    game_matchups = {}\n",
        "\n",
        "    # Use enumerate to get both the index and the game object for uniqueness\n",
        "    for index, game in enumerate(games):\n",
        "        teams = game.find_all('span', class_='starting-lineups__team-name')\n",
        "        away_team_code = teams[0].find('a')['data-tri-code']\n",
        "        home_team_code = teams[2].find('a')['data-tri-code']\n",
        "\n",
        "        # Extract pitcher IDs\n",
        "        pitchers = game.find_all('a', class_='starting-lineups__pitcher--link')\n",
        "        if len(pitchers) < 3:  # Ensure there are at least 3 links, indicating both away and home pitchers are present\n",
        "            continue  # Skip this game if there aren't enough pitcher links\n",
        "        away_pitcher_id = re.search(player_id_pattern, pitchers[0]['href']).group(1) if len(pitchers) > 0 else None\n",
        "        home_pitcher_id = re.search(player_id_pattern, pitchers[2]['href']).group(1) if len(pitchers) > 1 else None\n",
        "        if not away_pitcher_id or not home_pitcher_id:\n",
        "            continue\n",
        "        # Extract batters IDs for both teams\n",
        "        away_batters = game.find('ol', class_='starting-lineups__team--away').find_all('a', class_='starting-lineups__player--link')\n",
        "        home_batters = game.find('ol', class_='starting-lineups__team--home').find_all('a', class_='starting-lineups__player--link')\n",
        "\n",
        "        away_batters_ids = [re.search(player_id_pattern, batter['href']).group(1) for batter in away_batters]\n",
        "        home_batters_ids = [re.search(player_id_pattern, batter['href']).group(1) for batter in home_batters]\n",
        "        if not away_batters_ids or not home_batters_ids:\n",
        "            continue\n",
        "        # Populate the game_matchups dictionary with a unique game key\n",
        "        game_key = f\"{away_team_code}@{home_team_code}_{index}\"\n",
        "        game_matchups[game_key] = {\n",
        "            'away_team': away_team_code,\n",
        "            'home_team': home_team_code,\n",
        "            'away_pitcher': {'mlbam_id': away_pitcher_id, 'retro_id': mlbam_to_retro.get(int(away_pitcher_id)) if away_pitcher_id else None},\n",
        "            'home_pitcher': {'mlbam_id': home_pitcher_id, 'retro_id': mlbam_to_retro.get(int(home_pitcher_id)) if home_pitcher_id else None},\n",
        "            'away_players': [{'mlbam_id': id, 'retro_id': mlbam_to_retro.get(int(id))} for id in away_batters_ids],\n",
        "            'home_players': [{'mlbam_id': id, 'retro_id': mlbam_to_retro.get(int(id))} for id in home_batters_ids]\n",
        "        }\n",
        "\n",
        "    # print(game_matchups)\n",
        "\n",
        "    \"\"\"# Set-up Neural Network\n",
        "\n",
        "    # Predict the outcome of the season and calculate profit\n",
        "    \"\"\"\n",
        "\n",
        "    # Define a mapping of incorrect team abbreviations to correct ones\n",
        "    team_nomenclature_mapping = {\n",
        "        'CHC': 'CHN',  # Chicago Cubs\n",
        "        'WSH': 'WAS',  # Washington Nationals\n",
        "        'NYM': 'NYN',  # New York Mets\n",
        "        'BOS': 'BOS',  # Boston Red Sox\n",
        "        'ATL': 'ATL',  # Atlanta Braves\n",
        "        'TEX': 'TEX',  # Texas Rangers\n",
        "        'HOU': 'HOU',  # Houston Astros\n",
        "        'KC ': 'KCA',   # Kansas City Royals\n",
        "        'LAA': 'ANA',  # Los Angeles Angels\n",
        "        'MIN': 'MIN',  # Minnesota Twins\n",
        "        'MIL': 'MIL',  # Milwaukee Brewers\n",
        "        'AZ ': 'ARI',   # Arizona Diamondbacks\n",
        "        'SD ': 'SDN',   # San Diego Padres\n",
        "        'TB ': 'TBA',   # Tampa Bay Rays\n",
        "        'OAK': 'OAK',  # Oakland Athletics\n",
        "        'LAD': 'LAN',  # Los Angeles Dodgers\n",
        "        'SF ': 'SFN',   # San Francisco Giants\n",
        "        'SEA': 'SEA',  # Seattle Mariners\n",
        "        'CWS': 'CHA',  # Chicago White Sox\n",
        "        'CLE': 'CLE',  # Cleveland Guardians\n",
        "        'DET': 'DET',  # Detroit Tigers\n",
        "        'PHI': 'PHI',  # Philadelphia Phillies\n",
        "        'PIT': 'PIT',  # Pittsburgh Pirates\n",
        "        'STL': 'SLN',  # St. Louis Cardinals\n",
        "        'TOR': 'TOR',  # Toronto Blue Jays\n",
        "        'BAL': 'BAL',  # Baltimore Orioles\n",
        "        'MIA': 'MIA',  # Miami Marlins\n",
        "        'CIN': 'CIN',  # Cincinnati Reds\n",
        "        'COL': 'COL',  # Colorado Rockies\n",
        "        'NYY': 'NYA',  # New York Yankees\n",
        "        'AZ': 'ARI',   # Arizona Diamondbacks\n",
        "        'SD': 'SDN',   # San Diego Padres\n",
        "        'TB': 'TBA',   # Tampa Bay Rays\n",
        "        'SF': 'SFN',   # San Francisco Giants\n",
        "        'KC': 'KCA',   # Kansas City Royals\n",
        "        # ... add other mappings as needed\n",
        "    }\n",
        "    # ... (previous code)\n",
        "\n",
        "    # Create a list to hold all game data\n",
        "    games_data = []\n",
        "\n",
        "    # Create a dictionary to keep track of game counts for the day\n",
        "    game_counts = {}\n",
        "\n",
        "    # Iterate over the game matchups to populate the games_data list\n",
        "    for game, details in game_matchups.items():\n",
        "        # Correct the team abbreviations\n",
        "        home_team_corrected = team_nomenclature_mapping.get(details['home_team'], details['home_team'])\n",
        "        away_team_corrected = team_nomenclature_mapping.get(details['away_team'], details['away_team'])\n",
        "\n",
        "        # Create a unique game identifier based on the corrected teams and the number of games they've played that day\n",
        "        game_count_key = f\"{home_team_corrected}{away_team_corrected}\"\n",
        "        game_counts[game_count_key] = game_counts.get(game_count_key, 0) + 1\n",
        "        game_id = f\"{date.replace('-', '')}{home_team_corrected}{away_team_corrected}{str(game_counts[game_count_key])}\"  # Ensure the game number is two digits\n",
        "\n",
        "        # Define game_data inside the loop to reset it for each game\n",
        "        game_data = {\n",
        "            'Game_ID': game_id,\n",
        "            'HmTm': home_team_corrected,\n",
        "            'VisTm':away_team_corrected,\n",
        "            'HmStPchID': details['home_pitcher'].get('retro_id') if isinstance(details['home_pitcher'], dict) else None,\n",
        "            'VisStPchID': details['away_pitcher'].get('retro_id') if isinstance(details['away_pitcher'], dict) else None\n",
        "        }\n",
        "\n",
        "        # Add Home team batters in the order they were scraped\n",
        "        for i, player in enumerate(details['home_players'], start=1):\n",
        "            game_data[f'HmBat{i}ID'] = player.get('retro_id') if isinstance(player, dict) else None\n",
        "\n",
        "        # Add Away team batters in the order they were scraped\n",
        "        for i, player in enumerate(details['away_players'], start=1):\n",
        "            game_data[f'VisBat{i}ID'] = player.get('retro_id') if isinstance(player, dict) else None\n",
        "\n",
        "        # Append the game_data dictionary to the games_data list once per game\n",
        "        games_data.append(game_data)\n",
        "\n",
        "    # ... (rest of your code to convert games_data to DataFrame and export)\n",
        "\n",
        "    # Convert the games_data list to a DataFrame\n",
        "    games_df = pd.DataFrame(games_data)\n",
        "\n",
        "    # Define the column order explicitly to match the desired structure\n",
        "    column_order = ['Game_ID', 'HmStPchID', 'VisStPchID'] + \\\n",
        "                [f'HmBat{i}ID' for i in range(1, 10)] + \\\n",
        "                [f'VisBat{i}ID' for i in range(1, 10)] + \\\n",
        "                ['HmTm', 'VisTm']\n",
        "\n",
        "    # Reorder the DataFrame columns\n",
        "    games_df = games_df[column_order]\n",
        "\n",
        "    # Export the DataFrame to an Excel file\n",
        "    games_df.to_excel('pybaseball_v3_saved/Scraping_Games.xlsx', index=False)\n",
        "    return games_df\n",
        "\n",
        "def bwar_bat(return_all: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Get data from war_daily_bat table. Returns WAR, its components, and a few other useful stats.\n",
        "    To get all fields from this table, supply argument return_all=True.\n",
        "    \"\"\"\n",
        "    url = \"http://www.baseball-reference.com/data/war_daily_bat.txt\"\n",
        "    s = requests.get(url)\n",
        "    if return_all:\n",
        "        c = pd.read_csv(io.StringIO(s.text))\n",
        "        return c\n",
        "    else:\n",
        "        cols_to_keep = ['name_common', 'mlb_ID', 'player_ID', 'year_ID', 'team_ID', 'stint_ID', 'lg_ID',\n",
        "                        'pitcher','G', 'PA', 'salary', 'runs_above_avg', 'runs_above_avg_off','runs_above_avg_def',\n",
        "                        'WAR_rep','WAA','WAR']\n",
        "        c = pd.read_csv(io.StringIO(s.text), usecols=cols_to_keep)\n",
        "        return c\n",
        "\n",
        "\n",
        "def bwar_pitch(return_all: bool=True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Get data from war_daily_pitch table. Returns WAR, its components, and a few other useful stats.\n",
        "    To get all fields from this table, supply argument return_all=True.\n",
        "    \"\"\"\n",
        "    url = \"http://www.baseball-reference.com/data/war_daily_pitch.txt\"\n",
        "    s = requests.get(url)\n",
        "    if return_all:\n",
        "        c = pd.read_csv(io.StringIO(s.text))\n",
        "        return c\n",
        "    else:\n",
        "        cols_to_keep = ['name_common', 'mlb_ID', 'player_ID', 'year_ID', 'team_ID', 'stint_ID', 'lg_ID',\n",
        "                        'G', 'GS', 'RA','xRA', 'BIP', 'BIP_perc','salary', 'ERA_plus', 'WAR_rep', 'WAA',\n",
        "                        'WAA_adj','WAR']\n",
        "        c = pd.read_csv(io.StringIO(s.text), usecols=cols_to_keep)\n",
        "        return c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrdSLG_vVQU9"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOJ-NP25VQU-"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "GsGA0YD0VQU-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batter data fetched successfully.\n",
            "pitcher data fetched successfully.\n"
          ]
        }
      ],
      "source": [
        "betting_date = '2021-09-08'\n",
        "season_start_year = int(betting_date[:4])\n",
        "all_years = [str(year) for year in range(season_start_year - 8, season_start_year + 1)]\n",
        "train_years = all_years[:-1]\n",
        "season_start_date = f\"{season_start_year}-03-01\"\n",
        "starting_lineup=Starting_Lineup(betting_date)\n",
        "zip_links=list_old_bwar_data()\n",
        "# starting_lineup = starting_lineup.set_index('Game_ID')\n",
        "most_recent_link=find_most_recent_zip(zip_links,betting_date)\n",
        "\n",
        "starting_lineup=starting_lineup.set_index('Game_ID')\n",
        "\n",
        "#find the most recent bwar data for players. This could be as old as the previous season, depending on the date.\n",
        "content= scrape_old_bwar_data(most_recent_link)\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "\n",
        "#convert the batter and pitcher data into a dataframe\n",
        "# batter_data = pd.read_csv(io.StringIO(content[0]))\n",
        "# pitcher_data = pd.read_csv(io.StringIO(content[1]))\n",
        "\n",
        "\n",
        "##### UNCOMMENT THIS WHEN YOU NEED TO ACTUALLY CALL THE REAL BATTER AND PITCHER DATA\n",
        "#batter imports full data\n",
        "batter_data = fetch_data_with_retry(bwar_bat, 'batter')\n",
        "#pitcher imports full data\n",
        "pitcher_data = fetch_data_with_retry(bwar_pitch, 'pitcher')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                home_open visiting_open\n",
            "Game_ID                                \n",
            "20100404BOSNYY1      -114          -106\n",
            "20100405WASPHI1       170          -200\n",
            "20100405NYMMIA1      -115          -105\n",
            "20100405CINSTL1       135          -155\n",
            "20100405PITLOS1       135          -155\n",
            "...                   ...           ...\n",
            "20211027HOUATL1      -115          -105\n",
            "20211029ATLHOU1      -115          -105\n",
            "20211030ATLHOU1      -115          -105\n",
            "20211031ATLHOU1      -105          -115\n",
            "20211102HOUATL1      -120           100\n",
            "\n",
            "[28006 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# train_years = [str(year) for year in range(int(test_year) - 8, int(test_year)-1)]\n",
        "\n",
        "\n",
        "milb_batter = pd.read_csv(milb_batter_path)\n",
        "milb_pitcher = pd.read_csv(milb_pitcher_path)\n",
        "\n",
        "#betting data site\n",
        "scrambled_odds_full=pd.read_csv(scrambled_betting_paths)\n",
        "#People Import\n",
        "chad_v3 = pyb.chadwick_register()\n",
        "\n",
        "#Vegas Odds Import and process in one\n",
        "vegas_odds=process_vegas_odds(vegas_paths)\n",
        "\n",
        "\n",
        "#pitcher imports full data\n",
        "# pitcher_data = pd.read_csv(mlb_pitcher_path)\n",
        "\n",
        "# Example usage:\n",
        "# zip_links = list_old_bwar_data()  # This would be the list obtained from the previous function\n",
        "# input_date = '2023-04-01'  # Example input date\n",
        "# print(find_most_recent_zip(zip_links, input_date))\n",
        "\n",
        "\n",
        "\n",
        "per_game_data_full = pd.read_csv(per_game_path, header=0)\n",
        "\n",
        "#Drops all columns except for the columns below\n",
        "columns_to_keep = ['HmStPchID', 'VisStPchID', 'HmBat1ID', 'HmBat2ID', 'HmBat3ID', 'HmBat4ID', 'HmBat5ID', 'HmBat6ID', 'HmBat7ID', 'HmBat8ID', 'HmBat9ID', 'VisBat1ID', 'VisBat2ID', 'VisBat3ID', 'VisBat4ID', 'VisBat5ID', 'VisBat6ID', 'VisBat7ID', 'VisBat8ID', 'VisBat9ID', 'HmRuns', 'VisRuns','HmTm','VisTm']\n",
        "per_game_data = process_per_game_data(per_game_data_full, columns_to_keep,all_years, betting_date)\n",
        "# per_game_players = add_players_to_games()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "9b3Dq6JTVQU_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropping columns: Index(['date', 'GameId'], dtype='object')\n",
            "File pybaseball_v3_saved\\per_game_data_full.csv already exists.\n",
            "File pybaseball_v3_saved\\per_game_data.csv already exists.\n",
            "File pybaseball_v3_saved\\batter_bwar_data.csv already exists.\n",
            "File pybaseball_v3_saved\\pitcher_bwar_data.csv already exists.\n",
            "File pybaseball_v3_saved\\chad_v3.csv already exists.\n",
            "File pybaseball_v3_saved\\vegas_odds.csv already exists.\n",
            "File pybaseball_v3_saved\\scrambled_odds.csv already exists.\n",
            "File pybaseball_v3_saved\\scrambled_odds_full.csv already exists.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#process scrambled odds\n",
        "scrambled_odds=process_scrambled_odds(scrambled_odds_full)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#save it to a folder called pybaseball_v3_saved\n",
        "save_to_path(per_game_data_full, 'per_game_data_full')\n",
        "save_to_path(per_game_data, 'per_game_data')\n",
        "save_to_path(batter_data, 'batter_bwar_data')\n",
        "save_to_path(pitcher_data, 'pitcher_bwar_data')\n",
        "save_to_path(chad_v3,'chad_v3')\n",
        "save_to_path(vegas_odds,'vegas_odds')\n",
        "save_to_path(scrambled_odds,'scrambled_odds')\n",
        "save_to_path(scrambled_odds_full,'scrambled_odds_full')\n",
        "\n",
        "playerid='player_ID'\n",
        "# pitcher_data.reset_index(drop=False, inplace=True)\n",
        "# batter_data.reset_index(drop=False, inplace=True)\n",
        "\n",
        "batter_data.set_index(playerid, inplace=True)\n",
        "pitcher_data.set_index(playerid, inplace=True)\n",
        "pitcher_names = pyb.playerid_reverse_lookup(pitcher_data.index, key_type='bbref')\n",
        "batter_names = pyb.playerid_reverse_lookup(batter_data.index, key_type='bbref')\n",
        "#For some reason the chad data is missing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# batter_data_remove = batter_data[batter_data.index != 'stantha01']\n",
        "\n",
        "# Create a mapping from player_ID to key_retro for pitchers and batters\n",
        "# if 'pitcher_id_to_retro' not in locals():\n",
        "pitcher_id_to_retro = pitcher_names.set_index('key_bbref')['key_retro'].to_dict()\n",
        "# if 'batter_id_to_retro' not in locals():\n",
        "batter_id_to_retro = batter_names.set_index('key_bbref')['key_retro'].to_dict()\n",
        "\n",
        "\n",
        "# Create a copy of the original DataFrames to preserve the original 'playerid' columns\n",
        "pitcher_data_original = pitcher_data.copy()\n",
        "batter_data_remove_original = batter_data.copy()\n",
        "\n",
        "# Before performing the mapping, ensure that the index is set correctly on the original DataFrames\n",
        "# pitcher_data_original.set_index(playerid, inplace=True)\n",
        "# batter_data_remove_original.set_index(playerid, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                name_common   age    mlb_ID  year_ID team_ID  stint_ID lg_ID  \\\n",
            "player_ID                                                                      \n",
            "bechtge01    George Bechtel  22.0  110756.0     1871     ATH         1   NaN   \n",
            "brainas01      Asa Brainard  30.0  111373.0     1871     OLY         1   NaN   \n",
            "fergubo01      Bob Ferguson  26.0  114069.0     1871     NYU         1   NaN   \n",
            "fishech01   Cherokee Fisher  26.0  114181.0     1871     ROK         1   NaN   \n",
            "fleetfr01       Frank Fleet  23.0  114224.0     1871     NYU         1   NaN   \n",
            "...                     ...   ...       ...      ...     ...       ...   ...   \n",
            "youngal01        Alex Young  29.0  622065.0     2023     CIN         1    NL   \n",
            "youngda02       Danny Young  29.0  664849.0     2023     ATL         1    NL   \n",
            "zastrro01     Rob Zastryzny  31.0  642239.0     2023     PIT         1    NL   \n",
            "zavalse01       Seby Zavala  29.0  664874.0     2023     ARI         2    NL   \n",
            "zuniggu01  Guillermo Zuiga  24.0  670871.0     2023     STL         1    NL   \n",
            "\n",
            "            G  GS  IPouts  ...  pyth_exponent  waa_win_perc     WAA  WAA_adj  \\\n",
            "player_ID                  ...                                                 \n",
            "bechtge01   3   3      78  ...          2.499        0.3020 -0.5940  -0.0153   \n",
            "brainas01  30  30     792  ...          2.375        0.4951 -0.1470  -0.1556   \n",
            "fergubo01   1   0       3  ...          2.606        0.1823 -0.3177  -0.0006   \n",
            "fishech01  24  24     639  ...          2.367        0.5098  0.2352  -0.1255   \n",
            "fleetfr01   1   1      27  ...          2.698        0.1137 -0.3863  -0.0053   \n",
            "...        ..  ..     ...  ...            ...           ...     ...      ...   \n",
            "youngal01  63   0     161  ...          1.892        0.5054  0.3402  -0.1128   \n",
            "youngda02   8   0      25  ...          1.871        0.5426  0.3408  -0.1144   \n",
            "zastrro01  21   1      62  ...          1.908        0.4762 -0.4998   0.0269   \n",
            "zavalse01   1   0       2  ...          1.878        0.5302  0.0302  -0.0170   \n",
            "zuniggu01   2   0       6  ...          1.897        0.4952 -0.0096  -0.0011   \n",
            "\n",
            "           oppRpG_rep  pyth_exponent_rep  waa_win_perc_rep  WAR_rep  \\\n",
            "player_ID                                                             \n",
            "bechtge01    12.22880              2.432            0.4008   0.2455   \n",
            "brainas01    12.25747              2.432            0.3994   2.4889   \n",
            "fergubo01    10.58039              2.380            0.4878   0.0102   \n",
            "fishech01    12.27359              2.433            0.3986   2.0067   \n",
            "fleetfr01    12.30047              2.434            0.3973   0.0847   \n",
            "...               ...                ...               ...      ...   \n",
            "youngal01     4.79110              1.899            0.4916   0.5202   \n",
            "youngda02     4.80991              1.900            0.4897   0.0810   \n",
            "zastrro01     4.80421              1.900            0.4903   0.2002   \n",
            "zavalse01     4.77275              1.898            0.4934   0.0065   \n",
            "zuniggu01     4.80578              1.900            0.4901   0.0195   \n",
            "\n",
            "             ERA_plus    ER_lg  \n",
            "player_ID                       \n",
            "bechtge01   52.682609   12.117  \n",
            "brainas01   93.906818  123.957  \n",
            "fergubo01   19.733333    0.592  \n",
            "fishech01   97.129126  100.043  \n",
            "fleetfr01   39.680000    3.968  \n",
            "...               ...      ...  \n",
            "youngal01  118.926087   27.353  \n",
            "youngda02  426.900000    4.269  \n",
            "zastrro01   93.963636   10.336  \n",
            "zavalse01         NaN    0.491  \n",
            "zuniggu01  113.400000    1.134  \n",
            "\n",
            "[54852 rows x 42 columns]\n"
          ]
        }
      ],
      "source": [
        "print(pitcher_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Perform the mapping on the copies\n",
        "pitcher_data.index = pitcher_data_original.index.map(pitcher_id_to_retro)\n",
        "batter_data.index = batter_data_remove_original.index.map(batter_id_to_retro)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if 'level_0' not in batter_data.columns:\n",
        "    batter_data.reset_index(inplace=True)\n",
        "if 'level_0' not in pitcher_data.columns:\n",
        "    pitcher_data.reset_index(inplace=True)\n",
        "    batter_data_removed=remove_columns_with_nan(batter_data,40)\n",
        "    pitcher_data_removed=remove_columns_with_nan(pitcher_data,40)\n",
        "# Now you can call the aggregate_player_data function with 'playerid' as a column\n",
        "# THIS ALSO LABEL ENCODES THE DATA\n",
        "batter_data_removed.set_index(playerid, inplace=True)\n",
        "pitcher_data_removed.set_index(playerid, inplace=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "per_game_data_odds = combine_odds(per_game_data, vegas_odds, scrambled_odds)\n",
        "batter_aggregated = aggregate_player_data(batter_data, playerid)\n",
        "pitcher_aggregated = aggregate_player_data(pitcher_data, playerid)\n",
        "\n",
        "if 0 in batter_aggregated.index:\n",
        "    batter_aggregated.drop(index=0, inplace=True)\n",
        "if 0 in pitcher_aggregated.index:\n",
        "    pitcher_aggregated.drop(index=0, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "    # Check if the index has a 0 value and if so, remove the corresponding row\n",
        "\n",
        "\n",
        "\n",
        "pitcher_columns = ['HmStPchID', 'VisStPchID']\n",
        "batter_columns = ['HmBat1ID', 'HmBat2ID', 'HmBat3ID', 'HmBat4ID', 'HmBat5ID', 'HmBat6ID', 'HmBat7ID', 'HmBat8ID', 'HmBat9ID', 'VisBat1ID', 'VisBat2ID', 'VisBat3ID', 'VisBat4ID', 'VisBat5ID', 'VisBat6ID', 'VisBat7ID', 'VisBat8ID', 'VisBat9ID']\n",
        "\n",
        "    # Create separate dictionaries for the data\n",
        "player_data_dictionary = {\n",
        "    'pitcher': pitcher_aggregated.to_dict('index'),\n",
        "    'batter': batter_aggregated.to_dict('index')\n",
        "}\n",
        "\n",
        "player_id_columns = pitcher_columns + batter_columns\n",
        "per_game_finished, missing_player_ids = replace_ids_with_stats_and_catalog_missing(per_game_data_odds, player_id_columns, player_data_dictionary)\n",
        "# per_game_finished, missing_player_ids = prepare_data(batter_data_removed,pitcher_data_removed, per_game_data_odds,playerid,all_years)\n",
        "\n",
        "starting_lineup_finished, missing_player_ids = replace_ids_with_stats_and_catalog_missing(starting_lineup, player_id_columns, player_data_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The column home_team has 74.9% NaN values.\n",
            "The column visiting_team has 74.9% NaN values.\n",
            "The column Game_Number has 74.9% NaN values.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[69], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m view_data(per_game_finished)\n\u001b[0;32m      5\u001b[0m per_game_finished\u001b[38;5;241m=\u001b[39mremove_columns_with_nan(per_game_finished,\u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m per_game_finished\u001b[38;5;241m=\u001b[39m\u001b[43mImpute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mper_game_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmost_frequent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m per_game_finished \u001b[38;5;241m=\u001b[39m label_encode(per_game_finished)\n\u001b[0;32m      9\u001b[0m starting_lineup_finished\u001b[38;5;241m=\u001b[39mremove_columns_with_nan(starting_lineup_finished,\u001b[38;5;241m40\u001b[39m)\n",
            "Cell \u001b[1;32mIn[59], line 543\u001b[0m, in \u001b[0;36mImpute\u001b[1;34m(df, method)\u001b[0m\n\u001b[0;32m    541\u001b[0m imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39mmethod)\n\u001b[0;32m    542\u001b[0m \u001b[38;5;66;03m# Fit and transform the DataFrame, but keep the index\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m df_imputed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns, index\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_imputed\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    279\u001b[0m         )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1061\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1047\u001b[0m             (\n\u001b[0;32m   1048\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1057\u001b[0m         )\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1060\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1063\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\impute\\_base.py:376\u001b[0m, in \u001b[0;36mSimpleImputer.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the imputer on `X`.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\impute\\_base.py:322\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    319\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 322\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1013\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n\u001b[1;32m-> 1013\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmay_share_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray_orig\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1014\u001b[0m             array \u001b[38;5;241m=\u001b[39m _asarray_with_order(\n\u001b[0;32m   1015\u001b[0m                 array, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[0;32m   1016\u001b[0m             )\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;66;03m# always make a copy for non-numpy arrays\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py:2083\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2083\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\n\u001b[0;32m   2084\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   2085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2086\u001b[0m         astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   2087\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   2088\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block\n\u001b[0;32m   2089\u001b[0m     ):\n\u001b[0;32m   2090\u001b[0m         \u001b[38;5;66;03m# Check if both conversions can be done without a copy\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:1046\u001b[0m, in \u001b[0;36mDataFrame._values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1044\u001b[0m blocks \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_wrapped_if_datetimelike(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m)\n\u001b[0;32m   1048\u001b[0m arr \u001b[38;5;241m=\u001b[39m blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# non-2D ExtensionArray\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:12281\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m  12207\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m  12208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m  12209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  12210\u001b[0m \u001b[38;5;124;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[0;32m  12211\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12279\u001b[0m \u001b[38;5;124;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[0;32m  12280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 12281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:1656\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1654\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1656\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:1697\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m   1696\u001b[0m     rl \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mmgr_locs\n\u001b[1;32m-> 1697\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1698\u001b[0m     result[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m   1699\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\blocks.py:2247\u001b[0m, in \u001b[0;36mNumpyBlock.get_values\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: DtypeObj \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m   2246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m _dtype_obj:\n\u001b[1;32m-> 2247\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(_dtype_obj)\n\u001b[0;32m   2248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# Now you can check which games have players with all stats missing\n",
        "aaaaaaa = per_game_finished\n",
        "\n",
        "view_data(per_game_finished)\n",
        "per_game_finished=remove_columns_with_nan(per_game_finished,40)\n",
        "per_game_finished=Impute(per_game_finished,'most_frequent')\n",
        "per_game_finished = label_encode(per_game_finished)\n",
        "\n",
        "starting_lineup_finished=remove_columns_with_nan(starting_lineup_finished,40)\n",
        "starting_lineup_finished=Impute(starting_lineup_finished, 'most_frequent')\n",
        "starting_lineup_finished= label_encode(starting_lineup_finished)\n",
        "# starting_lineup_finished=Impute(starting_lineup_finished,'most')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOQj8_SUXLC6"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "classification_ranking() got an unexpected keyword argument 'choices'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[49], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m _, X_small, _, y_small \u001b[38;5;241m=\u001b[39m train_test_split(X_train, y_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# remove_columns = ['mlb_ID']\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# pitcher_removed=remove_excess_player_columns(pitcher_data,remove_columns)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# batter_removed=remove_excess_player_columns(batter_data,remove_columns)\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m rankings \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_ranking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf_test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# For classification tasks\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# # or\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# # #   ranking = regression_ranking(X_small, y_small, rf_hypers, xb_hypers)  # For regression tasks\u001b[39;00m\n\u001b[0;32m     47\u001b[0m scoring \u001b[38;5;241m=\u001b[39m voting(rankings)\n",
            "\u001b[1;31mTypeError\u001b[0m: classification_ranking() got an unexpected keyword argument 'choices'"
          ]
        }
      ],
      "source": [
        "\n",
        "# X,y = get_data(per_game_finished, labels='winner', thresh=0.2, columns_to_drop=['HmRuns','VisRuns','home_odds','visiting_odds'])\n",
        "#Split the data\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = split_dataframe_data(\n",
        "#     per_game_finished,\n",
        "#     train_years,\n",
        "#     validation_year,\n",
        "#     test_year\n",
        "# )\n",
        "# Assuming per_game_finished is your DataFrame\n",
        "# unlabeled_data = per_game_finished.drop(['home_odds', 'visiting_odds', 'HmRuns', 'VisRuns'], axis=1)\n",
        "# labeled_data = per_game_finished['winner']\n",
        "# per_game_finished.drop(['winner', 'home_odds', 'visiting_odds', 'HmRuns', 'VisRuns'], axis=1)\n",
        "\n",
        "# Assuming 'per_game_finished' is your DataFrame and 'winner' is the column with labels\n",
        "unlabeled_data = per_game_finished.drop(['home_odds', 'visiting_odds', 'HmRuns', 'VisRuns', 'winner'], axis=1)\n",
        "labeled_data = per_game_finished['winner']\n",
        "\n",
        "\n",
        "# Sort the DataFrame by its index to ensure chronological order\n",
        "per_game_finished = per_game_finished.sort_index()\n",
        "\n",
        "# Split off the test set from the most recent data\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    per_game_finished.drop(['home_odds', 'visiting_odds', 'HmRuns', 'VisRuns', 'winner'], axis=1),\n",
        "    per_game_finished['winner'],\n",
        "    test_size=0.03,  # Assuming the test set is 20% of the data\n",
        "    shuffle=False  # Important to keep the temporal order\n",
        ")\n",
        "\n",
        "# Split the remaining data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp,\n",
        "    y_temp,\n",
        "    test_size=0.1,  # This makes the validation set 25% of the remaining data, achieving approximately a 60/20/20 split\n",
        "    shuffle=False  # Preserve the chronological order\n",
        ")\n",
        "# Now you have X_train_temp, y_train_temp for training, X_val, y_val for validation, and X_test, y_test for testing.\n",
        "\n",
        "_, X_small, _, y_small = train_test_split(X_train, y_train, test_size=0.05, random_state=42)\n",
        "# remove_columns = ['mlb_ID']\n",
        "# pitcher_removed=remove_excess_player_columns(pitcher_data,remove_columns)\n",
        "# batter_removed=remove_excess_player_columns(batter_data,remove_columns)\n",
        "rankings = classification_ranking(X_small, y_small, predict=True, choices=['mi', 'f_test'])  # For classification tasks\n",
        "# # or\n",
        "# # #   ranking = regression_ranking(X_small, y_small, rf_hypers, xb_hypers)  # For regression tasks\n",
        "\n",
        "scoring = voting(rankings)\n",
        "# plot_after_vote(scoring, title='Feature Ranking')\n",
        "scores = [score[1] for score in scoring]  # Extract the second element from each tuple\n",
        "min_score = min(scores)\n",
        "max_score = max(scores)\n",
        "scoring = [(item[0], (item[1] - min_score) / (max_score - min_score)) for item in scoring]\n",
        "import pickle\n",
        "\n",
        "with open('bwar_new_scoring', 'wb') as f:\n",
        "    pickle.dump(scoring, f)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = scale_data(X_train, method='minmax')\n",
        "X_val = scale_data(X_val, method='minmax')\n",
        "X_test= scale_data(X_test, method='minmax')\n",
        "\n",
        "X_real = scale_data(starting_lineup_finished, method='minmax')\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('bwar_new_scoring', 'rb') as f:\n",
        "    scoring = pickle.load(f)\n",
        "\n",
        "threshold=0\n",
        "# Get the list of columns to keep\n",
        "# Convert `scoring` into a DataFrame\n",
        "scoring_df = pd.DataFrame(scoring, columns=['column_name', 'score'])\n",
        "\n",
        "# Filter the columns to keep\n",
        "columns_to_keep = scoring_df[scoring_df['score'] > threshold]['column_name'].tolist()\n",
        "\n",
        "# Ensure that only columns that exist in the DataFrame are kept\n",
        "columns_to_keep = [col for col in columns_to_keep if col in X_train.columns]\n",
        "# X_val_new=X_val\n",
        "# X_test_new=X_test\n",
        "X_train_reduced = X_train[columns_to_keep]\n",
        "X_val_reduced = X_val[columns_to_keep]\n",
        "X_test_reduced = X_test[columns_to_keep]\n",
        "X_real = X_real[columns_to_keep]\n",
        "\n",
        "X_train_reduced['game_index']=X_train_reduced.index\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train_res, y_train_res = rus.fit_resample(X_train_reduced, y_train)\n",
        "# X_train_res=X_train_new\n",
        "# y_train_res=y_train_new\n",
        "# Assuming `vector_dataset` is a class that converts your data into a format suitable for PyTorch\n",
        "# y_train_new.set_index('index',inplace=True)\n",
        "X_train_res.set_index('game_index',inplace=True)\n",
        "\n",
        "\n",
        "# Create a new model with the updated number of features\n",
        "\n",
        "# Create new DataLoaders with the updated datasets\n",
        "train_dataset_new = vector_dataset(X_train_res, y_train_res)\n",
        "val_dataset_new = vector_dataset(X_val_reduced, y_val)\n",
        "test_dataset_new = vector_dataset(X_test_reduced, y_test)\n",
        "\n",
        "# train_loader_new = DataLoader(train_dataset_new, batch_size=64, shuffle=True)\n",
        "# val_loader_new = DataLoader(val_dataset_new, batch_size=64, shuffle=False)\n",
        "# test_loader_new = DataLoader(test_dataset_new, batch_size=64, shuffle=False)\n",
        "train_loader_new = DataLoader(train_dataset_new, batch_size=256, shuffle=True)\n",
        "val_loader_new = DataLoader(val_dataset_new, batch_size=256, shuffle=False)\n",
        "test_loader_new = DataLoader(test_dataset_new, batch_size=256, shuffle=False)\n",
        "\n",
        "num_runs = 3\n",
        "conf_matrices = []\n",
        "class_reports = []\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Now you can train this model with the dropped datasets\n",
        "for _ in range(num_runs):\n",
        "    # Reinitialize the model for each run\n",
        "    if 'model' in locals():\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "    model = neural_net(X_train_res.shape[1], X_train_res.shape[1]*2, 2, 2, 0.15).to(device)\n",
        "    # model = neural_net(X_train_res.shape[1], X_train_res.shape[1]*2, 2, 2, 0.15)\n",
        "    # scores = cross_validate(model, X_train_res, y_train_res, n_splits=5)\n",
        "\n",
        "    # Reinitialize the model and optimizer for each run\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1)\n",
        "\n",
        "\n",
        "    # Train and evaluate the model\n",
        "\n",
        "    conf_matrix, class_report = train_and_evaluate_model(\n",
        "    model,\n",
        "    train_loader_new,\n",
        "    val_loader_new,\n",
        "    test_loader_new,\n",
        "    optimizer,\n",
        "    criterion,  # Make sure the criterion is passed here\n",
        "    scheduler,  # Then the scheduler\n",
        "    patience=5,\n",
        "    num_epochs=50\n",
        ")\n",
        "    # Store the results\n",
        "    conf_matrices.append(conf_matrix)\n",
        "    class_reports.append(pd.DataFrame(class_report).transpose())\n",
        "\n",
        "    # Step the scheduler\n",
        "\n",
        "\n",
        "# Calculate and print the average confusion matrix and classification report\n",
        "avg_conf_matrix = np.mean(conf_matrices, axis=0)\n",
        "avg_class_report = pd.concat(class_reports).groupby(level=0).mean()\n",
        "\n",
        "print('Average Confusion Matrix:')\n",
        "print(avg_conf_matrix)\n",
        "print('Average Classification Report:')\n",
        "print(avg_class_report)\n",
        "\n",
        "\n",
        "prob = None\n",
        "ok, bets, skipped, prob = predict_game_outcome(X_test_reduced.index, X_test_reduced.values, model, False, 10, 1)\n",
        "ok2, bets2, skipped2, prob2 =predict_game_outcome(X_real.index, X_real.values, model, False, 10, 1)\n",
        "better = Better()  # Create a Bettor instance with an initial wallet of 1000\n",
        "better2 = Better()\n",
        "# for game_id, bet_on_home_team in ok:\n",
        "#     better.bet(game_id, bet_on_home_team == 'True', bets, per_game_data_imputed)\n",
        "\n",
        "# for (game_id, bet_on_home_team), amount_bet in zip(ok, bets):\n",
        "#     better.bet(game_id, bet_on_home_team == 'True', amount_bet, per_game_finished, prob, True, 3)\n",
        "\n",
        "\n",
        "\n",
        "for (game_id, bet_on_home_team), amount_bet, game_prob in zip(ok, bets, prob):\n",
        "    better.bet(game_id, bet_on_home_team == 'True', amount_bet, per_game_finished, game_prob, True, 3)\n",
        "# Create a new model with the updated number of features\n",
        "for (game_id, bet_on_home_team), amount_bet, game_prob in zip(ok2, bets2, prob2):\n",
        "    better2.bet(game_id, bet_on_home_team == 'True', amount_bet, starting_lineup_finished, game_prob, True, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print((better.bet_history))\n",
        "print((better.wallet_history))\n",
        "print(ok[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91MYj8Bt_mQF"
      },
      "outputs": [],
      "source": [
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gHiocdYVQVA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def plot_wallet_balance(better, year):\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Number of Games Bet')\n",
        "    ax1.set_ylabel('Wallet Balance', color=color)\n",
        "    ax1.plot(range(len(better.wallet_history)), better.wallet_history, color=color)\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Bet Amount', color=color)  # we already handled the x-label with ax1\n",
        "    ax2.plot(range(len(better.bet_history)), better.bet_history, color=color)\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    title = f'Wallet Balance and Bet Amount Over Number of Games Bet in {year}'\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Save the plot as an image\n",
        "    if not os.path.exists('results'):\n",
        "        os.makedirs('results')\n",
        "    plt.savefig(f'results/{title}.png')\n",
        "\n",
        "    plt.show()\n",
        "    save_results=save_results_format(year)\n",
        "    # Save the better data, avg_class_report, and avg_conf_matrix to an Excel file\n",
        "    with pd.ExcelWriter(save_results) as writer:\n",
        "        pd.DataFrame(better.wallet_history).to_excel(writer, sheet_name='wallet_history')\n",
        "        pd.DataFrame(better.bet_history).to_excel(writer, sheet_name='bet_history')\n",
        "        avg_class_report.to_excel(writer, sheet_name='avg_class_report')\n",
        "        pd.DataFrame(avg_conf_matrix).to_excel(writer, sheet_name='avg_conf_matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juYdtTv4VQVB"
      },
      "outputs": [],
      "source": [
        "test_year=season_start_year\n",
        "plot_wallet_balance(better, test_year)\n",
        "print(max(better.bet_history))\n",
        "print(min(better.bet_history))\n",
        "print(min(better.wallet_history))\n",
        "print(better.wallet_history[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_wallet_balance(better2, test_year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dfa9Rc7VQVB"
      },
      "outputs": [],
      "source": [
        "view_data(per_game_finished)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFL_zO0sVQVB"
      },
      "outputs": [],
      "source": [
        "# view_data(pitcher_names)\n",
        "view_data(batter_data)\n",
        "\n",
        "\n",
        "\n",
        "    #return missing_player_ids_train, missing_player_ids_val, missing_player_ids_test, avg_class_report, avg_conf_matrix, better.bet_history, better.wallet_history\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
