{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "9pxTo6ch5qYo",
        "outputId": "9c453c03-e2d5-41c6-b42d-40882531f5c0"
      },
      "outputs": [],
      "source": [
        "#def main_function(test_year):\n",
        "\n",
        "# The rest of your script can go here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "QhIfx4UEM1Hd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pybaseball in c:\\python39\\lib\\site-packages (2.2.7)\n",
            "Requirement already satisfied: numpy>=1.13.0 in c:\\python39\\lib\\site-packages (from pybaseball) (1.26.2)\n",
            "Requirement already satisfied: pandas>=1.0.3 in c:\\python39\\lib\\site-packages (from pybaseball) (2.2.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.0 in c:\\python39\\lib\\site-packages (from pybaseball) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.18.1 in c:\\python39\\lib\\site-packages (from pybaseball) (2.31.0)\n",
            "Requirement already satisfied: lxml>=4.2.1 in c:\\python39\\lib\\site-packages (from pybaseball) (4.9.4)\n",
            "Requirement already satisfied: pyarrow>=1.0.1 in c:\\python39\\lib\\site-packages (from pybaseball) (14.0.2)\n",
            "Requirement already satisfied: pygithub>=1.51 in c:\\python39\\lib\\site-packages (from pybaseball) (2.1.1)\n",
            "Requirement already satisfied: scipy>=1.4.0 in c:\\python39\\lib\\site-packages (from pybaseball) (1.11.4)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in c:\\python39\\lib\\site-packages (from pybaseball) (3.8.2)\n",
            "Requirement already satisfied: tqdm>=4.50.0 in c:\\python39\\lib\\site-packages (from pybaseball) (4.66.1)\n",
            "Requirement already satisfied: attrs>=20.3.0 in c:\\python39\\lib\\site-packages (from pybaseball) (23.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\python39\\lib\\site-packages (from beautifulsoup4>=4.4.0->pybaseball) (2.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\python39\\lib\\site-packages (from matplotlib>=2.0.0->pybaseball) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\python39\\lib\\site-packages (from matplotlib>=2.0.0->pybaseball) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\python39\\lib\\site-packages (from matplotlib>=2.0.0->pybaseball) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python39\\lib\\site-packages (from matplotlib>=2.0.0->pybaseball) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\vile3\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=2.0.0->pybaseball) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\python39\\lib\\site-packages (from matplotlib>=2.0.0->pybaseball) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python39\\lib\\site-packages (from matplotlib>=2.0.0->pybaseball) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vile3\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib>=2.0.0->pybaseball) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\python39\\lib\\site-packages (from matplotlib>=2.0.0->pybaseball) (6.1.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python39\\lib\\site-packages (from pandas>=1.0.3->pybaseball) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\python39\\lib\\site-packages (from pandas>=1.0.3->pybaseball) (2023.3)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in c:\\python39\\lib\\site-packages (from pygithub>=1.51->pybaseball) (1.5.0)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in c:\\python39\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->pygithub>=1.51->pybaseball) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\vile3\\appdata\\roaming\\python\\python39\\site-packages (from pygithub>=1.51->pybaseball) (4.8.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in c:\\python39\\lib\\site-packages (from pygithub>=1.51->pybaseball) (2.1.0)\n",
            "Requirement already satisfied: Deprecated in c:\\python39\\lib\\site-packages (from pygithub>=1.51->pybaseball) (1.2.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python39\\lib\\site-packages (from requests>=2.18.1->pybaseball) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python39\\lib\\site-packages (from requests>=2.18.1->pybaseball) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python39\\lib\\site-packages (from requests>=2.18.1->pybaseball) (2023.11.17)\n",
            "Requirement already satisfied: colorama in c:\\users\\vile3\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.50.0->pybaseball) (0.4.6)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\vile3\\appdata\\roaming\\python\\python39\\site-packages (from importlib-resources>=3.2.0->matplotlib>=2.0.0->pybaseball) (3.17.0)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in c:\\python39\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->pygithub>=1.51->pybaseball) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.4.1 in c:\\python39\\lib\\site-packages (from pynacl>=1.4.0->pygithub>=1.51->pybaseball) (1.16.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\vile3\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pybaseball) (1.16.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in c:\\python39\\lib\\site-packages (from Deprecated->pygithub>=1.51->pybaseball) (1.16.0)\n",
            "Requirement already satisfied: pycparser in c:\\python39\\lib\\site-packages (from cffi>=1.4.1->pynacl>=1.4.0->pygithub>=1.51->pybaseball) (2.21)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip install pybaseball"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "UMzZjDAwM6KK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\python39\\lib\\site-packages (1.4.0)\n",
            "Requirement already satisfied: imbalanced-learn in c:\\python39\\lib\\site-packages (0.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade scikit-learn imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ekoD3VlRM7yy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: featureranker in c:\\python39\\lib\\site-packages (1.1.1)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.8.1 in c:\\python39\\lib\\site-packages (from featureranker) (3.8.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\python39\\lib\\site-packages (from featureranker) (1.26.2)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.1 in c:\\python39\\lib\\site-packages (from featureranker) (2.2.0)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.3.2 in c:\\python39\\lib\\site-packages (from featureranker) (1.4.0)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.11.2 in c:\\python39\\lib\\site-packages (from featureranker) (1.11.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\python39\\lib\\site-packages (from featureranker) (4.66.1)\n",
            "Requirement already satisfied: xgboost<3.0.0,>=2.0.1 in c:\\python39\\lib\\site-packages (from featureranker) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\vile3\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (10.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vile3\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.8.1->featureranker) (6.1.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python39\\lib\\site-packages (from pandas<3.0.0,>=2.1.1->featureranker) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\python39\\lib\\site-packages (from pandas<3.0.0,>=2.1.1->featureranker) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\python39\\lib\\site-packages (from scikit-learn<2.0.0,>=1.3.2->featureranker) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python39\\lib\\site-packages (from scikit-learn<2.0.0,>=1.3.2->featureranker) (3.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\vile3\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.66.1->featureranker) (0.4.6)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\vile3\\appdata\\roaming\\python\\python39\\site-packages (from importlib-resources>=3.2.0->matplotlib<4.0.0,>=3.8.1->featureranker) (3.17.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\vile3\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.8.1->featureranker) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip install featureranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "xQ6PJjFjVQU5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "import pybaseball as pyb\n",
        "import pandas as pd\n",
        "import time\n",
        "import inspect\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch import nn\n",
        "import json\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import os\n",
        "# from automatic_betting import Starting_Lineup\n",
        "from featureranker.utils import view_data\n",
        "from featureranker.rankers import classification_ranking\n",
        "from featureranker.rankers import voting\n",
        "# from featureranker import view_data\n",
        "# from featureranker import classification_hyper_param_search\n",
        "# from featureranker import classification_ranking\n",
        "# from featureranker import voting\n",
        "# from featureranker import plot_ranking\n",
        "from bs4 import BeautifulSoup\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import glob\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import pickle\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "from sklearn.model_selection import KFold\n",
        "warnings.filterwarnings('ignore')\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Seed value\n",
        "seed_value = 42\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `torch` pseudo-random generator at a fixed value\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "# 5. Depending on whether you are using CUDA\n",
        "torch.cuda.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n",
        "\n",
        "# 6. Configure a new global `torch` default floating point tensor type\n",
        "# (optional, if you are using PyTorch)\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "# 7. For `torch.backends.cudnn`\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "# import pycuda.autoinit\n",
        "# import pycuda.gpuarray as gpuarray\n",
        "pyb.cache.enable()\n",
        "pyb.cache.config.cache_type='csv'\n",
        "pyb.cache.config.save()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "UQhweF660E5m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Check if the code is running in Google Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    save_path='/content/drive/My Drive/baseball_pred/pybaseball_v3_saved'\n",
        "    #Vegas Odds path\n",
        "    vegas_betting_paths='/content/drive/My Drive/baseball_pred/betting_odds/'\n",
        "    vegas_paths = [path for path in glob.glob(vegas_betting_paths + '*.xlsx')]\n",
        "\n",
        "    #Scrambled Odds path\n",
        "    scrambled_betting_paths='/content/drive/My Drive/baseball_pred/all_money_lines.csv'\n",
        "\n",
        "\n",
        "    #Per_game_data path\n",
        "    per_game_path='/content/drive/My Drive/baseball_pred/pybaseball/pybaseball/data/Lahman_MLB_per_game_data.csv'\n",
        "\n",
        "\n",
        "\n",
        "    milb_batter_path = '/content/drive/My Drive/baseball_pred/SCRAPE_FANGRAPHS/APIBat/bat_standard.csv'\n",
        "    milb_pitcher_path = '/content/drive/My Drive/baseball_pred/SCRAPE_FANGRAPHS/APIPit/pit_standard.csv'\n",
        "    mlb_batter_path = '/content/drive/My Drive/baseball_pred/SCRAPE_FANGRAPHS/APIMlbbat/mlbbat_standard.csv'\n",
        "    mlb_pitcher_path = '/content/drive/My Drive/baseball_pred/SCRAPE_FANGRAPHS/APIMlbpit/mlbpit_standard.csv'\n",
        "    scoring_full_all_years='/content/drive/My Drive/baseball_pred/scoring_full_all_years.pkl'\n",
        "    scoring_save_path='/content/drive/My Drive/baseball_pred/scoring_milb.pkl'\n",
        "    def save_results_format(year):\n",
        "      save_results=f'/content/drive/My Drive/baseball_pred/results/better_data_{year}.xlsx'\n",
        "      return save_results\n",
        "else:\n",
        "    # Code to run if not in Google Colab\n",
        "    # For example, set a local path for your files if on a local machine\n",
        "    # local_drive_path = 'd:/Users/vile3/Google Drive'\n",
        "    # if not os.path.exists(local_drive_path):\n",
        "        # os.makedirs(local_drive_path)\n",
        "        save_path='pybaseball_v3_saved'\n",
        "        vegas_betting_paths='./betting_odds/'\n",
        "        vegas_paths = [path for path in glob.glob(vegas_betting_paths + '*.xlsx')]\n",
        "\n",
        "        #Scrambled Odds path\n",
        "        scrambled_betting_paths='all_money_lines.csv'\n",
        "\n",
        "\n",
        "        #Per_game_data path\n",
        "        per_game_path='./pybaseball/pybaseball/data/Lahman_MLB_per_game_data.csv'\n",
        "\n",
        "\n",
        "\n",
        "        milb_batter_path = './SCRAPE_FANGRAPHS/APIBat/bat_standard.csv'\n",
        "        milb_pitcher_path = './SCRAPE_FANGRAPHS/APIPit/pit_standard.csv'\n",
        "        mlb_batter_path = './SCRAPE_FANGRAPHS/APIMlbbat/mlbbat_standard.csv'\n",
        "        mlb_pitcher_path = './SCRAPE_FANGRAPHS/APIMlbpit/mlbpit_standard.csv'\n",
        "        scoring_full_all_years='scoring_full_all_years.pkl'\n",
        "        scoring_save_path='scoring_milb.pkl'\n",
        "        def save_results_format(year):\n",
        "          save_results=f'./results/better_data_{year}.xlsx'\n",
        "          return save_results\n",
        "    # Now you can use local_drive_path as the base path for your file operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDQ9gqifVQU7"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "xOV3PKKTVQU8"
      },
      "outputs": [],
      "source": [
        "# Check if CUDA is available\n",
        "\n",
        "# Move your model to the GPU\n",
        "# model = model.to(device)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# When you load your data, move it to the GPU\n",
        "# Assume inputs and labels are your input data and labels\n",
        "# inputs, labels = inputs.to(device), labels.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CEOkRzXVQU8"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Iznud8V5VQU8"
      },
      "outputs": [],
      "source": [
        "class vector_dataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = np.array(X)\n",
        "        self.y = np.array(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vec = torch.tensor(self.X[idx], dtype=torch.float)\n",
        "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
        "        return vec, label\n",
        "\n",
        "class neural_net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, hidden_num, output_size, dropout_rate):\n",
        "        super(neural_net, self).__init__()\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.hidden_num = hidden_num\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        for i in range(hidden_num):\n",
        "            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gelu(self.input_layer(x))\n",
        "        x = self.dropout(x)\n",
        "        for i in range(self.hidden_num):\n",
        "            x = self.gelu(self.hidden_layers[i](x))\n",
        "            x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "#CHECK TO SEE HOW GOOD MODEL IS\n",
        "def cross_validate(model, X, y, n_splits=5):\n",
        "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    scores = []\n",
        "\n",
        "    for train_indices, valid_indices in kfold.split(X):\n",
        "        # Split the data using .iloc for position-based indexing\n",
        "        X_train, X_valid = X.iloc[train_indices], X.iloc[valid_indices]\n",
        "        y_train, y_valid = y.iloc[train_indices], y.iloc[valid_indices]\n",
        "\n",
        "        # Convert to PyTorch datasets\n",
        "        train_dataset = vector_dataset(X_train, y_train)\n",
        "        valid_dataset = vector_dataset(X_valid, y_valid)\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "        valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "        # Define loss function and optimizer\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "        # Train the model on the training data\n",
        "        model.train()\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate the model on the validation data\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valid_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        accuracy = correct / total\n",
        "        scores.append(accuracy)\n",
        "\n",
        "    return scores\n",
        "\n",
        "def train_and_evaluate_model(model, train_loader, val_loader, test_loader, optimizer, criterion, scheduler, patience=5, num_epochs=50):\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_path = 'best_model.pth'  # File path for saving the best model\n",
        "    val_losses_per_epoch = []\n",
        "    # global label1,label2, label3, label4, label5, label6,model1, model2, model3, model_all\n",
        "    for param in model.parameters():\n",
        "        model_all=(param.device)\n",
        "    for epoch in range(num_epochs):  # number of epochs\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
        "            # label1=(inputs.device)\n",
        "            # label2=(labels.device)\n",
        "            # model1=(next(model.parameters()).device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        print(f'Training Loss: {avg_train_loss}')\n",
        "\n",
        "        model.eval()\n",
        "        valid_losses = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:  # Use validation data for validation\n",
        "                inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
        "                # label3=(inputs.device)\n",
        "                # label4=(labels.device)\n",
        "                # model2=(next(model.parameters()).device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels.squeeze())\n",
        "\n",
        "                valid_losses.append(loss.item())\n",
        "\n",
        "        avg_valid_loss = np.mean(valid_losses)\n",
        "        print(f'Validation Loss: {avg_valid_loss}')\n",
        "\n",
        "        scheduler.step(avg_valid_loss)\n",
        "        # Save the model at each epoch\n",
        "        epoch_model_path = f'model_epoch_{epoch + 1}.pth'\n",
        "        torch.save(model.state_dict(), epoch_model_path)\n",
        "\n",
        "        # Update the best model if validation loss improves\n",
        "        if avg_valid_loss < best_loss:\n",
        "            best_loss = avg_valid_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print('Early stopping triggered')\n",
        "                break\n",
        "\n",
        "    # Load the best model\n",
        "    # model.load_state_dict(torch.load(best_model_path))\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "\n",
        "    # Evaluate the best model\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:  # Use test data for final evaluation\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
        "\n",
        "            # label5=(inputs.device)\n",
        "            # label6=(labels.device)\n",
        "            # model3=(next(model.parameters()).device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate confusion matrix and classification report\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    class_report = classification_report(all_labels, all_preds,output_dict=True)\n",
        "    print('Confusion Matrix:')\n",
        "    print(conf_matrix)\n",
        "    print('Classification Report:')\n",
        "    print(class_report)\n",
        "    return conf_matrix, class_report\n",
        "\n",
        "import os\n",
        "\n",
        "def save_to_path(df, filename, folder=save_path):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    path = os.path.join(folder, f'{filename}.csv')\n",
        "\n",
        "    if os.path.exists(path):\n",
        "        print(f'File {path} already exists.')\n",
        "    else:\n",
        "        df.to_csv(path, index=True)\n",
        "        print(f'{filename} saved to {path}')\n",
        "\n",
        "def process_per_game_data(per_game_data, columns_to_keep, train_years, betting_date):\n",
        "    # Create 'DateHmTmVisTm' column\n",
        "    per_game_data['Game_Number'] = per_game_data.groupby(['Date', 'HmTm', 'VisTm']).cumcount() + 1\n",
        "    per_game_data['Game_ID'] = (\n",
        "        per_game_data['Date'].astype(str) +\n",
        "        per_game_data['HmTm'] +\n",
        "        per_game_data['VisTm'] +\n",
        "        per_game_data['Game_Number'].astype(str)\n",
        "    )\n",
        "    # Reset the current index\n",
        "    per_game_data = per_game_data.reset_index()\n",
        "    # Set the new index\n",
        "    per_game_data.set_index('Game_ID', inplace=True)\n",
        "    # Drop unnecessary columns\n",
        "    per_game_data = per_game_data[columns_to_keep]\n",
        "    # Create 'winner' column\n",
        "    per_game_data['winner'] = np.where(per_game_data['HmRuns'] > per_game_data['VisRuns'], 1, 0)\n",
        "\n",
        "    # Truncate rows based on the lowest train year\n",
        "    lowest_train_year = min(train_years)\n",
        "    per_game_data = per_game_data[per_game_data.index.str[:4] >= str(lowest_train_year)]\n",
        "    \n",
        "    # Convert betting_date from 'yyyy-mm-dd' to 'yyyymmdd' to match the index format\n",
        "    betting_date_formatted = betting_date.replace('-', '')\n",
        "    betting_date_formatted = str(int(betting_date_formatted)-1)\n",
        "    # Truncate rows based on the betting date using the formatted index\n",
        "    # Since the index is 'yyyymmddhometeamvisteam', we only compare the 'yyyymmdd' part\n",
        "    per_game_data = per_game_data[per_game_data.index.str[:8] <= betting_date_formatted]\n",
        "    \n",
        "    return per_game_data\n",
        "\n",
        "def process_vegas_odds(paths):\n",
        "\n",
        "    xlsx_dataframes = []\n",
        "    for i, path in enumerate(paths):\n",
        "        temp_df = pd.read_excel(path, skiprows=0 if i > 0 else 0)\n",
        "        temp_df.columns = temp_df.columns.str.replace('[^a-zA-Z0-9]', '')\n",
        "        year = path[-9:-5]  # extract year from file name\n",
        "        if 'Date' in temp_df.columns:\n",
        "            temp_df['Date'] = year + temp_df['Date'].astype(str).str.zfill(4)  # format date as yyyymmdd\n",
        "        xlsx_dataframes.append(temp_df)\n",
        "\n",
        "    try:\n",
        "        xlsx_dataframes = pd.concat(xlsx_dataframes, ignore_index=True)\n",
        "    except pd.errors.InvalidIndexError:\n",
        "        print('Error: Reindexing only valid with uniquely valued Index objects')\n",
        "\n",
        "    xlsx_dataframes = xlsx_dataframes[['Date', 'VH', 'Team', 'Open']]\n",
        "    home_teams_df = xlsx_dataframes[xlsx_dataframes['VH'] == 'H'].copy()\n",
        "    visiting_teams_df = xlsx_dataframes[xlsx_dataframes['VH'] == 'V'].copy()\n",
        "\n",
        "    home_teams_df.rename(columns={'Date': 'date', 'Team': 'home_team', 'Open': 'home_open'}, inplace=True)\n",
        "    visiting_teams_df.rename(columns={'Date': 'date', 'Team': 'visiting_team', 'Open': 'visiting_open'}, inplace=True)\n",
        "\n",
        "    # Merge on 'date'\n",
        "    xlsx_dataframes = pd.concat([home_teams_df.reset_index(drop=True), visiting_teams_df.reset_index(drop=True)], axis=1)\n",
        "    xlsx_dataframes = xlsx_dataframes.loc[:,~xlsx_dataframes.columns.duplicated()]\n",
        "    xlsx_dataframes = xlsx_dataframes[['date', 'home_team', 'visiting_team','home_open','visiting_open']]\n",
        "    xlsx_dataframes['Game_Number'] = xlsx_dataframes.groupby(['date', 'home_team', 'visiting_team']).cumcount() + 1\n",
        "    xlsx_dataframes['Game_ID'] = (\n",
        "        xlsx_dataframes['date'].astype(str) +\n",
        "        xlsx_dataframes['home_team'] +\n",
        "        xlsx_dataframes['visiting_team'] +\n",
        "        xlsx_dataframes['Game_Number'].astype(str)\n",
        "    )\n",
        "\n",
        "    xlsx_dataframes.set_index('Game_ID', inplace=True)\n",
        "\n",
        "    xlsx_dataframes.drop(['Game_Number','date','home_team','visiting_team'], axis=1, inplace=True)\n",
        "    print(xlsx_dataframes)\n",
        "\n",
        "    return xlsx_dataframes\n",
        "\n",
        "import time\n",
        "from requests.exceptions import RequestException\n",
        "\n",
        "def fetch_data_with_retry(fetch_func, data_type, max_retries=5, retry_delay=3):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            data = fetch_func(True)\n",
        "            print(f\"{data_type} data fetched successfully.\")\n",
        "            return data\n",
        "        except RequestException as e:\n",
        "            retries += 1\n",
        "            print(f\"Attempt {retries} failed with error: {e}. Retrying after {retry_delay} seconds...\")\n",
        "            time.sleep(retry_delay)\n",
        "            retry_delay *= 2  # Exponential backoff\n",
        "    print(f\"Failed to fetch {data_type} data after {max_retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "def process_scrambled_odds(df):\n",
        "    df['Game_Number'] = df.groupby(['date', 'home_team', 'visiting_team']).cumcount() + 1\n",
        "    df['Game_ID'] = (\n",
        "        df['date'].astype(str) +\n",
        "        df['home_team'] +\n",
        "        df['visiting_team'] +\n",
        "        df['Game_Number'].astype(str)\n",
        "    )\n",
        "\n",
        "    df.set_index('Game_ID', inplace=True)\n",
        "\n",
        "    # Fix: Correct the drop method usage by specifying the column indices correctly\n",
        "    columns_to_drop = df.columns[[0, 1]]\n",
        "    print(\"Dropping columns:\", columns_to_drop)\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "    return df\n",
        "\n",
        "def remove_columns_with_nan(df, NaN_cutoff_percentage):\n",
        "    NaN_cutoff = NaN_cutoff_percentage / 100.0\n",
        "    return df.loc[:, df.isnull().mean() < NaN_cutoff]\n",
        "\n",
        "def weighted_average(group, weights):\n",
        "    return pd.Series(\n",
        "        {col: np.average(group[col], weights=weights.loc[group.index]) for col in group.columns}\n",
        "    )\n",
        "\n",
        "# def replace_ids_with_stats_and_catalog_missing(per_game_data, player_id_columns, player_data_dicts, years):\n",
        "    # Ensure years is a list\n",
        "    if isinstance(years, int):\n",
        "        years = [years]\n",
        "\n",
        "    # Convert years to strings\n",
        "    years_str = [str(year) for year in years]\n",
        "\n",
        "    # Filter rows where the first 4 characters of the index are in years_str\n",
        "    per_game_data = per_game_data[per_game_data.index.str.slice(0, 4).isin(years_str)]\n",
        "    missing_player_ids = {}\n",
        "\n",
        "    for column in player_id_columns:\n",
        "        player_type = 'pitcher' if 'PchID' in column else 'batter'\n",
        "        player_data_dict = player_data_dicts[player_type]\n",
        "\n",
        "        # Map player IDs to stats\n",
        "\n",
        "        per_game_data[column] = per_game_data[column].map(player_data_dict)\n",
        "        # print(per_game_data[column])\n",
        "        # Split the stats dictionary into separate columns\n",
        "        stats_df = per_game_data[column].apply(pd.Series)\n",
        "        stats_df.columns = [f\"{column}_{col}\" for col in stats_df.columns]\n",
        "\n",
        "        # Concatenate the original DataFrame with the new columns\n",
        "        per_game_data = pd.concat([per_game_data, stats_df], axis=1)\n",
        "\n",
        "        # Check for missing player IDs\n",
        "        missing_ids = per_game_data[per_game_data[column].isna()].index.unique()\n",
        "        for missing_id in missing_ids:\n",
        "            if missing_id not in missing_player_ids:\n",
        "                missing_player_ids[missing_id] = []\n",
        "            missing_player_ids[missing_id].extend(\n",
        "                per_game_data[per_game_data[column] == missing_id].index.tolist()\n",
        "            )\n",
        "\n",
        "        # Drop the original player_ID column\n",
        "        per_game_data.drop(column, axis=1, inplace=True)\n",
        "\n",
        "    # Remove duplicates from the Game_ID lists and sort them\n",
        "    for player_id, game_ids in missing_player_ids.items():\n",
        "        missing_player_ids[player_id] = sorted(set(game_ids))\n",
        "\n",
        "    return per_game_data, missing_player_ids\n",
        "\n",
        "# def replace_player_ids_with_stats(per_game_data, player_data, player_id_columns):\n",
        "    # Calculate averages\n",
        "    player_avg = player_data.groupby(player_data.index).mean()\n",
        " \n",
        "\n",
        "\n",
        "# Continue with the rest of your code...\n",
        "\n",
        "    # Create a dictionary for each stat\n",
        "    stat_dicts = {stat: player_avg[stat].to_dict() for stat in player_avg.columns}\n",
        "\n",
        "    for column in player_id_columns:\n",
        "        # Replace the player_IDs in the column with the stats\n",
        "        for stat, stat_dict in stat_dicts.items():\n",
        "            per_game_data[column + '_' + stat] = per_game_data[column].map(stat_dict)\n",
        "\n",
        "        # Drop the original player_ID column\n",
        "        per_game_data.drop(column, axis=1, inplace=True)\n",
        "\n",
        "    return per_game_data\n",
        "\n",
        "# def replace_ids_with_stats_and_catalog_missing(per_game_data, player_id_columns, player_data_dicts):\n",
        "#     missing_player_ids = {}\n",
        "\n",
        "#     for column in player_id_columns:\n",
        "#         player_type = 'pitcher' if 'PchID' in column else 'batter'\n",
        "#         player_data_dict = player_data_dicts[player_type]\n",
        "\n",
        "#         # Preparing a DataFrame from player_data_dict for easier merging\n",
        "#         stats_df = pd.DataFrame.from_dict(player_data_dict, orient='index').reset_index().rename(columns={'index': column})\n",
        "        \n",
        "#         # Merging the stats into the main DataFrame\n",
        "#         per_game_data = per_game_data.merge(stats_df, on=column, how='left', suffixes=('', f'_{column}_stats'))\n",
        "\n",
        "#         # Identifying missing player IDs\n",
        "#         missing_ids_mask = per_game_data.loc[:, per_game_data.columns.str.contains(f'_{column}_stats')].isna().any(axis=1)\n",
        "#         missing_ids = per_game_data.loc[missing_ids_mask, column].dropna().unique()\n",
        "        \n",
        "#         for missing_id in missing_ids:\n",
        "#             if missing_id not in missing_player_ids:\n",
        "#                 missing_player_ids[missing_id] = []\n",
        "#             missing_player_ids[missing_id].extend(\n",
        "#                 per_game_data[per_game_data[column] == missing_id].index.tolist()\n",
        "#             )\n",
        "\n",
        "#         # Drop the original player_ID column\n",
        "#         per_game_data.drop(column, axis=1, inplace=True)\n",
        "\n",
        "#     # Remove duplicates from the Game_ID lists and sort them\n",
        "#     for player_id, game_ids in missing_player_ids.items():\n",
        "#         missing_player_ids[player_id] = sorted(set(game_ids))\n",
        "\n",
        "#     return per_game_data, missing_player_ids\n",
        "\n",
        "def replace_ids_with_stats_and_catalog_missing(per_game_data, player_id_columns, player_data_dicts):\n",
        "    missing_player_ids = {}\n",
        "\n",
        "    # Temporarily add the index as a column to preserve it through the merge\n",
        "    per_game_data = per_game_data.reset_index().rename(columns={'index': 'Game_ID'})\n",
        "\n",
        "    for column in player_id_columns:\n",
        "        player_type = 'pitcher' if 'PchID' in column else 'batter'\n",
        "        player_data_dict = player_data_dicts[player_type]\n",
        "\n",
        "        # Preparing a DataFrame from player_data_dict for easier merging\n",
        "        stats_df = pd.DataFrame.from_dict(player_data_dict, orient='index').reset_index().rename(columns={'index': column})\n",
        "        \n",
        "        # Merging the stats into the main DataFrame\n",
        "        per_game_data = per_game_data.merge(stats_df, on=column, how='left', suffixes=('', f'_{column}_stats'))\n",
        "\n",
        "        # Identifying missing player IDs\n",
        "        missing_ids_mask = per_game_data.loc[:, per_game_data.columns.str.contains(f'_{column}_stats')].isna().any(axis=1)\n",
        "        missing_ids = per_game_data.loc[missing_ids_mask, column].dropna().unique()\n",
        "        \n",
        "        for missing_id in missing_ids:\n",
        "            if missing_id not in missing_player_ids:\n",
        "                missing_player_ids[missing_id] = []\n",
        "            missing_player_ids[missing_id].extend(\n",
        "                per_game_data[per_game_data[column] == missing_id]['Game_ID'].tolist()\n",
        "            )\n",
        "\n",
        "        # Drop the original player_ID column\n",
        "        per_game_data.drop(column, axis=1, inplace=True)\n",
        "\n",
        "    # Remove duplicates from the Game_ID lists and sort them\n",
        "    for player_id, game_ids in missing_player_ids.items():\n",
        "        missing_player_ids[player_id] = sorted(set(game_ids))\n",
        "\n",
        "    # Set 'Game_ID' back as the index\n",
        "    per_game_data = per_game_data.set_index('Game_ID')\n",
        "\n",
        "    return per_game_data, missing_player_ids\n",
        "\n",
        "def label_encode(df):\n",
        "    le = LabelEncoder()\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = le.fit_transform(df[col])\n",
        "    return df\n",
        "\n",
        "def remove_excess_player_columns(player_data,columns_to_remove):\n",
        "    player_data = player_data.drop(columns=columns_to_remove)\n",
        "    return player_data\n",
        "\n",
        "def split_dataframe_data(per_game_data_reduced, train_years, validation_year, test_year):\n",
        "    # Create a mask for the training set\n",
        "    train_years = [str(year) for year in train_years]\n",
        "    validation_year = str(validation_year)\n",
        "    test_year = str(test_year)\n",
        "    train_mask = per_game_data_reduced.index.str.contains('|'.join(train_years))\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    X_train = per_game_data_reduced[train_mask].drop(['winner', 'home_odds', 'visiting_odds', 'HmRuns', 'VisRuns'], axis=1)\n",
        "    y_train = per_game_data_reduced.loc[train_mask, 'winner']\n",
        "\n",
        "    # Create a mask for the validation set\n",
        "    validation_mask = per_game_data_reduced.index.str.contains(validation_year)\n",
        "    X_validation = per_game_data_reduced[validation_mask].drop(['winner', 'home_odds', 'visiting_odds', 'HmRuns', 'VisRuns'], axis=1)\n",
        "    y_validation = per_game_data_reduced.loc[validation_mask, 'winner']\n",
        "\n",
        "    # Create a mask for the test set\n",
        "    test_mask = per_game_data_reduced.index.str.contains(test_year)\n",
        "    X_test = per_game_data_reduced[test_mask].drop(['winner', 'home_odds', 'visiting_odds', 'HmRuns', 'VisRuns'], axis=1)\n",
        "    y_test = per_game_data_reduced.loc[test_mask, 'winner']\n",
        "\n",
        "    return X_train, y_train, X_validation, y_validation, X_test, y_test\n",
        "\n",
        "def split_data_by_year(df, train_years, validation_year, test_year):\n",
        "    # Convert train_years and test_year to integers for the comparison\n",
        "    train_years_int = [int(year) for year in train_years]\n",
        "    test_year_int = int(test_year)\n",
        "\n",
        "    # Extract the year from the 'Game_ID' column\n",
        "    df['year'] = df.index.str[:4].astype(int)\n",
        "\n",
        "    # Create masks for splitting the data based on 'year'\n",
        "    train_mask = df['year'].isin(train_years_int)\n",
        "    val_mask = df['year'] == validation_year\n",
        "    test_mask = df['year'] == test_year_int\n",
        "\n",
        "    # Split the data\n",
        "    train_data = df[train_mask]\n",
        "    val_data = df[val_mask]\n",
        "    test_data = df[test_mask]\n",
        "\n",
        "    # Drop the 'year' column as it's no longer needed\n",
        "    df.drop('year', axis=1, inplace=True)\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def split_data(X, y, years):\n",
        "    # Check if 'years' is a list, if not, make it a list\n",
        "    if not isinstance(years, list):\n",
        "        years = [years]\n",
        "\n",
        "    # Convert all elements in 'years' to strings\n",
        "    years = [str(year) for year in years]\n",
        "\n",
        "    # Create a mask for the specified years\n",
        "    year_mask = X.index.str.contains('|'.join(years))\n",
        "\n",
        "    # Split the data for the specified years\n",
        "    X_years = X[year_mask]\n",
        "    y_years = y[year_mask]\n",
        "\n",
        "    return X_years, y_years\n",
        "\n",
        "def predict_game_outcome(game_ids, game_data, model, augment=False, base_bet=100, scaler=1):\n",
        "    num_games = len(game_ids)\n",
        "    results, bets, probss, skipped = [None]*num_games, [None]*num_games, [None]*num_games, [None]*num_games\n",
        "\n",
        "  \n",
        "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # model.eval()\n",
        "    # model.to(device)  # Move your model to the GPU\n",
        "    for i in range(num_games):\n",
        "        try:\n",
        "            game_vector = torch.tensor([game_data[i]], dtype=torch.float).to(device)\n",
        "            # game_vector = torch.tensor([game_data[i]], dtype=torch.float)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(game_vector)\n",
        "                probs = logits.softmax(dim=-1)\n",
        "                _, pred = torch.max(logits, 1)\n",
        "                pred = pred.item()\n",
        "                prob = probs[0][pred].item()\n",
        "            results[i] = (game_ids[i], 'True' if pred else 'False')\n",
        "            bets[i] = base_bet * prob * scaler if augment else base_bet\n",
        "            probss[i] = prob\n",
        "        except:\n",
        "            skipped.append(i)\n",
        "    return results, bets, skipped, probss\n",
        "\n",
        "def Impute(df, method):\n",
        "    # Create an imputer instance\n",
        "    imputer = SimpleImputer(strategy=method)\n",
        "    # Fit and transform the DataFrame, but keep the index\n",
        "    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns, index=df.index)\n",
        "    return df_imputed\n",
        "\n",
        "class Better:\n",
        "    def __init__(self, initial_wallet=1000):\n",
        "        self.wallet = initial_wallet\n",
        "        self.wallet_history = [initial_wallet]  # Store the initial wallet balance\n",
        "        self.bet_history = []\n",
        "    def bet(self, game_indices, bet_on_home_team, amount_bet, game_data, prob, augment=False, scaler=1):\n",
        "        # Ensure the bettor has enough money in the wallet\n",
        "        amount_bet = max(self.wallet * 0.01, 10)\n",
        "        # amount_bet = 10\n",
        "        amount_bet = amount_bet * prob * scaler if augment else amount_bet\n",
        "        if amount_bet > self.wallet:\n",
        "            print(\"Insufficient funds for this bet.\")\n",
        "            return\n",
        "\n",
        "        # Retrieve the game data\n",
        "        games = game_data.loc[game_indices]\n",
        "\n",
        "        # If games is a DataFrame, iterate over its rows\n",
        "        if isinstance(games, pd.DataFrame):\n",
        "            game_iter = games.iterrows()\n",
        "        else:  # If games is a Series (single row), create a one-item iterator\n",
        "            game_iter = [(game_indices, games)]\n",
        "\n",
        "        for _, game in game_iter:\n",
        "            # Retrieve the odds\n",
        "            home_odds = game['home_odds']\n",
        "            away_odds = game['visiting_odds']\n",
        "\n",
        "            # Determine the odds based on the team bet on\n",
        "            odds = home_odds if bet_on_home_team else away_odds\n",
        "\n",
        "            # Determine if the bet was a win or loss\n",
        "            bet_won = (game['winner'] == bet_on_home_team)\n",
        "\n",
        "            # Calculate the amount gained or lost\n",
        "            if bet_won:\n",
        "                if odds > 0:\n",
        "                    # For positive odds, the gain is the odds * the amount bet / 100\n",
        "                    gain = amount_bet * odds / 100\n",
        "                else:\n",
        "                    # For negative odds, the gain is the amount bet / (odds / -100)\n",
        "                    gain = amount_bet / (odds / -100)\n",
        "                self.wallet += gain\n",
        "            else:\n",
        "                # If the bet was lost, the loss is the amount bet\n",
        "                self.wallet -= amount_bet\n",
        "\n",
        "            # Store the new wallet balance\n",
        "            self.wallet_history.append(self.wallet)\n",
        "            self.bet_history.append(amount_bet)\n",
        "        return self.wallet\n",
        "\n",
        "def combine_odds(per_game_data, vegas_odds, scrambled_odds):\n",
        "    # Merge vegas_odds and scrambled_odds into per_game_data\n",
        "    per_game_data = per_game_data.merge(vegas_odds, how='left', left_index=True, right_index=True)\n",
        "    per_game_data = per_game_data.merge(scrambled_odds, how='left', left_index=True, right_index=True, suffixes=('_vegas', '_scrambled'))\n",
        "\n",
        "    # Create new columns 'home_odds' and 'visiting_odds' where vegas_odds takes precedence\n",
        "    per_game_data['home_odds'] = per_game_data['home_open_vegas'].combine_first(per_game_data['home_open_scrambled'])\n",
        "    per_game_data['visiting_odds'] = per_game_data['visiting_open_vegas'].combine_first(per_game_data['visiting_open_scrambled'])\n",
        "\n",
        "    # Fill any remaining NaNs with -110\n",
        "    per_game_data['home_odds'].fillna(-110, inplace=True)\n",
        "    per_game_data['visiting_odds'].fillna(-110, inplace=True)\n",
        "\n",
        "    # Drop the original odds columns\n",
        "    per_game_data.drop(columns=['home_open_vegas', 'visiting_open_vegas', 'home_open_scrambled', 'visiting_open_scrambled'], inplace=True)\n",
        "\n",
        "    # Reset the index before returning\n",
        "\n",
        "\n",
        "    # return per_game_data[['home_odds', 'visiting_odds']]\n",
        "    return per_game_data\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import shuffle\n",
        "import copy\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def permutation_importance(model, loader, criterion, metric):\n",
        "    model.eval()\n",
        "    original_score = metric(model, loader, criterion)\n",
        "    importances = []\n",
        "    for i in range(loader.dataset.X.size(1)):  # Assuming 'X' is the attribute storing your input data\n",
        "        temp = loader.dataset.X.clone()\n",
        "        temp[:, i] = torch.randperm(temp.size(0))\n",
        "        temp_dataset = vector_dataset(temp, loader.dataset.y)  # Assuming 'y' is the attribute storing your labels\n",
        "        temp_loader = DataLoader(temp_dataset, batch_size=64, shuffle=False)\n",
        "        score = metric(model, temp_loader, criterion)\n",
        "        importances.append(original_score - score)\n",
        "    return importances\n",
        "\n",
        "def automated_feature_selection(model, train_loader, val_loader, test_loader, optimizer, criterion, metric, patience=5, num_epochs=50):\n",
        "    best_score = 0\n",
        "    best_model = None\n",
        "    important_features = list(range(len(train_loader.dataset[0][0])))\n",
        "    while len(important_features) > 0:\n",
        "        # Train the model\n",
        "        conf_matrix, class_report = train_and_evaluate_model(model, train_loader, val_loader, test_loader, optimizer, criterion, patience, num_epochs)\n",
        "        # Calculate the score\n",
        "        score = class_report['accuracy']  # Assuming class_report is a dictionary with 'accuracy' key\n",
        "        # If the score has improved, update the best score and best model\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_model = copy.deepcopy(model)\n",
        "        else:\n",
        "            # If the score has not improved, add the last removed feature back and break the loop\n",
        "            important_features.append(last_removed_feature)\n",
        "            break\n",
        "        # Calculate the permutation importance\n",
        "        importances = permutation_importance(model, val_loader.dataset.tensors[0][:, important_features], val_loader.dataset.tensors[1], metric)\n",
        "        # Remove the least important feature\n",
        "        last_removed_feature = important_features.pop(np.argmin(importances))\n",
        "    return best_model, important_features\n",
        "# usage\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer, MaxAbsScaler, PowerTransformer, QuantileTransformer\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer, MaxAbsScaler, PowerTransformer, QuantileTransformer\n",
        "\n",
        "def scale_data(X, method='minmax'):\n",
        "    \"\"\"\n",
        "    Scales the data using various scaling methods.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Data as a pandas DataFrame.\n",
        "    - method: The scaling method to use ('minmax', 'standard', 'robust', 'normalizer', 'maxabs', 'power', 'quantile').\n",
        "\n",
        "    Returns:\n",
        "    - X_scaled: Scaled data as a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    scalers = {\n",
        "        'minmax': MinMaxScaler(),\n",
        "        'standard': StandardScaler(),\n",
        "        'robust': RobustScaler(),\n",
        "        'normalizer': Normalizer(),\n",
        "        'maxabs': MaxAbsScaler(),\n",
        "        'power': PowerTransformer(),\n",
        "        'quantile': QuantileTransformer(output_distribution='normal')\n",
        "    }\n",
        "\n",
        "    if method not in scalers:\n",
        "        raise ValueError(f\"Method should be one of {list(scalers.keys())}\")\n",
        "\n",
        "    scaler = scalers[method]\n",
        "\n",
        "    # Fit and transform the data.\n",
        "    X_scaled = pd.DataFrame(scaler.fit_transform(X.values), columns=X.columns, index=X.index)\n",
        "\n",
        "    return X_scaled\n",
        "\n",
        "    from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def process_data(df, labels, thresh=0.8, columns_to_drop=None):\n",
        "    # Ensure labels is a string and not a column itself\n",
        "    if not isinstance(labels, str):\n",
        "        raise ValueError(\"labels parameter should be a string representing the column name of the target variable\")\n",
        "\n",
        "    # Separate the target variable\n",
        "    y = df[labels].copy()\n",
        "    df_clean = df.drop(columns=columns_to_drop + [labels] if columns_to_drop is not None else [labels])\n",
        "\n",
        "    # Impute missing values for numerical columns\n",
        "    num_columns = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
        "    num_imputer = SimpleImputer(strategy='mean')\n",
        "    df_clean[num_columns] = num_imputer.fit_transform(df_clean[num_columns])\n",
        "\n",
        "    # Impute missing values for categorical columns\n",
        "    cat_columns = df_clean.select_dtypes(include=['object', 'string', 'bool']).columns\n",
        "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    df_clean[cat_columns] = cat_imputer.fit_transform(df_clean[cat_columns])\n",
        "\n",
        "    # Apply threshold for dropping columns with too many NAs\n",
        "    threshold = thresh * len(df)\n",
        "    df_clean = df_clean.dropna(axis=1, thresh=threshold)\n",
        "\n",
        "    # Label encoding for categorical columns\n",
        "    le = LabelEncoder()\n",
        "    for column in cat_columns:\n",
        "        # Ensure no NaN values are present before label encoding\n",
        "        df_clean[column] = le.fit_transform(df_clean[column])\n",
        "\n",
        "    # Concatenate the cleaned df_clean with the target y\n",
        "    combined = pd.concat([df_clean, y], axis=1)\n",
        "\n",
        "    # Drop rows with any remaining NaN values\n",
        "    combined_clean = combined.dropna()\n",
        "\n",
        "    # Separate the features and target again\n",
        "    df_clean = combined_clean.drop(columns=[labels])\n",
        "    y = combined_clean[labels]\n",
        "\n",
        "    # Convert boolean to int if necessary\n",
        "    if y.dtype == 'boolean':\n",
        "        y = y.astype(int)\n",
        "\n",
        "    return df_clean, y\n",
        "\n",
        "\n",
        "def aggregate_player_data(data, playerid):\n",
        "    # Set the playerid as the index if it's not already\n",
        "    # if isinstance(years, int):\n",
        "    #     years = [years]\n",
        "    # else:\n",
        "    #     years = list(map(int, years))\n",
        "    # data = data[data['year_ID'].isin(years)]\n",
        "    numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
        "    categorical_cols = data.select_dtypes(exclude=['int64', 'float64']).columns\n",
        "\n",
        "    # Impute numerical columns with mean\n",
        "    data[numerical_cols] = Impute(data[numerical_cols], 'mean')\n",
        "\n",
        "    # Impute categorical columns with the most frequent value\n",
        "    data[categorical_cols] = Impute(data[categorical_cols], 'most_frequent')\n",
        "\n",
        "    # Group by playerid and aggregate: mean for numerical columns, most frequent for categorical columns\n",
        "    data_aggregated = data.groupby(playerid).agg({**{col: 'mean' for col in numerical_cols}, \n",
        "                                                 **{col: lambda x: x.mode()[0] if not x.empty else None for col in categorical_cols}})\n",
        "\n",
        "    # Apply label encoding to categorical columns\n",
        "    data_aggregated = label_encode_columns(data_aggregated, categorical_cols)\n",
        " \n",
        "    return data_aggregated\n",
        "\n",
        "# Assuming batter_data and pitcher_data are pandas DataFrames and playerid is the column with player IDs\n",
        "# If you need to label encode the categorical columns after aggregation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Function to label encode categorical columns\n",
        "def label_encode_columns(df, categorical_columns):\n",
        "    le = LabelEncoder()\n",
        "    for col in categorical_columns:\n",
        "        # Fill NaN with a placeholder string and encode\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "    return df\n",
        "\n",
        "def prepare_data(batter_data, pitcher_data, per_game_data, playerid, years):\n",
        "    # Aggregate the player data for the specified years\n",
        "    batter_aggregated = aggregate_player_data(batter_data, playerid, years)\n",
        "    pitcher_aggregated = aggregate_player_data(pitcher_data, playerid, years)\n",
        "\n",
        "    # Check if the index has a 0 value and if so, remove the corresponding row\n",
        "    if 0 in batter_aggregated.index:\n",
        "        batter_aggregated.drop(index=0, inplace=True)\n",
        "    if 0 in pitcher_aggregated.index:\n",
        "        pitcher_aggregated.drop(index=0, inplace=True)\n",
        "\n",
        "\n",
        "    pitcher_columns = ['HmStPchID', 'VisStPchID']\n",
        "    batter_columns = ['HmBat1ID', 'HmBat2ID', 'HmBat3ID', 'HmBat4ID', 'HmBat5ID', 'HmBat6ID', 'HmBat7ID', 'HmBat8ID', 'HmBat9ID', 'VisBat1ID', 'VisBat2ID', 'VisBat3ID', 'VisBat4ID', 'VisBat5ID', 'VisBat6ID', 'VisBat7ID', 'VisBat8ID', 'VisBat9ID']\n",
        "\n",
        "    # Create separate dictionaries for the data\n",
        "    player_data_dictionary = {\n",
        "        'pitcher': pitcher_aggregated.to_dict('index'),\n",
        "        'batter': batter_aggregated.to_dict('index')\n",
        "    }\n",
        "\n",
        "    # Call the function with the player ID columns\n",
        "    player_id_columns = pitcher_columns + batter_columns\n",
        "\n",
        "    # Replace player IDs with stats and catalog missing IDs\n",
        "    per_game_finished, missing_player_ids = replace_ids_with_stats_and_catalog_missing(per_game_data, player_id_columns, player_data_dictionary)\n",
        "\n",
        "    return per_game_finished, missing_player_ids\n",
        "\n",
        "def list_old_bwar_data():\n",
        "    # The URL of the page where the zip files are listed\n",
        "    url = 'https://www.baseball-reference.com/data/'\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        # Find all the <a> tags in the HTML\n",
        "        a_tags = soup.find_all('a')\n",
        "        \n",
        "        # Filter out the <a> tags that link to .zip files and prepend the base URL to form a complete URL\n",
        "        zip_links = [url + a['href'] for a in a_tags if a['href'].endswith('.zip')]\n",
        "        \n",
        "        # Print out the list of zip file links\n",
        "        # for link in zip_links:\n",
        "            # print(link)\n",
        "    else:\n",
        "        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
        "    return zip_links\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def find_most_recent_zip(zip_links, input_date):\n",
        "    from datetime import datetime  # Ensure datetime is imported\n",
        "\n",
        "    # Convert input_date string to a datetime object\n",
        "    input_date = datetime.strptime(input_date, '%Y-%m-%d')\n",
        "    \n",
        "    # Initialize variable to keep track of the most recent date before input_date\n",
        "    most_recent_date = None\n",
        "    most_recent_link = None\n",
        "    \n",
        "    # Iterate over the list of zip links\n",
        "    for link in zip_links:\n",
        "        # Extract the date from the link (assuming the date is in the format YYYY-MM-DD)\n",
        "        parts = link.split('/')[-1].split('-')\n",
        "        link_date_str = '-'.join(parts[1:4]).replace('.zip', '')\n",
        "        try:\n",
        "            # Convert the extracted date string to a datetime object\n",
        "            link_date = datetime.strptime(link_date_str, '%Y-%m-%d')\n",
        "            \n",
        "            # Check if this date is more recent than the current most_recent_date and before the input_date\n",
        "            if (most_recent_date is None or link_date > most_recent_date) and link_date < input_date:\n",
        "                most_recent_date = link_date\n",
        "                most_recent_link = link\n",
        "        except ValueError:\n",
        "            # If there is a ValueError, it means the conversion to a datetime object failed, likely due to incorrect format\n",
        "            continue  # Skip this link if the date cannot be parsed\n",
        "    \n",
        "    # Return the most recent link\n",
        "    return most_recent_link\n",
        "\n",
        "def scrape_old_bwar_data(most_recent_link):\n",
        "    # URL of the zip file\n",
        "    zip_file_url = most_recent_link\n",
        "\n",
        "    # The path to the folder where you want to save the extracted files\n",
        "    # Make sure this folder exists or create it with os.makedirs()\n",
        "    download_folder = os.path.join(os.getcwd(), 'old_bwar_data')\n",
        "    if not os.path.exists(download_folder):\n",
        "        os.makedirs(download_folder)\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(zip_file_url)\n",
        "\n",
        "    # Initialize a list to hold the content of each text file\n",
        "    text_files_content = []\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Use BytesIO to read the downloaded zip file\n",
        "        zip_file_bytes = io.BytesIO(response.content)\n",
        "\n",
        "        # Open the zip file\n",
        "        with zipfile.ZipFile(zip_file_bytes, 'r') as zip_ref:\n",
        "            # Extract all the contents into the specified directory\n",
        "            zip_ref.extractall(download_folder)\n",
        "            # List all the file names in the zip file\n",
        "            for file_name in zip_ref.namelist():\n",
        "                # Check if the file is a text file\n",
        "                if file_name.endswith('.txt'):\n",
        "                    # Read the text file\n",
        "                    with zip_ref.open(file_name) as text_file:\n",
        "                        text_content = text_file.read()\n",
        "                        # Add the text content to the list\n",
        "                        text_files_content.append(text_content.decode('utf-8'))\n",
        "    else:\n",
        "        print(f\"Failed to download the zip file. Status code: {response.status_code}\")\n",
        "\n",
        "    # Return the list of text file contents\n",
        "    return text_files_content\n",
        "\n",
        "\n",
        "def Starting_Lineup(date):\n",
        "\n",
        "\n",
        "    \"\"\"# Functions\n",
        "\n",
        "    # Importing Data\n",
        "\n",
        "    # Formatting player data\n",
        "\n",
        "    # Formatting per game data\n",
        "    \"\"\"\n",
        "\n",
        "    # from bs4 import BeautifulSoup\n",
        "    import re\n",
        "    # import pybaseball as pyb\n",
        "    # Fetch the HTML content from the webpage\n",
        "    url = f'https://www.mlb.com/starting-lineups/{date}'\n",
        "    response = requests.get(url)\n",
        "    html_content = response.text\n",
        "\n",
        "\n",
        "    # Assuming `html_content` contains the HTML source you provided\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Regular expression to extract numeric ID from player URL\n",
        "    player_id_pattern = re.compile(r'/player/.*-(\\d+)$')\n",
        "\n",
        "    # Collect all MLBAM IDs\n",
        "    all_mlbam_ids = set()\n",
        "\n",
        "    # Find all game containers\n",
        "    games = soup.find_all('div', class_='starting-lineups__matchup')\n",
        "\n",
        "    for game in games:\n",
        "        # Extract pitcher and batter IDs\n",
        "        pitchers = game.find_all('a', class_='starting-lineups__pitcher--link')\n",
        "        batters = game.find_all('a', class_='starting-lineups__player--link')\n",
        "        player_links = pitchers + batters\n",
        "\n",
        "        for player_link in player_links:\n",
        "            player_id_match = re.search(player_id_pattern, player_link['href'])\n",
        "            if player_id_match:\n",
        "                all_mlbam_ids.add(int(player_id_match.group(1)))\n",
        "\n",
        "    # Perform reverse lookup to get Retro IDs\n",
        "    all_player_info = pyb.playerid_reverse_lookup(list(all_mlbam_ids), key_type='mlbam')\n",
        "    mlbam_to_retro = all_player_info.set_index('key_mlbam')['key_retro'].to_dict()\n",
        "\n",
        "\n",
        "    game_matchups = {}\n",
        "\n",
        "    # Use enumerate to get both the index and the game object for uniqueness\n",
        "    for index, game in enumerate(games):\n",
        "        teams = game.find_all('span', class_='starting-lineups__team-name')\n",
        "        away_team_code = teams[0].find('a')['data-tri-code']\n",
        "        home_team_code = teams[2].find('a')['data-tri-code']\n",
        "\n",
        "        # Extract pitcher IDs\n",
        "        pitchers = game.find_all('a', class_='starting-lineups__pitcher--link')\n",
        "        if len(pitchers) < 3:  # Ensure there are at least 3 links, indicating both away and home pitchers are present\n",
        "            continue  # Skip this game if there aren't enough pitcher links\n",
        "        away_pitcher_id = re.search(player_id_pattern, pitchers[0]['href']).group(1) if len(pitchers) > 0 else None\n",
        "        home_pitcher_id = re.search(player_id_pattern, pitchers[2]['href']).group(1) if len(pitchers) > 1 else None\n",
        "        if not away_pitcher_id or not home_pitcher_id:\n",
        "            continue\n",
        "        # Extract batters IDs for both teams\n",
        "        away_batters = game.find('ol', class_='starting-lineups__team--away').find_all('a', class_='starting-lineups__player--link')\n",
        "        home_batters = game.find('ol', class_='starting-lineups__team--home').find_all('a', class_='starting-lineups__player--link')\n",
        "\n",
        "        away_batters_ids = [re.search(player_id_pattern, batter['href']).group(1) for batter in away_batters]\n",
        "        home_batters_ids = [re.search(player_id_pattern, batter['href']).group(1) for batter in home_batters]\n",
        "        if not away_batters_ids or not home_batters_ids:\n",
        "            continue\n",
        "        # Populate the game_matchups dictionary with a unique game key\n",
        "        game_key = f\"{away_team_code}@{home_team_code}_{index}\"\n",
        "        game_matchups[game_key] = {\n",
        "            'away_team': away_team_code,\n",
        "            'home_team': home_team_code,\n",
        "            'away_pitcher': {'mlbam_id': away_pitcher_id, 'retro_id': mlbam_to_retro.get(int(away_pitcher_id)) if away_pitcher_id else None},\n",
        "            'home_pitcher': {'mlbam_id': home_pitcher_id, 'retro_id': mlbam_to_retro.get(int(home_pitcher_id)) if home_pitcher_id else None},\n",
        "            'away_players': [{'mlbam_id': id, 'retro_id': mlbam_to_retro.get(int(id))} for id in away_batters_ids],\n",
        "            'home_players': [{'mlbam_id': id, 'retro_id': mlbam_to_retro.get(int(id))} for id in home_batters_ids]\n",
        "        }\n",
        "\n",
        "    # print(game_matchups)\n",
        "\n",
        "    \"\"\"# Set-up Neural Network\n",
        "\n",
        "    # Predict the outcome of the season and calculate profit\n",
        "    \"\"\"\n",
        "\n",
        "    # Define a mapping of incorrect team abbreviations to correct ones\n",
        "    team_nomenclature_mapping = {\n",
        "        'CHC': 'CHN',  # Chicago Cubs\n",
        "        'WSH': 'WAS',  # Washington Nationals\n",
        "        'NYM': 'NYN',  # New York Mets\n",
        "        'BOS': 'BOS',  # Boston Red Sox\n",
        "        'ATL': 'ATL',  # Atlanta Braves\n",
        "        'TEX': 'TEX',  # Texas Rangers\n",
        "        'HOU': 'HOU',  # Houston Astros\n",
        "        'KC ': 'KCA',   # Kansas City Royals\n",
        "        'LAA': 'ANA',  # Los Angeles Angels\n",
        "        'MIN': 'MIN',  # Minnesota Twins\n",
        "        'MIL': 'MIL',  # Milwaukee Brewers\n",
        "        'AZ ': 'ARI',   # Arizona Diamondbacks\n",
        "        'SD ': 'SDN',   # San Diego Padres\n",
        "        'TB ': 'TBA',   # Tampa Bay Rays\n",
        "        'OAK': 'OAK',  # Oakland Athletics\n",
        "        'LAD': 'LAN',  # Los Angeles Dodgers\n",
        "        'SF ': 'SFN',   # San Francisco Giants\n",
        "        'SEA': 'SEA',  # Seattle Mariners\n",
        "        'CWS': 'CHA',  # Chicago White Sox\n",
        "        'CLE': 'CLE',  # Cleveland Guardians\n",
        "        'DET': 'DET',  # Detroit Tigers\n",
        "        'PHI': 'PHI',  # Philadelphia Phillies\n",
        "        'PIT': 'PIT',  # Pittsburgh Pirates\n",
        "        'STL': 'SLN',  # St. Louis Cardinals\n",
        "        'TOR': 'TOR',  # Toronto Blue Jays\n",
        "        'BAL': 'BAL',  # Baltimore Orioles\n",
        "        'MIA': 'MIA',  # Miami Marlins\n",
        "        'CIN': 'CIN',  # Cincinnati Reds\n",
        "        'COL': 'COL',  # Colorado Rockies\n",
        "        'NYY': 'NYA',  # New York Yankees\n",
        "        'AZ': 'ARI',   # Arizona Diamondbacks\n",
        "        'SD': 'SDN',   # San Diego Padres\n",
        "        'TB': 'TBA',   # Tampa Bay Rays\n",
        "        'SF': 'SFN',   # San Francisco Giants\n",
        "        'KC': 'KCA',   # Kansas City Royals\n",
        "        # ... add other mappings as needed\n",
        "    }\n",
        "    # ... (previous code)\n",
        "\n",
        "    # Create a list to hold all game data\n",
        "    games_data = []\n",
        "\n",
        "    # Create a dictionary to keep track of game counts for the day\n",
        "    game_counts = {}\n",
        "\n",
        "    # Iterate over the game matchups to populate the games_data list\n",
        "    for game, details in game_matchups.items():\n",
        "        # Correct the team abbreviations\n",
        "        home_team_corrected = team_nomenclature_mapping.get(details['home_team'], details['home_team'])\n",
        "        away_team_corrected = team_nomenclature_mapping.get(details['away_team'], details['away_team'])\n",
        "\n",
        "        # Create a unique game identifier based on the corrected teams and the number of games they've played that day\n",
        "        game_count_key = f\"{home_team_corrected}{away_team_corrected}\"\n",
        "        game_counts[game_count_key] = game_counts.get(game_count_key, 0) + 1\n",
        "        game_id = f\"{date.replace('-', '')}{home_team_corrected}{away_team_corrected}{str(game_counts[game_count_key])}\"  # Ensure the game number is two digits\n",
        "\n",
        "        # Define game_data inside the loop to reset it for each game\n",
        "        game_data = {\n",
        "            'Game_ID': game_id,\n",
        "            'HmTm': home_team_corrected,\n",
        "            'VisTm':away_team_corrected,\n",
        "            'HmStPchID': details['home_pitcher'].get('retro_id') if isinstance(details['home_pitcher'], dict) else None,\n",
        "            'VisStPchID': details['away_pitcher'].get('retro_id') if isinstance(details['away_pitcher'], dict) else None\n",
        "        }\n",
        "\n",
        "        # Add Home team batters in the order they were scraped\n",
        "        for i, player in enumerate(details['home_players'], start=1):\n",
        "            game_data[f'HmBat{i}ID'] = player.get('retro_id') if isinstance(player, dict) else None\n",
        "\n",
        "        # Add Away team batters in the order they were scraped\n",
        "        for i, player in enumerate(details['away_players'], start=1):\n",
        "            game_data[f'VisBat{i}ID'] = player.get('retro_id') if isinstance(player, dict) else None\n",
        "\n",
        "        # Append the game_data dictionary to the games_data list once per game\n",
        "        games_data.append(game_data)\n",
        "\n",
        "    # ... (rest of your code to convert games_data to DataFrame and export)\n",
        "\n",
        "    # Convert the games_data list to a DataFrame\n",
        "    games_df = pd.DataFrame(games_data)\n",
        "\n",
        "    # Define the column order explicitly to match the desired structure\n",
        "    column_order = ['Game_ID', 'HmStPchID', 'VisStPchID'] + \\\n",
        "                [f'HmBat{i}ID' for i in range(1, 10)] + \\\n",
        "                [f'VisBat{i}ID' for i in range(1, 10)] + \\\n",
        "                ['HmTm', 'VisTm']\n",
        "\n",
        "    # Reorder the DataFrame columns\n",
        "    games_df = games_df[column_order]\n",
        "\n",
        "    # Export the DataFrame to an Excel file\n",
        "    games_df.to_excel('pybaseball_v3_saved/Scraping_Games.xlsx', index=False)\n",
        "    return games_df\n",
        "\n",
        "def bwar_bat(return_all: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Get data from war_daily_bat table. Returns WAR, its components, and a few other useful stats.\n",
        "    To get all fields from this table, supply argument return_all=True.\n",
        "    \"\"\"\n",
        "    url = \"http://www.baseball-reference.com/data/war_daily_bat.txt\"\n",
        "    s = requests.get(url)\n",
        "    if return_all:\n",
        "        c = pd.read_csv(io.StringIO(s.text))\n",
        "        return c\n",
        "    else:\n",
        "        cols_to_keep = ['name_common', 'mlb_ID', 'player_ID', 'year_ID', 'team_ID', 'stint_ID', 'lg_ID',\n",
        "                        'pitcher','G', 'PA', 'salary', 'runs_above_avg', 'runs_above_avg_off','runs_above_avg_def',\n",
        "                        'WAR_rep','WAA','WAR']\n",
        "        c = pd.read_csv(io.StringIO(s.text), usecols=cols_to_keep)\n",
        "        return c\n",
        "\n",
        "\n",
        "def bwar_pitch(return_all: bool=True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Get data from war_daily_pitch table. Returns WAR, its components, and a few other useful stats.\n",
        "    To get all fields from this table, supply argument return_all=True.\n",
        "    \"\"\"\n",
        "    url = \"http://www.baseball-reference.com/data/war_daily_pitch.txt\"\n",
        "    s = requests.get(url)\n",
        "    if return_all:\n",
        "        c = pd.read_csv(io.StringIO(s.text))\n",
        "        return c\n",
        "    else:\n",
        "        cols_to_keep = ['name_common', 'mlb_ID', 'player_ID', 'year_ID', 'team_ID', 'stint_ID', 'lg_ID',\n",
        "                        'G', 'GS', 'RA','xRA', 'BIP', 'BIP_perc','salary', 'ERA_plus', 'WAR_rep', 'WAA',\n",
        "                        'WAA_adj','WAR']\n",
        "        c = pd.read_csv(io.StringIO(s.text), usecols=cols_to_keep)\n",
        "        return c\n",
        "    \n",
        "def fetch_and_process_game_results(date):\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    from collections import defaultdict\n",
        "    from datetime import datetime\n",
        "\n",
        "    # URL of the page to scrape\n",
        "    url = f'https://www.mlb.com/scores/{date}'\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all game containers\n",
        "    games = soup.find_all('div', {'data-test-mlb': 'singleGameContainer'})\n",
        "    results = []\n",
        "    # Loop through each game container to extract the team names and runs\n",
        "    for game in games:\n",
        "        # Extract team abbreviations and runs\n",
        "        teams = game.find_all('div', {'class': 'TeamWrappersstyle__MobileTeamWrapper-sc-uqs6qh-1 jXnGyx'})\n",
        "        home_runs = game.find_all('div', {'class': 'lineScorestyle__StyledInningCell-sc-1d7bghs-1 jPCzPZ'})\n",
        "        visiting_runs = game.find_all('div', {'class': 'lineScorestyle__StyledInningCell-sc-1d7bghs-1 ddFUsj'})\n",
        "        # Assuming each game container has exactly two teams and at least two run entries\n",
        "        if len(teams) >= 2:\n",
        "            visiting_team_abbr = teams[0].text.strip()\n",
        "            home_team_abbr = teams[1].text.strip()\n",
        "            \n",
        "            visiting_team_runs = visiting_runs[-1].text.strip() if visiting_runs else \"N/A\"\n",
        "            home_team_runs = home_runs[-1].text.strip() if home_runs else \"N/A\"\n",
        "            \n",
        "            # Skip the game if either team has \"N/A\" for runs\n",
        "            if visiting_team_runs == \"N/A\" or home_team_runs == \"N/A\":\n",
        "                continue\n",
        "            \n",
        "            result = {\n",
        "                \"home_team\": home_team_abbr,\n",
        "                \"away_team\": visiting_team_abbr,\n",
        "                \"HmRuns\": home_team_runs,\n",
        "                \"VisRuns\": visiting_team_runs\n",
        "            }\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "    team_nomenclature_mapping = {\n",
        "            'CHC': 'CHN',  # Chicago Cubs\n",
        "            'WSH': 'WAS',  # Washington Nationals\n",
        "            'NYM': 'NYN',  # New York Mets\n",
        "            'BOS': 'BOS',  # Boston Red Sox\n",
        "            'ATL': 'ATL',  # Atlanta Braves\n",
        "            'TEX': 'TEX',  # Texas Rangers\n",
        "            'HOU': 'HOU',  # Houston Astros\n",
        "            'KC ': 'KCA',   # Kansas City Royals\n",
        "            'LAA': 'ANA',  # Los Angeles Angels\n",
        "            'MIN': 'MIN',  # Minnesota Twins\n",
        "            'MIL': 'MIL',  # Milwaukee Brewers\n",
        "            'AZ ': 'ARI',   # Arizona Diamondbacks\n",
        "            'SD ': 'SDN',   # San Diego Padres\n",
        "            'TB ': 'TBA',   # Tampa Bay Rays\n",
        "            'OAK': 'OAK',  # Oakland Athletics\n",
        "            'LAD': 'LAN',  # Los Angeles Dodgers\n",
        "            'SF ': 'SFN',   # San Francisco Giants\n",
        "            'SEA': 'SEA',  # Seattle Mariners\n",
        "            'CWS': 'CHA',  # Chicago White Sox\n",
        "            'CLE': 'CLE',  # Cleveland Guardians\n",
        "            'DET': 'DET',  # Detroit Tigers\n",
        "            'PHI': 'PHI',  # Philadelphia Phillies\n",
        "            'PIT': 'PIT',  # Pittsburgh Pirates\n",
        "            'STL': 'SLN',  # St. Louis Cardinals\n",
        "            'TOR': 'TOR',  # Toronto Blue Jays\n",
        "            'BAL': 'BAL',  # Baltimore Orioles\n",
        "            'MIA': 'MIA',  # Miami Marlins\n",
        "            'CIN': 'CIN',  # Cincinnati Reds\n",
        "            'COL': 'COL',  # Colorado Rockies\n",
        "            'NYY': 'NYA',  # New York Yankees\n",
        "            'AZ': 'ARI',   # Arizona Diamondbacks\n",
        "            'SD': 'SDN',   # San Diego Padres\n",
        "            'TB': 'TBA',   # Tampa Bay Rays\n",
        "            'SF': 'SFN',   # San Francisco Giants\n",
        "            'KC': 'KCA',   # Kansas City Royals\n",
        "            # ... add other mappings as needed\n",
        "        }\n",
        "\n",
        "    date_obj = datetime.strptime(date, '%Y-%m-%d')\n",
        "    game_date = date_obj.strftime('%Y%m%d')\n",
        "\n",
        "    # Initialize a dictionary to keep track of the first encountered configuration for each team combination on a specific date\n",
        "    first_encountered_config = {}\n",
        "\n",
        "    # Initialize a dictionary to keep track of games played between each team combination on a specific date\n",
        "    games_played = defaultdict(int)\n",
        "\n",
        "    teams_without_mapping = {}\n",
        "\n",
        "    for game in results:\n",
        "        # Convert full team names to their initials\n",
        "        original_home_team = game['home_team']\n",
        "        original_away_team = game['away_team']\n",
        "\n",
        "        # Check if the home team has a mapping, if not add to the dictionary\n",
        "        if game['home_team'] not in team_nomenclature_mapping:\n",
        "            teams_without_mapping[game['home_team']] = original_home_team\n",
        "        \n",
        "        # Check if the away team has a mapping, if not add to the dictionary\n",
        "        if game['away_team'] not in team_nomenclature_mapping:\n",
        "            teams_without_mapping[game['away_team']] = original_away_team\n",
        "        \n",
        "        # Map these initials to the desired nomenclature\n",
        "        game['home_team'] = team_nomenclature_mapping.get(game['home_team'], game['home_team'])\n",
        "        game['away_team'] = team_nomenclature_mapping.get(game['away_team'], game['away_team'])\n",
        "\n",
        "        # Construct a unique key for each game based on the teams (independent of home/away)\n",
        "        teams_key = tuple(sorted([game['home_team'], game['away_team']]))\n",
        "\n",
        "        # Check if this is the first game encountered for this team combination today\n",
        "        if teams_key not in first_encountered_config:\n",
        "            # Store the home/away configuration for this team combination\n",
        "            first_encountered_config[teams_key] = (game['home_team'], game['away_team'])\n",
        "\n",
        "        # Use the first encountered configuration for this game\n",
        "        home_team, away_team = first_encountered_config[teams_key]\n",
        "\n",
        "        # Increment the game count for this specific matchup on this date\n",
        "        games_played[teams_key] += 1\n",
        "        \n",
        "        # Construct the game ID with the format yyyymmddhometeamawayteam + game count\n",
        "        game_id = f\"{game_date}{home_team}{away_team}{games_played[teams_key]}\"\n",
        "        \n",
        "        # Update the game dictionary with the consistent home/away configuration and game ID\n",
        "        game['Game_ID'] = game_id\n",
        "        # Remove the original home and away team keys to avoid confusion\n",
        "        del game['home_team']\n",
        "        del game['away_team']\n",
        "        # Determine the winner based on runs (assuming 'N/A' cases have been filtered out)\n",
        "        if int(game['HmRuns']) > int(game['VisRuns']):\n",
        "            game['winner'] = 1\n",
        "        else:\n",
        "            game['winner'] = 0\n",
        "\n",
        "    # Print the updated game results with game IDs and winners\n",
        "    for game in results:\n",
        "        print(game)\n",
        "\n",
        "    # Print original team names without mapping\n",
        "    print(\"Original team names without mapping:\", teams_without_mapping)\n",
        "    return results, teams_without_mapping\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def create_or_update_games_data(file_path, new_games_df):\n",
        "    \"\"\"Create or update a data file with multiple games' data from a DataFrame.\"\"\"\n",
        "    # Check if the file exists\n",
        "    if os.path.exists(file_path):\n",
        "        existing_df = pd.read_csv(file_path)\n",
        "        # If the file exists, merge the new data with the existing data on 'Game_ID'\n",
        "        # This will update existing records and add new ones\n",
        "        updated_df = pd.merge(existing_df, new_games_df, on='Game_ID', how='outer', suffixes=('', '_new'))\n",
        "        # Handle potential duplicate columns after the merge (e.g., if 'winner' columns exist in both DataFrames)\n",
        "        for col in updated_df.columns:\n",
        "            if col.endswith('_new'):\n",
        "                original_col = col.rstrip('_new')\n",
        "                # Update the original column with the values from the new column where they exist\n",
        "                updated_df[original_col].update(updated_df[col])\n",
        "                # Drop the '_new' columns after updating\n",
        "                updated_df.drop(columns=[col], inplace=True)\n",
        "    else:\n",
        "        updated_df = new_games_df\n",
        "    \n",
        "    # Save the updated DataFrame\n",
        "    updated_df.to_csv(file_path, index=True)\n",
        "\n",
        "# Example usage\n",
        "file_path = 'games_data.csv'\n",
        "# Assuming 'games_df' is your DataFrame containing the new games data with a 'Game_ID' column\n",
        "# Example DataFrame creation for demonstration purposes\n",
        "games_df = pd.DataFrame({\n",
        "    'Game_ID': ['20210908HOUSEA1', '20210908COLSFN1'],\n",
        "    'Starting_Lineups': ['lineups_game1', 'lineups_game2'],\n",
        "    'Predictions': ['predictions_game1', 'predictions_game2'],\n",
        "    'Bets': ['bets_game1', 'bets_game2'],\n",
        "    'winner': [0, 1]\n",
        "})\n",
        "\n",
        "def full_integration(df, betting_date, scraped_data=False):\n",
        "    df = remove_columns_with_nan(df, 40)\n",
        "    df = Impute(df,'most_frequent')\n",
        "    df = label_encode(df)\n",
        "    if scraped_data==True:\n",
        "        mlb_game_results, mlb_score_missing_ids = fetch_and_process_game_results(betting_date)\n",
        "        mlb_game_results_df = pd.DataFrame(mlb_game_results)\n",
        "        mlb_game_results_df.set_index('Game_ID', inplace=True)\n",
        "        df = df.merge(mlb_game_results_df[['winner']], left_index=True, right_index=True, how='left')\n",
        "        df = scale_data(df, method='minmax')\n",
        "\n",
        "    else:\n",
        "        df = df.sort_index()\n",
        "    return  df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrdSLG_vVQU9"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOJ-NP25VQU-"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "GsGA0YD0VQU-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batter data fetched successfully.\n",
            "pitcher data fetched successfully.\n"
          ]
        }
      ],
      "source": [
        "betting_date = '2023-09-08'\n",
        "season_start_year = int(betting_date[:4])\n",
        "all_years = [str(year) for year in range(season_start_year - 8, season_start_year + 1)]\n",
        "train_years = all_years[:-1]\n",
        "season_start_date = f\"{season_start_year}-03-01\"\n",
        "starting_lineup=Starting_Lineup(betting_date)\n",
        "zip_links=list_old_bwar_data()\n",
        "# starting_lineup = starting_lineup.set_index('Game_ID')\n",
        "most_recent_link=find_most_recent_zip(zip_links,betting_date)\n",
        "\n",
        "starting_lineup=starting_lineup.set_index('Game_ID')\n",
        "\n",
        "#find the most recent bwar data for players. This could be as old as the previous season, depending on the date.\n",
        "content= scrape_old_bwar_data(most_recent_link)\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "\n",
        "#convert the batter and pitcher data into a dataframe\n",
        "# batter_data = pd.read_csv(io.StringIO(content[0]))\n",
        "# pitcher_data = pd.read_csv(io.StringIO(content[1]))\n",
        "\n",
        "\n",
        "##### UNCOMMENT THIS WHEN YOU NEED TO ACTUALLY CALL THE REAL BATTER AND PITCHER DATA\n",
        "#batter imports full data\n",
        "batter_data = fetch_data_with_retry(bwar_bat, 'batter')\n",
        "#pitcher imports full data\n",
        "pitcher_data = fetch_data_with_retry(bwar_pitch, 'pitcher')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 home_open  visiting_open\n",
            "Game_ID                                  \n",
            "20100404BOSNYY1       -114           -106\n",
            "20100405WASPHI1        170           -200\n",
            "20100405NYMMIA1       -115           -105\n",
            "20100405CINSTL1        135           -155\n",
            "20100405PITLOS1        135           -155\n",
            "...                    ...            ...\n",
            "20211027HOUATL1       -115           -105\n",
            "20211029ATLHOU1       -115           -105\n",
            "20211030ATLHOU1       -115           -105\n",
            "20211031ATLHOU1       -105           -115\n",
            "20211102HOUATL1       -120            100\n",
            "\n",
            "[28006 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# train_years = [str(year) for year in range(int(test_year) - 8, int(test_year)-1)]\n",
        "\n",
        "\n",
        "milb_batter = pd.read_csv(milb_batter_path)\n",
        "milb_pitcher = pd.read_csv(milb_pitcher_path)\n",
        "\n",
        "#betting data site\n",
        "scrambled_odds_full=pd.read_csv(scrambled_betting_paths)\n",
        "#People Import\n",
        "chad_v3 = pyb.chadwick_register()\n",
        "\n",
        "#Vegas Odds Import and process in one\n",
        "vegas_odds=process_vegas_odds(vegas_paths)\n",
        "\n",
        "\n",
        "#pitcher imports full data\n",
        "# pitcher_data = pd.read_csv(mlb_pitcher_path)\n",
        "\n",
        "# Example usage:\n",
        "# zip_links = list_old_bwar_data()  # This would be the list obtained from the previous function\n",
        "# input_date = '2023-04-01'  # Example input date\n",
        "# print(find_most_recent_zip(zip_links, input_date))\n",
        "\n",
        "\n",
        "\n",
        "per_game_data_full = pd.read_csv(per_game_path, header=0)\n",
        "\n",
        "#Drops all columns except for the columns below\n",
        "columns_to_keep = ['HmStPchID', 'VisStPchID', 'HmBat1ID', 'HmBat2ID', 'HmBat3ID', 'HmBat4ID', 'HmBat5ID', 'HmBat6ID', 'HmBat7ID', 'HmBat8ID', 'HmBat9ID', 'VisBat1ID', 'VisBat2ID', 'VisBat3ID', 'VisBat4ID', 'VisBat5ID', 'VisBat6ID', 'VisBat7ID', 'VisBat8ID', 'VisBat9ID', 'HmRuns', 'VisRuns','HmTm','VisTm']\n",
        "per_game_data = process_per_game_data(per_game_data_full, columns_to_keep,all_years, betting_date)\n",
        "# per_game_players = add_players_to_games()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "9b3Dq6JTVQU_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropping columns: Index(['date', 'GameId'], dtype='object')\n",
            "File pybaseball_v3_saved\\per_game_data_full.csv already exists.\n",
            "File pybaseball_v3_saved\\per_game_data.csv already exists.\n",
            "File pybaseball_v3_saved\\batter_bwar_data.csv already exists.\n",
            "File pybaseball_v3_saved\\pitcher_bwar_data.csv already exists.\n",
            "File pybaseball_v3_saved\\chad_v3.csv already exists.\n",
            "File pybaseball_v3_saved\\vegas_odds.csv already exists.\n",
            "File pybaseball_v3_saved\\scrambled_odds.csv already exists.\n",
            "File pybaseball_v3_saved\\scrambled_odds_full.csv already exists.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#process scrambled odds\n",
        "scrambled_odds=process_scrambled_odds(scrambled_odds_full)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#save it to a folder called pybaseball_v3_saved\n",
        "save_to_path(per_game_data_full, 'per_game_data_full')\n",
        "save_to_path(per_game_data, 'per_game_data')\n",
        "save_to_path(batter_data, 'batter_bwar_data')\n",
        "save_to_path(pitcher_data, 'pitcher_bwar_data')\n",
        "save_to_path(chad_v3,'chad_v3')\n",
        "save_to_path(vegas_odds,'vegas_odds')\n",
        "save_to_path(scrambled_odds,'scrambled_odds')\n",
        "save_to_path(scrambled_odds_full,'scrambled_odds_full')\n",
        "\n",
        "playerid='player_ID'\n",
        "# pitcher_data.reset_index(drop=False, inplace=True)\n",
        "# batter_data.reset_index(drop=False, inplace=True)\n",
        "\n",
        "batter_data.set_index(playerid, inplace=True)\n",
        "pitcher_data.set_index(playerid, inplace=True)\n",
        "pitcher_names = pyb.playerid_reverse_lookup(pitcher_data.index, key_type='bbref')\n",
        "batter_names = pyb.playerid_reverse_lookup(batter_data.index, key_type='bbref')\n",
        "#For some reason the chad data is missing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# batter_data_remove = batter_data[batter_data.index != 'stantha01']\n",
        "\n",
        "# Create a mapping from player_ID to key_retro for pitchers and batters\n",
        "# if 'pitcher_id_to_retro' not in locals():\n",
        "pitcher_id_to_retro = pitcher_names.set_index('key_bbref')['key_retro'].to_dict()\n",
        "# if 'batter_id_to_retro' not in locals():\n",
        "batter_id_to_retro = batter_names.set_index('key_bbref')['key_retro'].to_dict()\n",
        "\n",
        "\n",
        "# Create a copy of the original DataFrames to preserve the original 'playerid' columns\n",
        "pitcher_data_original = pitcher_data.copy()\n",
        "batter_data_remove_original = batter_data.copy()\n",
        "\n",
        "# Before performing the mapping, ensure that the index is set correctly on the original DataFrames\n",
        "# pitcher_data_original.set_index(playerid, inplace=True)\n",
        "# batter_data_remove_original.set_index(playerid, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                name_common   age    mlb_ID  year_ID team_ID  stint_ID lg_ID  \\\n",
            "player_ID                                                                      \n",
            "bechtge01    George Bechtel  22.0  110756.0     1871     ATH         1   NaN   \n",
            "brainas01      Asa Brainard  30.0  111373.0     1871     OLY         1   NaN   \n",
            "fergubo01      Bob Ferguson  26.0  114069.0     1871     NYU         1   NaN   \n",
            "fishech01   Cherokee Fisher  26.0  114181.0     1871     ROK         1   NaN   \n",
            "fleetfr01       Frank Fleet  23.0  114224.0     1871     NYU         1   NaN   \n",
            "...                     ...   ...       ...      ...     ...       ...   ...   \n",
            "youngal01        Alex Young  29.0  622065.0     2023     CIN         1    NL   \n",
            "youngda02       Danny Young  29.0  664849.0     2023     ATL         1    NL   \n",
            "zastrro01     Rob Zastryzny  31.0  642239.0     2023     PIT         1    NL   \n",
            "zavalse01       Seby Zavala  29.0  664874.0     2023     ARI         2    NL   \n",
            "zuniggu01  Guillermo Zuñiga  24.0  670871.0     2023     STL         1    NL   \n",
            "\n",
            "            G  GS  IPouts  ...  pyth_exponent  waa_win_perc     WAA  WAA_adj  \\\n",
            "player_ID                  ...                                                 \n",
            "bechtge01   3   3      78  ...          2.499        0.3020 -0.5940  -0.0153   \n",
            "brainas01  30  30     792  ...          2.375        0.4951 -0.1470  -0.1556   \n",
            "fergubo01   1   0       3  ...          2.606        0.1823 -0.3177  -0.0006   \n",
            "fishech01  24  24     639  ...          2.367        0.5098  0.2352  -0.1255   \n",
            "fleetfr01   1   1      27  ...          2.698        0.1137 -0.3863  -0.0053   \n",
            "...        ..  ..     ...  ...            ...           ...     ...      ...   \n",
            "youngal01  63   0     161  ...          1.892        0.5054  0.3402  -0.1128   \n",
            "youngda02   8   0      25  ...          1.871        0.5426  0.3408  -0.1144   \n",
            "zastrro01  21   1      62  ...          1.908        0.4762 -0.4998   0.0269   \n",
            "zavalse01   1   0       2  ...          1.878        0.5302  0.0302  -0.0170   \n",
            "zuniggu01   2   0       6  ...          1.897        0.4952 -0.0096  -0.0011   \n",
            "\n",
            "           oppRpG_rep  pyth_exponent_rep  waa_win_perc_rep  WAR_rep  \\\n",
            "player_ID                                                             \n",
            "bechtge01    12.22880              2.432            0.4008   0.2455   \n",
            "brainas01    12.25747              2.432            0.3994   2.4889   \n",
            "fergubo01    10.58039              2.380            0.4878   0.0102   \n",
            "fishech01    12.27359              2.433            0.3986   2.0067   \n",
            "fleetfr01    12.30047              2.434            0.3973   0.0847   \n",
            "...               ...                ...               ...      ...   \n",
            "youngal01     4.79110              1.899            0.4916   0.5202   \n",
            "youngda02     4.80991              1.900            0.4897   0.0810   \n",
            "zastrro01     4.80421              1.900            0.4903   0.2002   \n",
            "zavalse01     4.77275              1.898            0.4934   0.0065   \n",
            "zuniggu01     4.80578              1.900            0.4901   0.0195   \n",
            "\n",
            "             ERA_plus    ER_lg  \n",
            "player_ID                       \n",
            "bechtge01   52.682609   12.117  \n",
            "brainas01   93.906818  123.957  \n",
            "fergubo01   19.733333    0.592  \n",
            "fishech01   97.129126  100.043  \n",
            "fleetfr01   39.680000    3.968  \n",
            "...               ...      ...  \n",
            "youngal01  118.926087   27.353  \n",
            "youngda02  426.900000    4.269  \n",
            "zastrro01   93.963636   10.336  \n",
            "zavalse01         NaN    0.491  \n",
            "zuniggu01  113.400000    1.134  \n",
            "\n",
            "[54852 rows x 42 columns]\n"
          ]
        }
      ],
      "source": [
        "print(pitcher_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Perform the mapping on the copies\n",
        "pitcher_data.index = pitcher_data_original.index.map(pitcher_id_to_retro)\n",
        "batter_data.index = batter_data_remove_original.index.map(batter_id_to_retro)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if 'level_0' not in batter_data.columns:\n",
        "    batter_data.reset_index(inplace=True)\n",
        "if 'level_0' not in pitcher_data.columns:\n",
        "    pitcher_data.reset_index(inplace=True)\n",
        "    batter_data_removed=remove_columns_with_nan(batter_data,40)\n",
        "    pitcher_data_removed=remove_columns_with_nan(pitcher_data,40)\n",
        "# Now you can call the aggregate_player_data function with 'playerid' as a column\n",
        "# THIS ALSO LABEL ENCODES THE DATA\n",
        "batter_data_removed.set_index(playerid, inplace=True)\n",
        "pitcher_data_removed.set_index(playerid, inplace=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "per_game_data_odds = combine_odds(per_game_data, vegas_odds, scrambled_odds)\n",
        "batter_aggregated = aggregate_player_data(batter_data, playerid)\n",
        "pitcher_aggregated = aggregate_player_data(pitcher_data, playerid)\n",
        "\n",
        "if 0 in batter_aggregated.index:\n",
        "    batter_aggregated.drop(index=0, inplace=True)\n",
        "if 0 in pitcher_aggregated.index:\n",
        "    pitcher_aggregated.drop(index=0, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The column home_team has 68.4% NaN values.\n",
            "The column visiting_team has 68.4% NaN values.\n",
            "The column Game_Number has 68.4% NaN values.\n",
            "{'HmRuns': '0', 'VisRuns': '1', 'Game_ID': '20230908CHNARI1', 'winner': 0}\n",
            "{'HmRuns': '4', 'VisRuns': '9', 'Game_ID': '20230908CINSLN1', 'winner': 0}\n",
            "{'HmRuns': '7', 'VisRuns': '4', 'Game_ID': '20230908TBASEA1', 'winner': 1}\n",
            "{'HmRuns': '0', 'VisRuns': '6', 'Game_ID': '20230908DETCHA1', 'winner': 0}\n",
            "{'HmRuns': '5', 'VisRuns': '8', 'Game_ID': '20230908WASLAN1', 'winner': 0}\n",
            "{'HmRuns': '2', 'VisRuns': '3', 'Game_ID': '20230908PHIMIA1', 'winner': 0}\n",
            "{'HmRuns': '2', 'VisRuns': '8', 'Game_ID': '20230908NYAMIL1', 'winner': 0}\n",
            "{'HmRuns': '5', 'VisRuns': '4', 'Game_ID': '20230908TORKCA1', 'winner': 1}\n",
            "{'HmRuns': '2', 'VisRuns': '11', 'Game_ID': '20230908BOSBAL1', 'winner': 0}\n",
            "{'HmRuns': '8', 'VisRuns': '2', 'Game_ID': '20230908ATLPIT1', 'winner': 1}\n",
            "{'HmRuns': '3', 'VisRuns': '6', 'Game_ID': '20230908TEXOAK1', 'winner': 0}\n",
            "{'HmRuns': '2', 'VisRuns': '11', 'Game_ID': '20230908HOUSDN1', 'winner': 0}\n",
            "{'HmRuns': '5', 'VisRuns': '2', 'Game_ID': '20230908MINNYN1', 'winner': 1}\n",
            "{'HmRuns': '3', 'VisRuns': '6', 'Game_ID': '20230908ANACLE1', 'winner': 0}\n",
            "{'HmRuns': '9', 'VisRuns': '8', 'Game_ID': '20230908SFNCOL1', 'winner': 1}\n",
            "Original team names without mapping: {}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "    # Check if the index has a 0 value and if so, remove the corresponding row\n",
        "\n",
        "\n",
        "\n",
        "pitcher_columns = ['HmStPchID', 'VisStPchID']\n",
        "batter_columns = ['HmBat1ID', 'HmBat2ID', 'HmBat3ID', 'HmBat4ID', 'HmBat5ID', 'HmBat6ID', 'HmBat7ID', 'HmBat8ID', 'HmBat9ID', 'VisBat1ID', 'VisBat2ID', 'VisBat3ID', 'VisBat4ID', 'VisBat5ID', 'VisBat6ID', 'VisBat7ID', 'VisBat8ID', 'VisBat9ID']\n",
        "\n",
        "    # Create separate dictionaries for the data\n",
        "player_data_dictionary = {\n",
        "    'pitcher': pitcher_aggregated.to_dict('index'),\n",
        "    'batter': batter_aggregated.to_dict('index')\n",
        "}\n",
        "\n",
        "player_id_columns = pitcher_columns + batter_columns\n",
        "per_game_finished, missing_player_ids = replace_ids_with_stats_and_catalog_missing(per_game_data_odds, player_id_columns, player_data_dictionary)\n",
        "# per_game_finished, missing_player_ids = prepare_data(batter_data_removed,pitcher_data_removed, per_game_data_odds,playerid,all_years)\n",
        "\n",
        "starting_lineup_finished, missing_player_ids = replace_ids_with_stats_and_catalog_missing(starting_lineup, player_id_columns, player_data_dictionary)\n",
        "\n",
        "# Now you can check which games have players with all stats missing\n",
        "aaaaaaa = per_game_finished\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "view_data(per_game_finished)\n",
        "per_game_finished= full_integration(per_game_finished, betting_date)\n",
        "X_real = full_integration(starting_lineup_finished, betting_date, scraped_data=True)\n",
        "\n",
        "\n",
        "# mlb_game_results_df=mlb_game_results_df.set_index('Game_ID')\n",
        "# Assuming mlb_game_results is a DataFrame with 'Game_ID' and 'winner' columns\n",
        "\n",
        "# Merge starting_lineup_finished with mlb_game_results to map the 'winner' based on 'Game_ID'\n",
        "# Ensure mlb_game_results_df has 'Game_ID' as a column\n",
        "\n",
        "\n",
        "# Perform the merge using the index of starting_lineup_finished and the index of mlb_game_results_df\n",
        "\n",
        "\n",
        "# If starting_lineup_finished should not have 'Game_ID' as its index after the merge, reset the index\n",
        "\n",
        "\n",
        "# This will add the 'winner' column to starting_lineup_finished based on matching 'Game_ID'\n",
        "# starting_lineup_finished=Impute(starting_lineup_finished,'most')\n",
        "\n",
        "# X,y = get_data(per_game_finished, labels='winner', thresh=0.2, columns_to_drop=['HmRuns','VisRuns','home_odds','visiting_odds'])\n",
        "#Split the data\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test = split_dataframe_data(\n",
        "#     per_game_finished,\n",
        "#     train_years,\n",
        "#     validation_year,\n",
        "#     test_year\n",
        "# )\n",
        "# Assuming per_game_finished is your DataFrame\n",
        "# unlabeled_data = per_game_finished.drop(['home_odds', 'visiting_odds', 'HmRuns', 'VisRuns'], axis=1)\n",
        "# labeled_data = per_game_finished['winner']\n",
        "# per_game_finished.drop(['winner', 'home_odds', 'visiting_odds', 'HmRuns', 'VisRuns'], axis=1)\n",
        "\n",
        "# Assuming 'per_game_finished' is your DataFrame and 'winner' is the column with labels\n",
        "unlabeled_data = per_game_finished.drop(['home_odds', 'visiting_odds', 'HmRuns', 'VisRuns', 'winner'], axis=1)\n",
        "labeled_data = per_game_finished['winner']\n",
        "\n",
        "\n",
        "# Sort the DataFrame by its index to ensure chronological order\n",
        "\n",
        "\n",
        "\n",
        "# Split off the test set from the most recent data\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    per_game_finished.drop(['home_odds', 'visiting_odds', 'HmRuns', 'VisRuns', 'winner'], axis=1),\n",
        "    per_game_finished['winner'],\n",
        "    test_size=0.03,  # Assuming the test set is 20% of the data\n",
        "    shuffle=False  # Important to keep the temporal order\n",
        ")\n",
        "\n",
        "# Split the remaining data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp,\n",
        "    y_temp,\n",
        "    test_size=0.1,  # This makes the validation set 25% of the remaining data, achieving approximately a 60/20/20 split\n",
        "    shuffle=False  # Preserve the chronological order\n",
        ")\n",
        "# Now you have X_train_temp, y_train_temp for training, X_val, y_val for validation, and X_test, y_test for testing.\n",
        "\n",
        "_, X_small, _, y_small = train_test_split(X_train, y_train, test_size=0.05, random_state=42)\n",
        "# remove_columns = ['mlb_ID']\n",
        "# pitcher_removed=remove_excess_player_columns(pitcher_data,remove_columns)\n",
        "# batter_removed=remove_excess_player_columns(batter_data,remove_columns)\n",
        "rankings = classification_ranking(X_small, y_small, predict=True, choices=['mi', 'f_test'])  # For classification tasks\n",
        "# # or\n",
        "# # #   ranking = regression_ranking(X_small, y_small, rf_hypers, xb_hypers)  # For regression tasks\n",
        "\n",
        "\n",
        "scoring = voting(rankings)\n",
        "# plot_after_vote(scoring, title='Feature Ranking')\n",
        "scores = [score[1] for score in scoring]  # Extract the second element from each tuple\n",
        "min_score = min(scores)\n",
        "max_score = max(scores)\n",
        "scoring = [(item[0], (item[1] - min_score) / (max_score - min_score)) for item in scoring]\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 64/64 [00:00<00:00, 83.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6924959300085902\n",
            "Validation Loss: 0.6840482726693153\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 64/64 [00:00<00:00, 109.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6819372177124023\n",
            "Validation Loss: 0.6790062412619591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 64/64 [00:00<00:00, 105.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6766733257099986\n",
            "Validation Loss: 0.6728473082184792\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 64/64 [00:00<00:00, 106.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6742575885728002\n",
            "Validation Loss: 0.672591008245945\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 64/64 [00:00<00:00, 105.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6758295614272356\n",
            "Validation Loss: 0.676586240530014\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 64/64 [00:00<00:00, 104.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6719954675063491\n",
            "Validation Loss: 0.6734485030174255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 64/64 [00:00<00:00, 108.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6738195298239589\n",
            "Validation Loss: 0.6874051913619041\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 64/64 [00:00<00:00, 107.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6736055994406343\n",
            "Validation Loss: 0.6757742911577225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 64/64 [00:00<00:00, 102.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6711013810709119\n",
            "Validation Loss: 0.732095755636692\n",
            "Early stopping triggered\n",
            "Confusion Matrix:\n",
            "[[135 155]\n",
            " [ 96 215]]\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.5844155844155844, 'recall': 0.46551724137931033, 'f1-score': 0.5182341650671785, 'support': 290.0}, '1': {'precision': 0.581081081081081, 'recall': 0.6913183279742765, 'f1-score': 0.631424375917768, 'support': 311.0}, 'accuracy': 0.5823627287853578, 'macro avg': {'precision': 0.5827483327483327, 'recall': 0.5784177846767934, 'f1-score': 0.5748292704924732, 'support': 601.0}, 'weighted avg': {'precision': 0.582690076034502, 'recall': 0.5823627287853578, 'f1-score': 0.5768068032943554, 'support': 601.0}}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 64/64 [00:00<00:00, 105.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6913587292656302\n",
            "Validation Loss: 0.6735208928585052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 64/64 [00:00<00:00, 103.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6791224358603358\n",
            "Validation Loss: 0.6753575652837753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 64/64 [00:00<00:00, 101.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6744788372889161\n",
            "Validation Loss: 0.6744090765714645\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 64/64 [00:00<00:00, 107.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.675543743185699\n",
            "Validation Loss: 0.6743911877274513\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 64/64 [00:00<00:00, 108.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6728009637445211\n",
            "Validation Loss: 0.6749254167079926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 64/64 [00:00<00:00, 99.58it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6730928523465991\n",
            "Validation Loss: 0.6756536811590195\n",
            "Early stopping triggered\n",
            "Confusion Matrix:\n",
            "[[133 157]\n",
            " [ 93 218]]\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.588495575221239, 'recall': 0.4586206896551724, 'f1-score': 0.5155038759689923, 'support': 290.0}, '1': {'precision': 0.5813333333333334, 'recall': 0.7009646302250804, 'f1-score': 0.6355685131195336, 'support': 311.0}, 'accuracy': 0.5840266222961731, 'macro avg': {'precision': 0.5849144542772862, 'recall': 0.5797926599401264, 'f1-score': 0.575536194544263, 'support': 601.0}, 'weighted avg': {'precision': 0.5847893235953844, 'recall': 0.5840266222961731, 'f1-score': 0.5776338296359113, 'support': 601.0}}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 64/64 [00:00<00:00, 102.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6909131519496441\n",
            "Validation Loss: 0.6839845851063728\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 64/64 [00:00<00:00, 110.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6800339762121439\n",
            "Validation Loss: 0.6860010325908661\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 64/64 [00:00<00:00, 109.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6788461348041892\n",
            "Validation Loss: 0.6799736768007278\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 64/64 [00:00<00:00, 96.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6773605719208717\n",
            "Validation Loss: 0.6737841665744781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 64/64 [00:00<00:00, 102.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6737995343282819\n",
            "Validation Loss: 0.6935609504580498\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 64/64 [00:00<00:00, 110.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6732529876753688\n",
            "Validation Loss: 0.6742381528019905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 64/64 [00:00<00:00, 110.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6718062162399292\n",
            "Validation Loss: 0.6732497960329056\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 64/64 [00:00<00:00, 82.39it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6703558769077063\n",
            "Validation Loss: 0.6766574904322624\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 64/64 [00:00<00:00, 93.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6756709134206176\n",
            "Validation Loss: 0.6745670735836029\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 64/64 [00:00<00:00, 97.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6733155073598027\n",
            "Validation Loss: 0.691588506102562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|██████████| 64/64 [00:00<00:00, 97.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6685698255896568\n",
            "Validation Loss: 0.680963359773159\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|██████████| 64/64 [00:00<00:00, 99.21it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.669200531207025\n",
            "Validation Loss: 0.6836863085627556\n",
            "Early stopping triggered\n",
            "Confusion Matrix:\n",
            "[[121 169]\n",
            " [ 88 223]]\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.5789473684210527, 'recall': 0.41724137931034483, 'f1-score': 0.4849699398797595, 'support': 290.0}, '1': {'precision': 0.5688775510204082, 'recall': 0.7170418006430869, 'f1-score': 0.6344238975817923, 'support': 311.0}, 'accuracy': 0.5723793677204659, 'macro avg': {'precision': 0.5739124597207304, 'recall': 0.5671415899767158, 'f1-score': 0.5596969187307759, 'support': 601.0}, 'weighted avg': {'precision': 0.5737365311305361, 'recall': 0.5723793677204659, 'f1-score': 0.5623080111698298, 'support': 601.0}}\n",
            "Average Confusion Matrix:\n",
            "[[129.66666667 160.33333333]\n",
            " [ 92.33333333 218.66666667]]\n",
            "Average Classification Report:\n",
            "              precision    recall  f1-score    support\n",
            "0              0.583953  0.447126  0.506236  290.00000\n",
            "1              0.577097  0.703108  0.633806  311.00000\n",
            "accuracy       0.579590  0.579590  0.579590    0.57959\n",
            "macro avg      0.580525  0.575117  0.570021  601.00000\n",
            "weighted avg   0.580405  0.579590  0.572250  601.00000\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'winner'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'winner'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[71], line 136\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Create a new model with the updated number of features\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (game_id, bet_on_home_team), amount_bet, game_prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ok2, bets2, prob2):\n\u001b[1;32m--> 136\u001b[0m     \u001b[43mbetter2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbet_on_home_team\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamount_bet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarting_lineup_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[61], line 578\u001b[0m, in \u001b[0;36mBetter.bet\u001b[1;34m(self, game_indices, bet_on_home_team, amount_bet, game_data, prob, augment, scaler)\u001b[0m\n\u001b[0;32m    575\u001b[0m odds \u001b[38;5;241m=\u001b[39m home_odds \u001b[38;5;28;01mif\u001b[39;00m bet_on_home_team \u001b[38;5;28;01melse\u001b[39;00m away_odds\n\u001b[0;32m    577\u001b[0m \u001b[38;5;66;03m# Determine if the bet was a win or loss\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m bet_won \u001b[38;5;241m=\u001b[39m (\u001b[43mgame\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwinner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m bet_on_home_team)\n\u001b[0;32m    580\u001b[0m \u001b[38;5;66;03m# Calculate the amount gained or lost\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bet_won:\n",
            "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\core\\series.py:1111\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
            "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\core\\series.py:1227\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1227\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
            "File \u001b[1;32mc:\\python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3807\u001b[0m     ):\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'winner'"
          ]
        }
      ],
      "source": [
        "\n",
        "with open('bwar_new_scoring', 'wb') as f:\n",
        "    pickle.dump(scoring, f)\n",
        "\n",
        "\n",
        "\n",
        "X_train = scale_data(X_train, method='minmax')\n",
        "X_val = scale_data(X_val, method='minmax')\n",
        "X_test= scale_data(X_test, method='minmax')\n",
        "\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('bwar_new_scoring', 'rb') as f:\n",
        "    scoring = pickle.load(f)\n",
        "\n",
        "threshold=0\n",
        "# Get the list of columns to keep\n",
        "# Convert `scoring` into a DataFrame\n",
        "scoring_df = pd.DataFrame(scoring, columns=['column_name', 'score'])\n",
        "\n",
        "# Filter the columns to keep\n",
        "columns_to_keep = scoring_df[scoring_df['score'] > threshold]['column_name'].tolist()\n",
        "\n",
        "# Ensure that only columns that exist in the DataFrame are kept\n",
        "columns_to_keep = [col for col in columns_to_keep if col in X_train.columns]\n",
        "# X_val_new=X_val\n",
        "# X_test_new=X_test\n",
        "X_train_reduced = X_train[columns_to_keep]\n",
        "X_val_reduced = X_val[columns_to_keep]\n",
        "X_test_reduced = X_test[columns_to_keep]\n",
        "X_real = X_real[columns_to_keep]\n",
        "\n",
        "X_train_reduced['game_index']=X_train_reduced.index\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train_res, y_train_res = rus.fit_resample(X_train_reduced, y_train)\n",
        "# X_train_res=X_train_new\n",
        "# y_train_res=y_train_new\n",
        "# Assuming `vector_dataset` is a class that converts your data into a format suitable for PyTorch\n",
        "# y_train_new.set_index('index',inplace=True)\n",
        "X_train_res.set_index('game_index',inplace=True)\n",
        "\n",
        "\n",
        "# Create a new model with the updated number of features\n",
        "\n",
        "# Create new DataLoaders with the updated datasets\n",
        "train_dataset_new = vector_dataset(X_train_res, y_train_res)\n",
        "val_dataset_new = vector_dataset(X_val_reduced, y_val)\n",
        "test_dataset_new = vector_dataset(X_test_reduced, y_test)\n",
        "\n",
        "# train_loader_new = DataLoader(train_dataset_new, batch_size=64, shuffle=True)\n",
        "# val_loader_new = DataLoader(val_dataset_new, batch_size=64, shuffle=False)\n",
        "# test_loader_new = DataLoader(test_dataset_new, batch_size=64, shuffle=False)\n",
        "train_loader_new = DataLoader(train_dataset_new, batch_size=256, shuffle=True)\n",
        "val_loader_new = DataLoader(val_dataset_new, batch_size=256, shuffle=False)\n",
        "test_loader_new = DataLoader(test_dataset_new, batch_size=256, shuffle=False)\n",
        "\n",
        "num_runs = 3\n",
        "conf_matrices = []\n",
        "class_reports = []\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Now you can train this model with the dropped datasets\n",
        "for _ in range(num_runs):\n",
        "    # Reinitialize the model for each run\n",
        "    if 'model' in locals():\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "    model = neural_net(X_train_res.shape[1], X_train_res.shape[1]*2, 2, 2, 0.15).to(device)\n",
        "    # model = neural_net(X_train_res.shape[1], X_train_res.shape[1]*2, 2, 2, 0.15)\n",
        "    # scores = cross_validate(model, X_train_res, y_train_res, n_splits=5)\n",
        "\n",
        "    # Reinitialize the model and optimizer for each run\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1)\n",
        "\n",
        "\n",
        "    # Train and evaluate the model\n",
        "\n",
        "    conf_matrix, class_report = train_and_evaluate_model(\n",
        "    model,\n",
        "    train_loader_new,\n",
        "    val_loader_new,\n",
        "    test_loader_new,\n",
        "    optimizer,\n",
        "    criterion,  # Make sure the criterion is passed here\n",
        "    scheduler,  # Then the scheduler\n",
        "    patience=5,\n",
        "    num_epochs=50\n",
        ")\n",
        "    # Store the results\n",
        "    conf_matrices.append(conf_matrix)\n",
        "    class_reports.append(pd.DataFrame(class_report).transpose())\n",
        "\n",
        "    # Step the scheduler\n",
        "\n",
        "\n",
        "# Calculate and print the average confusion matrix and classification report\n",
        "avg_conf_matrix = np.mean(conf_matrices, axis=0)\n",
        "avg_class_report = pd.concat(class_reports).groupby(level=0).mean()\n",
        "\n",
        "print('Average Confusion Matrix:')\n",
        "print(avg_conf_matrix)\n",
        "print('Average Classification Report:')\n",
        "print(avg_class_report)\n",
        "\n",
        "\n",
        "prob = None\n",
        "ok, bets, skipped, prob = predict_game_outcome(X_test_reduced.index, X_test_reduced.values, model, False, 10, 1)\n",
        "ok2, bets2, skipped2, prob2 =predict_game_outcome(X_real.index, X_real.values, model, False, 10, 1)\n",
        "better = Better()  # Create a Bettor instance with an initial wallet of 1000\n",
        "better2 = Better()\n",
        "# for game_id, bet_on_home_team in ok:\n",
        "#     better.bet(game_id, bet_on_home_team == 'True', bets, per_game_data_imputed)\n",
        "\n",
        "# for (game_id, bet_on_home_team), amount_bet in zip(ok, bets):\n",
        "#     better.bet(game_id, bet_on_home_team == 'True', amount_bet, per_game_finished, prob, True, 3)\n",
        "\n",
        "\n",
        "starting_lineup_finished['home_odds'] = -110\n",
        "starting_lineup_finished['visiting_odds'] = -110\n",
        "\n",
        "\n",
        "\n",
        "for (game_id, bet_on_home_team), amount_bet, game_prob in zip(ok, bets, prob):\n",
        "    better.bet(game_id, bet_on_home_team == 'True', amount_bet, per_game_finished, game_prob, True, 3)\n",
        "# Create a new model with the updated number of features\n",
        "for (game_id, bet_on_home_team), amount_bet, game_prob in zip(ok2, bets2, prob2):\n",
        "    better2.bet(game_id, bet_on_home_team == 'True', amount_bet, starting_lineup_finished, game_prob, True, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print((better.bet_history))\n",
        "print((better.wallet_history))\n",
        "print(ok[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# file_path='./pybaseball_v3_saved/Saved_Betting_History.csv'\n",
        "# create_or_update_games_data(file_path, starting_lineup_finished)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91MYj8Bt_mQF"
      },
      "outputs": [],
      "source": [
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gHiocdYVQVA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def plot_wallet_balance(better, year):\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('Number of Games Bet')\n",
        "    ax1.set_ylabel('Wallet Balance', color=color)\n",
        "    ax1.plot(range(len(better.wallet_history)), better.wallet_history, color=color)\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Bet Amount', color=color)  # we already handled the x-label with ax1\n",
        "    ax2.plot(range(len(better.bet_history)), better.bet_history, color=color)\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    title = f'Wallet Balance and Bet Amount Over Number of Games Bet in {year}'\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Save the plot as an image\n",
        "    if not os.path.exists('results'):\n",
        "        os.makedirs('results')\n",
        "    plt.savefig(f'results/{title}.png')\n",
        "\n",
        "    plt.show()\n",
        "    save_results=save_results_format(year)\n",
        "    # Save the better data, avg_class_report, and avg_conf_matrix to an Excel file\n",
        "    with pd.ExcelWriter(save_results) as writer:\n",
        "        pd.DataFrame(better.wallet_history).to_excel(writer, sheet_name='wallet_history')\n",
        "        pd.DataFrame(better.bet_history).to_excel(writer, sheet_name='bet_history')\n",
        "        avg_class_report.to_excel(writer, sheet_name='avg_class_report')\n",
        "        pd.DataFrame(avg_conf_matrix).to_excel(writer, sheet_name='avg_conf_matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juYdtTv4VQVB"
      },
      "outputs": [],
      "source": [
        "test_year=season_start_year\n",
        "plot_wallet_balance(better, test_year)\n",
        "print(max(better.bet_history))\n",
        "print(min(better.bet_history))\n",
        "print(min(better.wallet_history))\n",
        "print(better.wallet_history[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(better2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_wallet_balance(better2, test_year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dfa9Rc7VQVB"
      },
      "outputs": [],
      "source": [
        "view_data(per_game_finished)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFL_zO0sVQVB"
      },
      "outputs": [],
      "source": [
        "# view_data(pitcher_names)\n",
        "view_data(batter_data)\n",
        "\n",
        "\n",
        "\n",
        "    #return missing_player_ids_train, missing_player_ids_val, missing_player_ids_test, avg_class_report, avg_conf_matrix, better.bet_history, better.wallet_history\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
