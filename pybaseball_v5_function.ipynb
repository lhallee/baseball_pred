{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(test_year):\n",
    "    import os\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    import pybaseball as pyb\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import inspect\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    import json\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from torch import optim\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import os\n",
    "    # from featureranker.utils import *\n",
    "    # from featureranker.plots import *\n",
    "    # from featureranker.rankers import *\n",
    "    from featureranker.utils import view_data\n",
    "    # from featureranker.plots import *\n",
    "    # from featureranker.rankers import *\n",
    "    import glob\n",
    "    import numpy as np\n",
    "    from tqdm.auto import tqdm\n",
    "    import pickle\n",
    "    from sklearn import svm\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    import warnings\n",
    "    from sklearn.model_selection import KFold\n",
    "    warnings.filterwarnings('ignore')\n",
    "    from torch.utils.data import TensorDataset\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pycuda.autoinit\n",
    "    import pycuda.gpuarray as gpuarray\n",
    "    pyb.cache.enable()\n",
    "    pyb.cache.config.cache_type='csv'\n",
    "    pyb.cache.config.save()\n",
    "    # Check if CUDA is available\n",
    "\n",
    "    # Move your model to the GPU\n",
    "    # model = model.to(device)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # When you load your data, move it to the GPU\n",
    "    # Assume inputs and labels are your input data and labels\n",
    "    # inputs, labels = inputs.to(device), labels.to(device)\n",
    "    class vector_dataset(Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            self.X = np.array(X)\n",
    "            self.y = np.array(y)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.y)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            vec = torch.tensor(self.X[idx], dtype=torch.float)\n",
    "            label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "            return vec, label\n",
    "\n",
    "\n",
    "    class neural_net(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, hidden_num, output_size, dropout_rate):\n",
    "            super(neural_net, self).__init__()\n",
    "            self.gelu = nn.GELU()\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            self.hidden_num = hidden_num\n",
    "            self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "            self.hidden_layers = nn.ModuleList()\n",
    "            for i in range(hidden_num):\n",
    "                self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.gelu(self.input_layer(x))\n",
    "            x = self.dropout(x)\n",
    "            for i in range(self.hidden_num):\n",
    "                x = self.gelu(self.hidden_layers[i](x))\n",
    "                x = self.dropout(x)\n",
    "            x = self.output_layer(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    #CHECK TO SEE HOW GOOD MODEL IS\n",
    "    def cross_validate(model, X, y, n_splits=5):\n",
    "        kfold = KFold(n_splits=n_splits)\n",
    "        scores = []\n",
    "\n",
    "        for train_indices, valid_indices in kfold.split(X):\n",
    "            # Split the data\n",
    "            X_train, X_valid = X[train_indices], X[valid_indices]\n",
    "            y_train, y_valid = y[train_indices], y[valid_indices]\n",
    "\n",
    "            # Convert to PyTorch datasets\n",
    "            train_dataset = vector_dataset(X_train, y_train)\n",
    "            valid_dataset = vector_dataset(X_valid, y_valid)\n",
    "\n",
    "            # Create data loaders\n",
    "            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "            valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "            # Define loss function and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "            # Train the model on the training data\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Evaluate the model on the validation data\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in valid_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "            accuracy = correct / total\n",
    "            scores.append(accuracy)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    #CALL THE MODEL\n",
    "    # def train_and_evaluate_model(model, train_loader, test_loader, optimizer, criterion, patience=5, num_epochs=50):\n",
    "    #     best_loss = float('inf')\n",
    "    #     patience_counter = 0\n",
    "    #     best_model_path = 'best_model.pth'  # File path for saving the best model\n",
    "\n",
    "    #     for epoch in range(num_epochs):  # number of epochs\n",
    "    #         model.train()\n",
    "    #         train_losses = []\n",
    "    #         for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):\n",
    "    #             optimizer.zero_grad()\n",
    "    #             outputs = model(inputs)\n",
    "    #             loss = criterion(outputs, labels.squeeze())\n",
    "    #             loss.backward()\n",
    "    #             optimizer.step()\n",
    "    #             train_losses.append(loss.item())\n",
    "\n",
    "    #         avg_train_loss = np.mean(train_losses)\n",
    "    #         print(f'Training Loss: {avg_train_loss}')\n",
    "\n",
    "    #         model.eval()\n",
    "    #         valid_losses = []\n",
    "    #         with torch.no_grad():\n",
    "    #             for inputs, labels in test_loader:\n",
    "    #                 outputs = model(inputs)\n",
    "    #                 loss = criterion(outputs, labels.squeeze())\n",
    "    #                 valid_losses.append(loss.item())\n",
    "\n",
    "    #         avg_valid_loss = np.mean(valid_losses)\n",
    "    #         print(f'Validation Loss: {avg_valid_loss}')\n",
    "\n",
    "    #         # Save the model at each epoch\n",
    "    #         epoch_model_path = f'model_epoch_{epoch + 1}.pth'\n",
    "    #         torch.save(model.state_dict(), epoch_model_path)\n",
    "\n",
    "    #         # Update the best model if validation loss improves\n",
    "    #         if avg_valid_loss < best_loss:\n",
    "    #             best_loss = avg_valid_loss\n",
    "    #             torch.save(model.state_dict(), best_model_path)\n",
    "    #             patience_counter = 0\n",
    "    #         else:\n",
    "    #             patience_counter += 1\n",
    "    #             if patience_counter >= patience:\n",
    "    #                 print('Early stopping triggered')\n",
    "    #                 break\n",
    "\n",
    "    #     # Load the best model\n",
    "    #     model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #     # Evaluate the best model\n",
    "    #     model.eval()\n",
    "    #     all_preds = []\n",
    "    #     all_labels = []\n",
    "    #     with torch.no_grad():\n",
    "    #         for inputs, labels in test_loader:\n",
    "    #             outputs = model(inputs)\n",
    "    #             _, preds = torch.max(outputs, 1)\n",
    "    #             all_preds.extend(preds.cpu().numpy())\n",
    "    #             all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    #     # Calculate confusion matrix and classification report\n",
    "    #     conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    #     class_report = classification_report(all_labels, all_preds,output_dict=True)\n",
    "    #     print('Confusion Matrix:')\n",
    "    #     print(conf_matrix)\n",
    "    #     print('Classification Report:')\n",
    "    #     print(class_report)\n",
    "    #     return conf_matrix, class_report\n",
    "\n",
    "    # def train_and_evaluate_model(model, train_loader, val_loader, test_loader, optimizer, criterion, patience=5, num_epochs=50):\n",
    "    #     best_loss = float('inf')\n",
    "    #     patience_counter = 0\n",
    "    #     best_model_path = 'best_model.pth'  # File path for saving the best model\n",
    "    #     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #     model.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "    #     for epoch in range(num_epochs):  # number of epochs\n",
    "    #         model.train()\n",
    "    #         train_losses = []\n",
    "    #         for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):\n",
    "    #             inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
    "    #             optimizer.zero_grad()\n",
    "    #             outputs = model(inputs)\n",
    "    #             loss = criterion(outputs, labels.squeeze())\n",
    "    #             loss.backward()\n",
    "    #             optimizer.step()\n",
    "    #             train_losses.append(loss.item())\n",
    "\n",
    "    #         avg_train_loss = np.mean(train_losses)\n",
    "    #         print(f'Training Loss: {avg_train_loss}')\n",
    "\n",
    "    #         model.eval()\n",
    "    #         valid_losses = []\n",
    "    #         with torch.no_grad():\n",
    "    #             for inputs, labels in val_loader:  # Use validation data for validation\n",
    "    #                 inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
    "    #                 outputs = model(inputs)\n",
    "    #                 loss = criterion(outputs, labels.squeeze())\n",
    "    #                 valid_losses.append(loss.item())\n",
    "\n",
    "    #         avg_valid_loss = np.mean(valid_losses)\n",
    "    #         print(f'Validation Loss: {avg_valid_loss}')\n",
    "\n",
    "    #         # Save the model at each epoch\n",
    "    #         epoch_model_path = f'model_epoch_{epoch + 1}.pth'\n",
    "    #         torch.save(model.state_dict(), epoch_model_path)\n",
    "\n",
    "    #         # Update the best model if validation loss improves\n",
    "    #         if avg_valid_loss < best_loss:\n",
    "    #             best_loss = avg_valid_loss\n",
    "    #             torch.save(model.state_dict(), best_model_path)\n",
    "    #             patience_counter = 0\n",
    "    #         else:\n",
    "    #             patience_counter += 1\n",
    "    #             if patience_counter >= patience:\n",
    "    #                 print('Early stopping triggered')\n",
    "    #                 break\n",
    "\n",
    "    #     # Load the best model\n",
    "    #     model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    #     model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #     # Evaluate the best model\n",
    "    #     model.eval()\n",
    "    #     all_preds = []\n",
    "    #     all_labels = []\n",
    "    #     with torch.no_grad():\n",
    "    #         for inputs, labels in test_loader:  # Use test data for final evaluation\n",
    "    #             inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
    "    #             outputs = model(inputs)\n",
    "    #             _, preds = torch.max(outputs, 1)\n",
    "    #             all_preds.extend(preds.cpu().numpy())\n",
    "    #             all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    #     # Calculate confusion matrix and classification report\n",
    "    #     conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    #     class_report = classification_report(all_labels, all_preds, output_dict=True)\n",
    "    #     print('Confusion Matrix:')\n",
    "    #     print(conf_matrix)\n",
    "    #     print('Classification Report:')\n",
    "    #     print(class_report)\n",
    "    #     return conf_matrix, class_report\n",
    "\n",
    "\n",
    "    def train_and_evaluate_model(model, train_loader, val_loader, test_loader, optimizer, criterion, patience=5, num_epochs=50):\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_model_path = 'best_model.pth'  # File path for saving the best model\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        model = model.to(device)  # Move the model to the appropriate device\n",
    "        for epoch in range(num_epochs):  # number of epochs\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "            for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "            avg_train_loss = np.mean(train_losses)\n",
    "            print(f'Training Loss: {avg_train_loss}')\n",
    "\n",
    "            model.eval()\n",
    "            valid_losses = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:  # Use validation data for validation\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels.squeeze())\n",
    "\n",
    "                    valid_losses.append(loss.item())\n",
    "\n",
    "            avg_valid_loss = np.mean(valid_losses)\n",
    "            print(f'Validation Loss: {avg_valid_loss}')\n",
    "\n",
    "            # Save the model at each epoch\n",
    "            epoch_model_path = f'model_epoch_{epoch + 1}.pth'\n",
    "            torch.save(model.state_dict(), epoch_model_path)\n",
    "\n",
    "            # Update the best model if validation loss improves\n",
    "            if avg_valid_loss < best_loss:\n",
    "                best_loss = avg_valid_loss\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print('Early stopping triggered')\n",
    "                    break\n",
    "\n",
    "        # Load the best model\n",
    "        # model.load_state_dict(torch.load(best_model_path))\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "        # Evaluate the best model\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:  # Use test data for final evaluation\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate confusion matrix and classification report\n",
    "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "        class_report = classification_report(all_labels, all_preds,output_dict=True)\n",
    "        print('Confusion Matrix:')\n",
    "        print(conf_matrix)\n",
    "        print('Classification Report:')\n",
    "        print(class_report)\n",
    "        return conf_matrix, class_report\n",
    "\n",
    "    import os\n",
    "\n",
    "    def save_to_path(df, filename, folder='pybaseball_v3_saved'):\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        \n",
    "        path = os.path.join(folder, f'{filename}.csv')\n",
    "        \n",
    "        if os.path.exists(path):\n",
    "            print(f'File {path} already exists.')\n",
    "        else:\n",
    "            df.to_csv(path, index=True)\n",
    "            print(f'{filename} saved to {path}')\n",
    "\n",
    "    def process_per_game_data(per_game_data, columns_to_keep, train_years):\n",
    "        # Create 'DateHmTmVisTm' column\n",
    "        per_game_data['Game_Number'] = per_game_data.groupby(['Date', 'HmTm', 'VisTm']).cumcount() + 1\n",
    "        per_game_data['Game_ID'] = (\n",
    "            per_game_data['Date'].astype(str) +\n",
    "            per_game_data['HmTm'] +\n",
    "            per_game_data['VisTm'] +\n",
    "            per_game_data['Game_Number'].astype(str)\n",
    "        )\n",
    "        # Set the index to 'DateHmTmVisTm'\n",
    "        # Reset the current index\n",
    "        per_game_data = per_game_data.reset_index()\n",
    "        # Set the new index\n",
    "        per_game_data.set_index('Game_ID', inplace=True)\n",
    "        # Drop unnecessary columns\n",
    "        per_game_data = per_game_data[columns_to_keep]\n",
    "        # Create 'winner' column\n",
    "        per_game_data['winner'] = np.where(per_game_data['HmRuns'] > per_game_data['VisRuns'], 1, 0)\n",
    "        \n",
    "        # Truncate rows based on the lowest train year\n",
    "        lowest_train_year = min(train_years)\n",
    "        per_game_data = per_game_data[per_game_data.index.str[:4] >= lowest_train_year]\n",
    "        \n",
    "        return per_game_data\n",
    "\n",
    "    def process_vegas_odds(paths):\n",
    "        \n",
    "        xlsx_dataframes = []\n",
    "        for i, path in enumerate(paths):\n",
    "            temp_df = pd.read_excel(path, skiprows=0 if i > 0 else 0)\n",
    "            temp_df.columns = temp_df.columns.str.replace('[^a-zA-Z0-9]', '')\n",
    "            year = path[-9:-5]  # extract year from file name\n",
    "            if 'Date' in temp_df.columns:\n",
    "                temp_df['Date'] = year + temp_df['Date'].astype(str).str.zfill(4)  # format date as yyyymmdd\n",
    "            xlsx_dataframes.append(temp_df)\n",
    "\n",
    "        try:\n",
    "            xlsx_dataframes = pd.concat(xlsx_dataframes, ignore_index=True)\n",
    "        except pd.errors.InvalidIndexError:\n",
    "            print('Error: Reindexing only valid with uniquely valued Index objects')\n",
    "\n",
    "        xlsx_dataframes = xlsx_dataframes[['Date', 'VH', 'Team', 'Open']]\n",
    "        home_teams_df = xlsx_dataframes[xlsx_dataframes['VH'] == 'H'].copy()\n",
    "        visiting_teams_df = xlsx_dataframes[xlsx_dataframes['VH'] == 'V'].copy()\n",
    "\n",
    "        home_teams_df.rename(columns={'Date': 'date', 'Team': 'home_team', 'Open': 'home_open'}, inplace=True)\n",
    "        visiting_teams_df.rename(columns={'Date': 'date', 'Team': 'visiting_team', 'Open': 'visiting_open'}, inplace=True)\n",
    "\n",
    "        # Merge on 'date'\n",
    "        xlsx_dataframes = pd.concat([home_teams_df.reset_index(drop=True), visiting_teams_df.reset_index(drop=True)], axis=1)\n",
    "        xlsx_dataframes = xlsx_dataframes.loc[:,~xlsx_dataframes.columns.duplicated()]\n",
    "        xlsx_dataframes = xlsx_dataframes[['date', 'home_team', 'visiting_team','home_open','visiting_open']]\n",
    "        xlsx_dataframes['Game_Number'] = xlsx_dataframes.groupby(['date', 'home_team', 'visiting_team']).cumcount() + 1\n",
    "        xlsx_dataframes['Game_ID'] = (\n",
    "            xlsx_dataframes['date'].astype(str) +\n",
    "            xlsx_dataframes['home_team'] +\n",
    "            xlsx_dataframes['visiting_team'] +\n",
    "            xlsx_dataframes['Game_Number'].astype(str)\n",
    "        )\n",
    "    \n",
    "        xlsx_dataframes.set_index('Game_ID', inplace=True)\n",
    "\n",
    "        xlsx_dataframes.drop(['Game_Number','date','home_team','visiting_team'], axis=1, inplace=True)\n",
    "        print(xlsx_dataframes)\n",
    "\n",
    "        return xlsx_dataframes\n",
    "\n",
    "    import time\n",
    "    from requests.exceptions import RequestException\n",
    "\n",
    "    def fetch_data_with_retry(fetch_func, data_type, max_retries=5, retry_delay=3):\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                data = fetch_func(True)\n",
    "                print(f\"{data_type} data fetched successfully.\")\n",
    "                return data\n",
    "            except RequestException as e:\n",
    "                retries += 1\n",
    "                print(f\"Attempt {retries} failed with error: {e}. Retrying after {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Exponential backoff\n",
    "        print(f\"Failed to fetch {data_type} data after {max_retries} attempts.\")\n",
    "        return None\n",
    "\n",
    "    def process_scrambled_odds(df):\n",
    "        df['Game_Number'] = df.groupby(['date', 'home_team', 'visiting_team']).cumcount() + 1\n",
    "        df['Game_ID'] = (\n",
    "            df['date'].astype(str) +\n",
    "            df['home_team'] +\n",
    "            df['visiting_team'] +\n",
    "            df['Game_Number'].astype(str)\n",
    "        )\n",
    "    \n",
    "        df.set_index('Game_ID', inplace=True)  \n",
    "        \n",
    "        # Fix: Correct the drop method usage by specifying the column indices correctly\n",
    "        columns_to_drop = df.columns[[0, 1]]\n",
    "        print(\"Dropping columns:\", columns_to_drop)\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "        return df\n",
    "\n",
    "    def remove_columns_with_nan(df, NaN_cutoff_percentage):\n",
    "        NaN_cutoff = NaN_cutoff_percentage / 100.0\n",
    "        return df.loc[:, df.isnull().mean() < NaN_cutoff]\n",
    "\n",
    "    def replace_player_ids_with_stats(per_game_data, player_data, player_id_columns):\n",
    "        # Calculate averages\n",
    "        player_avg = player_data.groupby(player_data.index).mean()\n",
    "\n",
    "        # Create a dictionary for each stat\n",
    "        stat_dicts = {stat: player_avg[stat].to_dict() for stat in player_avg.columns}\n",
    "\n",
    "        for column in player_id_columns:\n",
    "            # Replace the player_IDs in the column with the stats\n",
    "            for stat, stat_dict in stat_dicts.items():\n",
    "                per_game_data[column + '_' + stat] = per_game_data[column].map(stat_dict)\n",
    "\n",
    "            # Drop the original player_ID column\n",
    "            per_game_data.drop(column, axis=1, inplace=True)\n",
    "\n",
    "        return per_game_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def label_encode(df):\n",
    "        le = LabelEncoder()\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = le.fit_transform(df[col])\n",
    "        return df\n",
    "\n",
    "    def remove_excess_player_columns(player_data,columns_to_remove):\n",
    "        player_data = player_data.drop(columns=columns_to_remove)\n",
    "        return player_data\n",
    "\n",
    "    def split_data(per_game_data_reduced, train_year, test_year):\n",
    "        # Create a mask for the training set\n",
    "        train_mask = per_game_data_reduced.index.str.contains('|'.join(train_year))\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train = per_game_data_reduced[train_mask].drop(['winner','home_odds','visiting_odds','HmRuns','VisRuns'], axis=1)\n",
    "        y_train = per_game_data_reduced.loc[train_mask, 'winner']\n",
    "\n",
    "        # Create a mask for the test set\n",
    "        test_mask = per_game_data_reduced.index.str.contains(test_year)\n",
    "\n",
    "        X_test = per_game_data_reduced[test_mask].drop(['winner', 'home_odds','visiting_odds','HmRuns','VisRuns'], axis=1)\n",
    "        y_test = per_game_data_reduced.loc[test_mask, 'winner']\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "    # def predict_game_outcome(game_ids, game_data, model, augment=False, base_bet=100, scaler=1):\n",
    "    #     num_games = len(game_ids)\n",
    "    #     results, bets = [None]*num_games, [None]*num_games\n",
    "    #     skipped = []\n",
    "    #     for i in range(num_games):\n",
    "    #         try:\n",
    "    #             game_vector = torch.tensor([game_data[i]], dtype=torch.float)\n",
    "    #             model.eval()\n",
    "    #             with torch.no_grad():\n",
    "    #                 logits = model(game_vector)\n",
    "    #                 probs = logits.softmax(dim=-1)\n",
    "    #                 _, pred = torch.max(logits, 1)\n",
    "    #                 pred = pred.item()\n",
    "    #                 prob = probs[0][pred].item()\n",
    "    #             results[i] = (game_ids[i], 'True' if pred else 'False')\n",
    "    #             bets[i] = base_bet * prob * scaler if augment else base_bet\n",
    "    #         except:\n",
    "    #             skipped.append(i)\n",
    "    #     return results, bets, skipped\n",
    "\n",
    "    def predict_game_outcome(game_ids, game_data, model, augment=False, base_bet=100, scaler=1):\n",
    "        num_games = len(game_ids)\n",
    "        results, bets = [None]*num_games, [None]*num_games\n",
    "        skipped = []\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # model.to(device)  # Move your model to the GPU\n",
    "        for i in range(num_games):\n",
    "            try:\n",
    "                game_vector = torch.tensor([game_data[i]], dtype=torch.float).to(device)\n",
    "                # game_vector = torch.tensor([game_data[i]], dtype=torch.float)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    logits = model(game_vector)\n",
    "                    probs = logits.softmax(dim=-1)\n",
    "                    _, pred = torch.max(logits, 1)\n",
    "                    pred = pred.item()\n",
    "                    prob = probs[0][pred].item()\n",
    "                results[i] = (game_ids[i], 'True' if pred else 'False')\n",
    "                bets[i] = base_bet * prob * scaler if augment else base_bet\n",
    "            except:\n",
    "                skipped.append(i)\n",
    "        return results, bets, skipped, prob\n",
    "\n",
    "\n",
    "\n",
    "    def Impute(df, method):\n",
    "        # Create an imputer instance\n",
    "        imputer = SimpleImputer(strategy=method, fill_value=0)\n",
    "        # Fit and transform all columns\n",
    "        df[:] = imputer.fit_transform(df)\n",
    "        return df\n",
    "\n",
    "    class Better:\n",
    "        def __init__(self, initial_wallet=1000):\n",
    "            self.wallet = initial_wallet\n",
    "            self.wallet_history = [initial_wallet]  # Store the initial wallet balance\n",
    "            self.bet_history = []\n",
    "        def bet(self, game_indices, bet_on_home_team, amount_bet, game_data, prob, augment=False, scaler=1):\n",
    "            # Ensure the bettor has enough money in the wallet\n",
    "            amount_bet = max(self.wallet * 0.01, 10)\n",
    "            amount_bet = amount_bet * prob * scaler if augment else amount_bet\n",
    "            if amount_bet > self.wallet:\n",
    "                print(\"Insufficient funds for this bet.\")\n",
    "                return\n",
    "\n",
    "            # Retrieve the game data\n",
    "            games = game_data.loc[game_indices]\n",
    "            \n",
    "            # If games is a DataFrame, iterate over its rows\n",
    "            if isinstance(games, pd.DataFrame):\n",
    "                game_iter = games.iterrows()\n",
    "            else:  # If games is a Series (single row), create a one-item iterator\n",
    "                game_iter = [(game_indices, games)]\n",
    "            \n",
    "            for _, game in game_iter:\n",
    "                # Retrieve the odds\n",
    "                home_odds = game['home_odds']\n",
    "                away_odds = game['visiting_odds']\n",
    "                \n",
    "                # Determine the odds based on the team bet on\n",
    "                odds = home_odds if bet_on_home_team else away_odds\n",
    "                \n",
    "                # Determine if the bet was a win or loss\n",
    "                bet_won = (game['winner'] == bet_on_home_team)\n",
    "                \n",
    "                # Calculate the amount gained or lost\n",
    "                if bet_won:\n",
    "                    if odds > 0:\n",
    "                        # For positive odds, the gain is the odds * the amount bet / 100\n",
    "                        gain = amount_bet * odds / 100\n",
    "                    else:\n",
    "                        # For negative odds, the gain is the amount bet / (odds / -100)\n",
    "                        gain = amount_bet / (odds / -100)\n",
    "                    self.wallet += gain\n",
    "                else:\n",
    "                    # If the bet was lost, the loss is the amount bet\n",
    "                    self.wallet -= amount_bet\n",
    "\n",
    "                # Store the new wallet balance\n",
    "                self.wallet_history.append(self.wallet)\n",
    "                self.bet_history.append(amount_bet)\n",
    "            return self.wallet\n",
    "        \n",
    "    def combine_odds(per_game_data, vegas_odds, scrambled_odds):\n",
    "        # Merge vegas_odds and scrambled_odds into per_game_data\n",
    "        per_game_data = per_game_data.merge(vegas_odds, how='left', left_index=True, right_index=True)\n",
    "        per_game_data = per_game_data.merge(scrambled_odds, how='left', left_index=True, right_index=True, suffixes=('_vegas', '_scrambled'))\n",
    "\n",
    "        # Create new columns 'home_odds' and 'visiting_odds' where vegas_odds takes precedence\n",
    "        per_game_data['home_odds'] = per_game_data['home_open_vegas'].combine_first(per_game_data['home_open_scrambled'])\n",
    "        per_game_data['visiting_odds'] = per_game_data['visiting_open_vegas'].combine_first(per_game_data['visiting_open_scrambled'])\n",
    "\n",
    "        # Fill any remaining NaNs with -110\n",
    "        per_game_data['home_odds'].fillna(-110, inplace=True)\n",
    "        per_game_data['visiting_odds'].fillna(-110, inplace=True)\n",
    "\n",
    "        # Drop the original odds columns\n",
    "        per_game_data.drop(columns=['home_open_vegas', 'visiting_open_vegas', 'home_open_scrambled', 'visiting_open_scrambled'], inplace=True)\n",
    "\n",
    "        # Reset the index before returning\n",
    "\n",
    "\n",
    "        # return per_game_data[['home_odds', 'visiting_odds']]\n",
    "        return per_game_data\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.utils import shuffle\n",
    "    import copy\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    def permutation_importance(model, loader, criterion, metric):\n",
    "        model.eval()\n",
    "        original_score = metric(model, loader, criterion)\n",
    "        importances = []\n",
    "        for i in range(loader.dataset.X.size(1)):  # Assuming 'X' is the attribute storing your input data\n",
    "            temp = loader.dataset.X.clone()\n",
    "            temp[:, i] = torch.randperm(temp.size(0))\n",
    "            temp_dataset = vector_dataset(temp, loader.dataset.y)  # Assuming 'y' is the attribute storing your labels\n",
    "            temp_loader = DataLoader(temp_dataset, batch_size=64, shuffle=False)\n",
    "            score = metric(model, temp_loader, criterion)\n",
    "            importances.append(original_score - score)\n",
    "        return importances\n",
    "\n",
    "\n",
    "    def automated_feature_selection(model, train_loader, val_loader, test_loader, optimizer, criterion, metric, patience=5, num_epochs=50):\n",
    "        best_score = 0\n",
    "        best_model = None\n",
    "        important_features = list(range(len(train_loader.dataset[0][0])))\n",
    "        while len(important_features) > 0:\n",
    "            # Train the model\n",
    "            conf_matrix, class_report = train_and_evaluate_model(model, train_loader, val_loader, test_loader, optimizer, criterion, patience, num_epochs)\n",
    "            # Calculate the score\n",
    "            score = class_report['accuracy']  # Assuming class_report is a dictionary with 'accuracy' key\n",
    "            # If the score has improved, update the best score and best model\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = copy.deepcopy(model)\n",
    "            else:\n",
    "                # If the score has not improved, add the last removed feature back and break the loop\n",
    "                important_features.append(last_removed_feature)\n",
    "                break\n",
    "            # Calculate the permutation importance\n",
    "            importances = permutation_importance(model, val_loader.dataset.tensors[0][:, important_features], val_loader.dataset.tensors[1], metric)\n",
    "            # Remove the least important feature\n",
    "            last_removed_feature = important_features.pop(np.argmin(importances))\n",
    "        return best_model, important_features\n",
    "    # usage\n",
    "\n",
    "    train_years = [str(year) for year in range(int(test_year) - 8, int(test_year))]\n",
    "    #Vegas Odds path\n",
    "    vegas_betting_paths='./betting_odds/'\n",
    "    vegas_paths = [path for path in glob.glob(vegas_betting_paths + '*.xlsx')]\n",
    "\n",
    "    #Scrambled Odds path\n",
    "    scrambled_betting_paths='all_money_lines.csv'\n",
    "    scrambled_odds_full=pd.read_csv(scrambled_betting_paths)\n",
    "\n",
    "    #Per_game_data path\n",
    "    per_game_path='./pybaseball/pybaseball/data/Lahman_MLB_per_game_data.csv'\n",
    "\n",
    "\n",
    "\n",
    "    #People Import\n",
    "    chad_v3 = pyb.chadwick_register()\n",
    "\n",
    "    #Vegas Odds Import and process in one\n",
    "    vegas_odds=process_vegas_odds(vegas_paths)\n",
    "\n",
    "    #batter imports full data\n",
    "    batter_data = fetch_data_with_retry(pyb.bwar_bat, 'batter')\n",
    "    #pitcher imports full data\n",
    "    pitcher_data = fetch_data_with_retry(pyb.bwar_pitch, 'pitcher')\n",
    "\n",
    "\n",
    "    #per game data full data\n",
    "    per_game_data_full = pd.read_csv(per_game_path, header=0)\n",
    "\n",
    "\n",
    "\n",
    "    #Drops all columns except for the columns below\n",
    "    columns_to_keep = ['HmStPchID', 'VisStPchID', 'HmBat1ID', 'HmBat2ID', 'HmBat3ID', 'HmBat4ID', 'HmBat5ID', 'HmBat6ID', 'HmBat7ID', 'HmBat8ID', 'HmBat9ID', 'VisBat1ID', 'VisBat2ID', 'VisBat3ID', 'VisBat4ID', 'VisBat5ID', 'VisBat6ID', 'VisBat7ID', 'VisBat8ID', 'VisBat9ID', 'HmRuns', 'VisRuns','HmTm','VisTm']\n",
    "    per_game_data = process_per_game_data(per_game_data_full, columns_to_keep,train_years)\n",
    "    # per_game_players = add_players_to_games()\n",
    "\n",
    "    #process scrambled odds\n",
    "    scrambled_odds=process_scrambled_odds(scrambled_odds_full)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #save it to a folder called pybaseball_v3_saved\n",
    "    save_to_path(per_game_data_full, 'per_game_data_full')\n",
    "    save_to_path(per_game_data, 'per_game_data')\n",
    "    save_to_path(batter_data, 'batter_bwar_data')\n",
    "    save_to_path(pitcher_data, 'pitcher_bwar_data')\n",
    "    save_to_path(chad_v3,'chad_v3')\n",
    "    save_to_path(vegas_odds,'vegas_odds')\n",
    "    save_to_path(scrambled_odds,'scrambled_odds')\n",
    "    save_to_path(scrambled_odds_full,'scrambled_odds_full')\n",
    "\n",
    "\n",
    "\n",
    "    pitcher_names = pyb.playerid_reverse_lookup(pitcher_data['player_ID'], key_type='bbref')\n",
    "    batter_names = pyb.playerid_reverse_lookup(batter_data['player_ID'], key_type='bbref')\n",
    "    #For some reason the chad data is missing\n",
    "    batter_data = batter_data[batter_data['player_ID'] != 'stantha01']\n",
    "\n",
    "    # Create a mapping from player_ID to key_retro for pitchers and batters\n",
    "    pitcher_id_to_retro = pitcher_names.set_index('key_bbref')['key_retro'].to_dict()\n",
    "    batter_id_to_retro = batter_names.set_index('key_bbref')['key_retro'].to_dict()\n",
    "\n",
    "    # Replace player_ID with key_retro in pitcher_data and batter_data\n",
    "    pitcher_data['player_ID'] = pitcher_data['player_ID'].map(pitcher_id_to_retro)\n",
    "    batter_data['player_ID'] = batter_data['player_ID'].map(batter_id_to_retro)\n",
    "\n",
    "\n",
    "\n",
    "    # batter_data.drop(['name_common','mlb_ID','pitcher','year_ID','salary','PA'], axis=1, inplace=True)\n",
    "    # pitcher_data.drop(['name_common','mlb_ID','RpO_replacement','salary','year_ID'], axis=1, inplace=True)\n",
    "    batter_data.drop(['name_common','mlb_ID','year_ID'], axis=1, inplace=True)\n",
    "    pitcher_data.drop(['name_common','mlb_ID','year_ID'], axis=1, inplace=True)\n",
    "    batter_data.drop(batter_data.columns[0],axis=1,inplace=True)\n",
    "    pitcher_data.drop(pitcher_data.columns[0],axis=1,inplace=True)\n",
    "\n",
    "    pitcher_data=remove_columns_with_nan(pitcher_data,40)\n",
    "    batter_data=remove_columns_with_nan(batter_data,40)\n",
    "\n",
    "    batter_data.set_index('player_ID', inplace=True)\n",
    "    pitcher_data.set_index('player_ID', inplace=True)\n",
    "\n",
    "\n",
    "    pitcher_encoded=label_encode(pitcher_data)\n",
    "    batter_encoded=label_encode(batter_data)\n",
    "\n",
    "    # Calculate averages\n",
    "    pitcher_avg = pitcher_data.groupby('player_ID').mean()\n",
    "    batter_avg = batter_data.groupby('player_ID').mean()\n",
    "\n",
    "    # #map player_ids to per_game_data\n",
    "    pitcher_stats_dict = pitcher_avg.to_dict('index')\n",
    "    batter_stats_dict = batter_avg.to_dict('index')\n",
    "\n",
    "\n",
    "    per_game_data_odds = combine_odds(per_game_data, vegas_odds, scrambled_odds)\n",
    "\n",
    "\n",
    "\n",
    "    pitcher_columns = ['HmStPchID', 'VisStPchID']\n",
    "    batter_columns = ['HmBat1ID', 'HmBat2ID', 'HmBat3ID', 'HmBat4ID', 'HmBat5ID', 'HmBat6ID', 'HmBat7ID', 'HmBat8ID', 'HmBat9ID', 'VisBat1ID', 'VisBat2ID', 'VisBat3ID', 'VisBat4ID', 'VisBat5ID', 'VisBat6ID', 'VisBat7ID', 'VisBat8ID', 'VisBat9ID']\n",
    "\n",
    "    # Replace the player_IDs with stats\n",
    "    per_game_finished = replace_player_ids_with_stats(per_game_data_odds, pitcher_data, pitcher_columns)\n",
    "    per_game_finished = replace_player_ids_with_stats(per_game_data_odds, batter_data, batter_columns)\n",
    "    per_game_finished = label_encode(per_game_finished)\n",
    "    view_data(per_game_finished)\n",
    "    per_game_finished=Impute(per_game_finished,'mean')\n",
    "\n",
    "\n",
    "    per_game_finished.drop(['home_team','visiting_team','Game_Number'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # remove_columns = ['mlb_ID']\n",
    "    # pitcher_removed=remove_excess_player_columns(pitcher_data,remove_columns)\n",
    "    # batter_removed=remove_excess_player_columns(batter_data,remove_columns)\n",
    "\n",
    "    X_train, y_train, X_test, y_test = split_data(per_game_finished, train_years, test_year)\n",
    "    # Assume X_train and X_test are your input data\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "    # Fit on training set only.\n",
    "    scaler.fit(X_train.values)\n",
    "\n",
    "    # Apply transform to both the training set and the test set.\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train.values), columns=X_train.columns, index=X_train.index)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test.values), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Move your model to the specified device\n",
    "\n",
    "    # Split the original training data into new training data and validation data\n",
    "    X_train_new, X_val, y_train_new, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    # X_val_tensor = torch.from_numpy(X_val.values).float()\n",
    "    # y_val_tensor = torch.from_numpy(y_val.values).long()\n",
    "    # importances = permutation_importance(model, X_val_tensor, y_val_tensor, accuracy_score)\n",
    "    # importances = permutation_importance(model, X_val_tensor, y_val_tensor, accuracy_score)\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "    # Assume X and y are your input data and labels\n",
    "\n",
    "\n",
    "    # val_dataset = vector_dataset(X_val_tensor, y_val_tensor)\n",
    "    # train_dataset = vector_dataset(X_train_new, y_train_new)\n",
    "    # test_dataset = vector_dataset(X_test, y_test)\n",
    "\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Move your model to the specified device\n",
    "\n",
    "    # When you iterate over your DataLoader, move each batch to the GPU\n",
    "\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    # X_train_new=gpuarray.to_gpu(X_train_new)\n",
    "    # y_train_new=gpuarray.to_gpu(y_train_new)\n",
    "    # Assuming 'target' is the name of your target variable\n",
    "\n",
    "    # X_small, _, y_small, _ = train_test_split(X_train_new, y_train_new, test_size=0.95, random_state=42)  # Use 10% of the data\n",
    "    # hypers = classification_hyper_param_search(X_small, y_small, 5, 10)  # For classification tasks\n",
    "    # # # or\n",
    "    # # hypers = regression_hyper_param_search(X_small, y_small, 3, 5)  # For regression tasks\n",
    "    # xb_hypers = hypers[0]['best_params']\n",
    "    # rf_hypers = hypers[1]['best_params']\n",
    "\n",
    "    # ranking = classification_ranking(X_small, y_small, rf_hypers, xb_hypers)  # For classification tasks\n",
    "    # # # or\n",
    "    # # # #   ranking = regression_ranking(X_small, y_small, rf_hypers, xb_hypers)  # For regression tasks\n",
    "\n",
    "    # scoring = voting(ranking)\n",
    "    # plot_ranking(scoring, title='Feature Ranking')\n",
    "\n",
    "    # import pickle\n",
    "\n",
    "    # with open('scoring_full.pkl', 'wb') as f:\n",
    "    #     pickle.dump(scoring, f)\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    with open('scoring_full.pkl', 'rb') as f:\n",
    "        scoring = pickle.load(f)\n",
    "\n",
    "    threshold=200\n",
    "    # Get the list of columns to keep\n",
    "    # Convert `scoring` into a DataFrame\n",
    "    scoring_df = pd.DataFrame(scoring, columns=['column_name', 'score'])\n",
    "\n",
    "    # Filter the columns to keep\n",
    "    columns_to_keep = scoring_df[scoring_df['score'] > threshold]['column_name'].tolist()\n",
    "\n",
    "    # Drop the low ranking columns\n",
    "    X_train_new = X_train_new[columns_to_keep]\n",
    "    X_val_new = X_val[columns_to_keep]\n",
    "    X_test_new=X_test[columns_to_keep]\n",
    "\n",
    "\n",
    "    # y_train_new['index']=y_train_new.index\n",
    "\n",
    "    X_train_new['index']=X_train_new.index\n",
    "\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_train_res, y_train_res = rus.fit_resample(X_train_new, y_train_new)\n",
    "    # X_train_res=X_train_new\n",
    "    # y_train_res=y_train_new\n",
    "    # Assuming `vector_dataset` is a class that converts your data into a format suitable for PyTorch\n",
    "    # y_train_new.set_index('index',inplace=True)\n",
    "    X_train_res.set_index('index',inplace=True)\n",
    "\n",
    "\n",
    "    # Create a new model with the updated number of features\n",
    "\n",
    "    # Create new DataLoaders with the updated datasets\n",
    "    train_dataset_new = vector_dataset(X_train_res, y_train_res)\n",
    "    val_dataset_new = vector_dataset(X_val_new, y_val)\n",
    "    test_dataset_new = vector_dataset(X_test_new, y_test)\n",
    "\n",
    "    train_loader_new = DataLoader(train_dataset_new, batch_size=64, shuffle=True)\n",
    "    val_loader_new = DataLoader(val_dataset_new, batch_size=64, shuffle=False)\n",
    "    test_loader_new = DataLoader(test_dataset_new, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "    num_runs = 3\n",
    "    conf_matrices = []\n",
    "    class_reports = []\n",
    "\n",
    "\n",
    "    # Now you can train this model with the dropped datasets\n",
    "    for _ in range(num_runs):\n",
    "        # Reinitialize the model for each run\n",
    "    \n",
    "        model = neural_net(X_train_res.shape[1], X_train_res.shape[1]*2, 2, 2, 0.15).to(device)\n",
    "        \n",
    "\n",
    "    # When you iterate over your DataLoader, move each batch to the GPU\n",
    "        # model = neural_net(X_train.shape[1], X_train.shape[1]*2, 2, 2, 0.15)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        conf_matrix, class_report = train_and_evaluate_model(model, train_loader_new, val_loader_new, test_loader_new, optimizer, criterion)\n",
    "        # conf_matrix, class_report = train_and_evaluate_model(model, train_loader,val_loader, test_loader, optimizer, criterion)\n",
    "        conf_matrices.append(conf_matrix)\n",
    "        class_reports.append(pd.DataFrame(class_report).transpose())\n",
    "\n",
    "    #Average the confusion matrices\n",
    "    avg_conf_matrix = sum(conf_matrices) / num_runs\n",
    "\n",
    "    # Average the classification reports\n",
    "    avg_class_report = pd.concat(class_reports).groupby(level=0).mean()\n",
    "\n",
    "    print('Average Confusion Matrix:')\n",
    "    print(avg_conf_matrix)\n",
    "    print('Average Classification Report:')\n",
    "    print(avg_class_report)\n",
    "\n",
    "\n",
    "\n",
    "    prob = None\n",
    "    ok, bets, skipped, prob = predict_game_outcome(X_test_new.index, X_test_new.values, model, False, 10, 1)\n",
    "\n",
    "    better = Better()  # Create a Bettor instance with an initial wallet of 1000\n",
    "    # for game_id, bet_on_home_team in ok:\n",
    "    #     better.bet(game_id, bet_on_home_team == 'True', bets, per_game_data_imputed)\n",
    "\n",
    "    for (game_id, bet_on_home_team), amount_bet in zip(ok, bets):\n",
    "        better.bet(game_id, bet_on_home_team == 'True', amount_bet, per_game_finished, prob, True, 3)\n",
    "    # Create a new model with the updated number of features\n",
    "    print(np.bincount(y_train_res))\n",
    "    # def plot_wallet_balance(better):\n",
    "    #     fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    #     color = 'tab:blue'\n",
    "    #     ax1.set_xlabel('Number of Games Bet')\n",
    "    #     ax1.set_ylabel('Wallet Balance', color=color)\n",
    "    #     ax1.plot(range(len(better.wallet_history)), better.wallet_history, color=color)\n",
    "    #     ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    #     ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "    #     color = 'tab:red'\n",
    "    #     ax2.set_ylabel('Bet Amount', color=color)  # we already handled the x-label with ax1\n",
    "    #     ax2.plot(range(len(better.bet_history)), better.bet_history, color=color)\n",
    "    #     ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    #     fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    #     plt.title('Wallet Balance and Bet Amount Over Number of Games Bet')\n",
    "    #     plt.grid(True)\n",
    "    #     plt.show()\n",
    "        \n",
    "\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    def plot_wallet_balance(better, year):\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        color = 'tab:blue'\n",
    "        ax1.set_xlabel('Number of Games Bet')\n",
    "        ax1.set_ylabel('Wallet Balance', color=color)\n",
    "        ax1.plot(range(len(better.wallet_history)), better.wallet_history, color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "        color = 'tab:red'\n",
    "        ax2.set_ylabel('Bet Amount', color=color)  # we already handled the x-label with ax1\n",
    "        ax2.plot(range(len(better.bet_history)), better.bet_history, color=color)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        title = f'Wallet Balance and Bet Amount Over Number of Games Bet in {year}'\n",
    "        plt.title(title)\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot as an image\n",
    "        if not os.path.exists('results'):\n",
    "            os.makedirs('results')\n",
    "        plt.savefig(f'results/{title}.png')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Save the better data, avg_class_report, and avg_conf_matrix to an Excel file\n",
    "        with pd.ExcelWriter(f'results/better_data_{year}.xlsx') as writer:\n",
    "            pd.DataFrame(better.wallet_history).to_excel(writer, sheet_name='wallet_history')\n",
    "            pd.DataFrame(better.bet_history).to_excel(writer, sheet_name='bet_history')\n",
    "            avg_class_report.to_excel(writer, sheet_name='avg_class_report')\n",
    "            pd.DataFrame(avg_conf_matrix).to_excel(writer, sheet_name='avg_conf_matrix')\n",
    "    plot_wallet_balance(better, test_year)\n",
    "    print(max(better.bet_history))\n",
    "    print(min(better.bet_history))\n",
    "    print(min(better.wallet_history))\n",
    "    print(better.wallet_history[-1])\n",
    "\n",
    "\n",
    "\n",
    "    return avg_class_report, avg_conf_matrix, better.bet_history, better.wallet_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique batter and pitcher ids from per_game_data\n",
    "# batter_ids_per_game = per_game_data[batter_columns].values.flatten()\n",
    "# pitcher_ids_per_game = per_game_data[pitcher_columns].values.flatten()\n",
    "\n",
    "# # Check if these ids exist in the batter and pitcher data\n",
    "# batter_ids_exist = batter_data['player_ID'].isin(batter_ids_per_game).any()\n",
    "# pitcher_ids_exist = pitcher_data['player_ID'].isin(pitcher_ids_per_game).any()\n",
    "\n",
    "# print(f\"Do any batter ids in per_game_data exist in batter_data? {batter_ids_exist}\")\n",
    "# print(f\"Do any pitcher ids in per_game_data exist in pitcher_data? {pitcher_ids_exist}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
